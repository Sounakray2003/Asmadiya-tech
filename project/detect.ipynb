{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efc8aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ultralytics>=8.0.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 1)) (8.3.176)\n",
      "Requirement already satisfied: pillow in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 3)) (4.12.0.88)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 4)) (0.3.13)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 5)) (2.8.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 6)) (0.23.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 7)) (2.8.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 9)) (2.2.6)\n",
      "Requirement already satisfied: albumentations in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 10)) (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (1.16.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from ultralytics>=8.0.0->-r requirements.txt (line 1)) (2.0.15)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pytesseract->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.13->-r requirements.txt (line 5)) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->-r requirements.txt (line 8)) (0.4.6)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from albumentations->-r requirements.txt (line 10)) (2.11.7)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from albumentations->-r requirements.txt (line 10)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from albumentations->-r requirements.txt (line 10)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 10)) (3.12.6)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 10)) (6.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=1.1.4->ultralytics>=8.0.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas>=1.1.4->ultralytics>=8.0.0->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 10)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.23.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.23.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.23.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.23.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy>=1.13.3->torch>=1.13->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.13->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics>=8.0.0->-r requirements.txt (line 1)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\souna\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bff4e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100%|██████████| 6.25M/6.25M [00:00<00:00, 29.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=aadhaar_detect, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\aadhaar_detect, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    754237  ultralytics.nn.modules.head.Detect           [15, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,013,773 parameters, 3,013,757 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 238.390.5 MB/s, size: 38.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\train\\labels... 1182 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1182/1182 [00:00<00:00, 1805.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\train\\labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 368.8102.7 MB/s, size: 43.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<00:00, 2026.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache\n",
      "Plotting labels to runs\\detect\\aadhaar_detect\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\aadhaar_detect\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30         0G       1.42        3.2      1.373        105        640: 100%|██████████| 148/148 [07:25<00:00,  3.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:20<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.528        0.5      0.339      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30         0G       1.24      1.843      1.247         84        640: 100%|██████████| 148/148 [07:13<00:00,  2.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:16<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.509      0.721      0.578      0.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30         0G      1.165      1.522      1.196         65        640: 100%|██████████| 148/148 [05:30<00:00,  2.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.467      0.718      0.598      0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30         0G      1.128      1.402      1.164         71        640: 100%|██████████| 148/148 [10:02<00:00,  4.07s/it]  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.476      0.795      0.637      0.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30         0G        1.1      1.338      1.152         88        640: 100%|██████████| 148/148 [08:51<00:00,  3.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:10<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.481      0.887      0.631       0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30         0G      1.079      1.314      1.135         68        640: 100%|██████████| 148/148 [06:52<00:00,  2.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.48       0.88      0.648      0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30         0G      1.053      1.266      1.118         93        640: 100%|██████████| 148/148 [05:07<00:00,  2.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.536      0.848      0.666      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30         0G      1.028      1.209      1.104         66        640: 100%|██████████| 148/148 [04:45<00:00,  1.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.57      0.806      0.661      0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30         0G      1.016      1.193        1.1         70        640: 100%|██████████| 148/148 [08:12<00:00,  3.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:19<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.597      0.801       0.68      0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30         0G     0.9777       1.17      1.083         61        640: 100%|██████████| 148/148 [07:47<00:00,  3.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.502      0.852       0.62      0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30         0G     0.9728      1.148      1.081         69        640: 100%|██████████| 148/148 [05:19<00:00,  2.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:21<00:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.524      0.912       0.69      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30         0G      0.958      1.132      1.071        104        640: 100%|██████████| 148/148 [06:06<00:00,  2.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.56      0.797      0.678      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30         0G     0.9375      1.117      1.063         53        640: 100%|██████████| 148/148 [06:13<00:00,  2.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:18<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.611      0.781      0.687      0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30         0G     0.9304      1.093      1.058        107        640: 100%|██████████| 148/148 [08:33<00:00,  3.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.59       0.77      0.661      0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30         0G     0.9118      1.078      1.049         99        640: 100%|██████████| 148/148 [04:58<00:00,  2.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.57      0.835      0.679      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30         0G     0.8925      1.056      1.038         67        640: 100%|██████████| 148/148 [04:52<00:00,  1.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.647      0.757      0.708      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30         0G     0.8706      1.046      1.037         93        640: 100%|██████████| 148/148 [04:53<00:00,  1.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.65      0.743      0.691      0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30         0G     0.8621       1.04      1.031         71        640: 100%|██████████| 148/148 [05:01<00:00,  2.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.612      0.809       0.69       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30         0G     0.8545      1.046      1.037         71        640: 100%|██████████| 148/148 [06:53<00:00,  2.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.545      0.959      0.691      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30         0G     0.8499      1.028      1.023         91        640: 100%|██████████| 148/148 [04:51<00:00,  1.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.53      0.944       0.69      0.536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "      21/30         0G     0.8033       1.01      1.038         47        640: 100%|██████████| 148/148 [04:57<00:00,  2.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:10<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.615      0.753      0.684      0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30         0G     0.7716     0.9831      1.015         49        640: 100%|██████████| 148/148 [06:23<00:00,  2.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:21<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.581      0.777       0.67      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30         0G     0.7534      0.971      1.012         44        640: 100%|██████████| 148/148 [04:54<00:00,  1.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.605      0.777      0.671      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30         0G     0.7312     0.9581     0.9966         47        640: 100%|██████████| 148/148 [04:44<00:00,  1.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.623      0.777      0.692      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30         0G     0.7191     0.9485      0.992         49        640: 100%|██████████| 148/148 [04:44<00:00,  1.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.655      0.717      0.667      0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30         0G      0.713     0.9483     0.9914         48        640: 100%|██████████| 148/148 [04:48<00:00,  1.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.659      0.732      0.684      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30         0G     0.6914      0.927     0.9772         47        640: 100%|██████████| 148/148 [06:15<00:00,  2.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.635      0.768      0.686       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30         0G     0.6759      0.927     0.9677         44        640: 100%|██████████| 148/148 [05:08<00:00,  2.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.596      0.775      0.673      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30         0G     0.6776     0.9239     0.9792         42        640: 100%|██████████| 148/148 [06:21<00:00,  2.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:09<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.625      0.759      0.682      0.544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30         0G     0.6616     0.9199     0.9664         45        640: 100%|██████████| 148/148 [04:54<00:00,  1.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.61       0.76      0.672      0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 3.143 hours.\n",
      "Optimizer stripped from runs\\detect\\aadhaar_detect\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\aadhaar_detect\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\aadhaar_detect\\weights\\best.pt...\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Model summary (fused): 72 layers, 3,008,573 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.635      0.768      0.685      0.549\n",
      "                     0          5          5      0.901          1      0.995      0.856\n",
      "                     1         16         16      0.788          1      0.986      0.827\n",
      "                    10         39         39      0.619      0.359      0.491      0.376\n",
      "                    11          8          8      0.562      0.483      0.517      0.402\n",
      "                    12         96         97      0.612      0.773      0.667      0.489\n",
      "                    13         39         39      0.339      0.564      0.445      0.373\n",
      "                    14        104        105      0.666       0.78       0.66      0.543\n",
      "                     2         93         96      0.615      0.732      0.691      0.536\n",
      "                     3         16         16      0.572      0.875       0.64      0.565\n",
      "                     4         96         98      0.617      0.786      0.677      0.493\n",
      "                     5         73         74      0.655      0.948      0.737      0.548\n",
      "                     6         70         71      0.739      0.915      0.755      0.562\n",
      "                     7         96         97      0.619      0.794      0.715       0.53\n",
      "                     8         16         16      0.624      0.812      0.649      0.567\n",
      "                     9        101        102      0.599      0.696      0.652      0.572\n",
      "Speed: 1.1ms preprocess, 64.8ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\aadhaar_detect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def train(data_yaml=\"data.yaml\", epochs=30, imgsz=640, batch=8, model=\"yolov8n.pt\"):\n",
    "    model = YOLO(model)\n",
    "    model.train(\n",
    "        data=data_yaml,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        name=\"aadhaar_detect\",\n",
    "    )\n",
    "\n",
    "# Call the train function directly with arguments\n",
    "train(\n",
    "    data_yaml=\"data.yaml\",  # path to your dataset yaml\n",
    "    epochs=30,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    model=\"yolov8n.pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c98de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verhoeff checksum algorithm for Aadhaar validation\n",
    "# Standard implementation using multiplication and permutation tables.\n",
    "\n",
    "mul = [\n",
    "    [0,1,2,3,4,5,6,7,8,9],\n",
    "    [1,2,3,4,0,6,7,8,9,5],\n",
    "    [2,3,4,0,1,7,8,9,5,6],\n",
    "    [3,4,0,1,2,8,9,5,6,7],\n",
    "    [4,0,1,2,3,9,5,6,7,8],\n",
    "    [5,9,8,7,6,0,4,3,2,1],\n",
    "    [6,5,9,8,7,1,0,4,3,2],\n",
    "    [7,6,5,9,8,2,1,0,4,3],\n",
    "    [8,7,6,5,9,3,2,1,0,4],\n",
    "    [9,8,7,6,5,4,3,2,1,0]\n",
    "]\n",
    "\n",
    "perm = [\n",
    "    [0,1,2,3,4,5,6,7,8,9],\n",
    "    [1,5,7,6,2,8,3,0,9,4],\n",
    "    [5,8,0,3,7,9,6,1,4,2],\n",
    "    [8,9,1,6,0,4,3,5,2,7],\n",
    "    [9,4,5,3,1,2,6,8,7,0],\n",
    "    [4,2,8,6,5,7,3,9,0,1],\n",
    "    [2,7,9,3,8,0,6,4,1,5],\n",
    "    [7,0,4,6,9,1,3,2,5,8]\n",
    "]\n",
    "\n",
    "inv = [0,4,3,2,1,5,6,7,8,9]\n",
    "\n",
    "def verhoeff_check(num_str: str) -> bool:\n",
    "    \"\"\"Return True if num_str passes Verhoeff checksum.\"\"\"\n",
    "    try:\n",
    "        digits = list(map(int, reversed(num_str)))\n",
    "    except Exception:\n",
    "        return False\n",
    "    c = 0\n",
    "    for i, d in enumerate(digits):\n",
    "        c = mul[c][perm[(i % 8)][d]]\n",
    "    return c == 0\n",
    "\n",
    "def verhoeff_generate(num_str_without_checkdigit: str) -> int:\n",
    "    c = 0\n",
    "    digits = list(map(int, reversed(num_str_without_checkdigit)))\n",
    "    for i, d in enumerate(digits):\n",
    "        c = mul[c][perm[((i+1) % 8)][d]]\n",
    "    return inv[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216e85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimal CRNN OCR model using PyTorch for digit-only recognition.\n",
    "Input images expected as grayscale, resized to height H with aspect-ratio preserved.\n",
    "Outputs sequence of digits using CTC.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH=32, nc=1, nclass=11, nh=256):\n",
    "        \"\"\"\n",
    "        nclass = 10 digits + 1 blank for CTC\n",
    "        \"\"\"\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH % 16 == 0, \"imgH must be multiple of 16 for downsampling\"\n",
    "        ks = [3,3,3,3,3,3,2]\n",
    "        ps = [1,1,1,1,1,1,0]\n",
    "        ss = [1,1,1,1,1,1,1]\n",
    "        nm = [64,128,256,256,512,512,512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "        def conv_relu(i, batch_norm=False):\n",
    "            nIn = nc if i==0 else nm[i-1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module(f\"conv{i}\", nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batch_norm:\n",
    "                cnn.add_module(f\"bn{i}\", nn.BatchNorm2d(nOut))\n",
    "            cnn.add_module(f\"relu{i}\", nn.ReLU(True))\n",
    "        conv_relu(0)\n",
    "        cnn.add_module(\"pool0\", nn.MaxPool2d(2,2))  # /2\n",
    "        conv_relu(1)\n",
    "        cnn.add_module(\"pool1\", nn.MaxPool2d(2,2))  # /4\n",
    "        conv_relu(2, batch_norm=True)\n",
    "        conv_relu(3)\n",
    "        cnn.add_module(\"pool2\", nn.MaxPool2d((2,1),(2,1),(0,0)))  # /8 height\n",
    "        conv_relu(4, batch_norm=True)\n",
    "        conv_relu(5)\n",
    "        cnn.add_module(\"pool3\", nn.MaxPool2d((2,1),(2,1),(0,0)))  # /16 height\n",
    "        conv_relu(6, batch_norm=True)\n",
    "\n",
    "        self.cnn = cnn\n",
    "\n",
    "        # rnn layers\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh),\n",
    "            BidirectionalLSTM(nh, nh, nclass)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input: batch x channel x H x W\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1 or h==2, \"expected small height after convs\"\n",
    "        conv = conv.squeeze(2)  # b x c x w\n",
    "        conv = conv.permute(2, 0, 1)  # w x b x c\n",
    "        output = self.rnn(conv)  # seq_len x batch x nclass\n",
    "        return output\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.linear = nn.Linear(nHidden*2, nOut)\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T*b, h)\n",
    "        output = self.linear(t_rec)\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba90f379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels from directory: ./train/labels/, found 1182 .txt files\n",
      "Dataset: found 1182 labeled samples in ./train/images/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 74/74 [00:53<00:00,  1.38it/s, loss=162.5631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss 2.1968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 74/74 [01:10<00:00,  1.05it/s, loss=129.3342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss 1.7478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 74/74 [01:01<00:00,  1.20it/s, loss=118.6004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss 1.6027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 74/74 [01:00<00:00,  1.22it/s, loss=118.4427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss 1.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 74/74 [00:59<00:00,  1.24it/s, loss=148.9334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss 2.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 74/74 [00:59<00:00,  1.25it/s, loss=145.9531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average loss 1.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 74/74 [01:03<00:00,  1.17it/s, loss=130.4121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 average loss 1.7623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 74/74 [01:01<00:00,  1.20it/s, loss=123.3487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 average loss 1.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 74/74 [00:59<00:00,  1.24it/s, loss=128.0496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 average loss 1.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 74/74 [01:01<00:00,  1.20it/s, loss=125.4469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 average loss 1.6952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 74/74 [01:04<00:00,  1.14it/s, loss=133.7067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 average loss 1.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 74/74 [01:02<00:00,  1.19it/s, loss=135.6471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 average loss 1.8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 74/74 [01:03<00:00,  1.17it/s, loss=132.4950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 average loss 1.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 74/74 [01:02<00:00,  1.18it/s, loss=129.6272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 average loss 1.7517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 74/74 [01:01<00:00,  1.20it/s, loss=122.6996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 average loss 1.6581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 74/74 [01:02<00:00,  1.19it/s, loss=121.1719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 average loss 1.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 74/74 [01:02<00:00,  1.19it/s, loss=118.1901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 average loss 1.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 74/74 [01:02<00:00,  1.18it/s, loss=116.8814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 average loss 1.5795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 74/74 [01:02<00:00,  1.19it/s, loss=116.8103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 average loss 1.5785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 74/74 [01:03<00:00,  1.16it/s, loss=118.8271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 average loss 1.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train CRNN for digit recognition using CTC.\n",
    "\n",
    "Supports:\n",
    "- Single labels file: each line \"filename.png 123456789012\"\n",
    "- Labels directory: per-image .txt files containing the digit string (e.g. 0001.txt -> \"123456789012\")\n",
    "\n",
    "Requires:\n",
    "- rec_crops/ or your images folder with crops\n",
    "- crnn.py implementing CRNN class available in the same directory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# import your CRNN definition (make sure crnn.py is present)\n",
    "#from crnn import CRNN\n",
    "\n",
    "# Only digits 0–9\n",
    "CHARSET = \"0123456789\"\n",
    "BLANK_IDX = len(CHARSET)  # for CTC blank\n",
    "VALID_IMG_EXTS = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"]\n",
    "\n",
    "def find_image_path(crops_dir: str, base_name: str):\n",
    "    \"\"\"Find an image file in crops_dir that matches base_name (without extension).\"\"\"\n",
    "    for ext in VALID_IMG_EXTS:\n",
    "        candidate = os.path.join(crops_dir, base_name + ext)\n",
    "        if os.path.isfile(candidate):\n",
    "            return candidate\n",
    "    # try direct filename (in case user passed full filenames earlier)\n",
    "    candidate = os.path.join(crops_dir, base_name)\n",
    "    if os.path.isfile(candidate):\n",
    "        return candidate\n",
    "    return None\n",
    "\n",
    "class AadhaarCropDataset(Dataset):\n",
    "    def __init__(self, crops_dir=\"./train/images/\", labels_path=\"./train/labels/\", imgH=32, max_width=256, verbose=True):\n",
    "        \"\"\"\n",
    "        labels_path: either a path to a single text file (each line \"fname label\")\n",
    "                     or a directory containing per-image .txt files with the label string.\n",
    "        crops_dir: directory where the images live.\n",
    "        \"\"\"\n",
    "        self.crops_dir = crops_dir\n",
    "        self.imgH = imgH\n",
    "        self.max_width = max_width\n",
    "        self.transform = T.Compose([\n",
    "            T.Grayscale(num_output_channels=1),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "        self.samples = []  # list of tuples (image_filename_relative, label_string)\n",
    "\n",
    "        if os.path.isdir(labels_path):\n",
    "            # labels_path is a directory with one .txt per image (e.g., 0001.txt containing \"1234...\")\n",
    "            labels_dir = labels_path\n",
    "            label_files = sorted([p for p in os.listdir(labels_dir) if p.lower().endswith('.txt')])\n",
    "            if verbose:\n",
    "                print(f\"Loading labels from directory: {labels_dir}, found {len(label_files)} .txt files\")\n",
    "            for lf in label_files:\n",
    "                base = os.path.splitext(lf)[0]\n",
    "                img_path = find_image_path(crops_dir, base)\n",
    "                if img_path is None:\n",
    "                    # skip if corresponding image not found\n",
    "                    continue\n",
    "                with open(os.path.join(labels_dir, lf), 'r', encoding='utf-8') as f:\n",
    "                    label = f.read().strip().splitlines()[0].strip() if f.readable() else \"\"\n",
    "                if not label:\n",
    "                    # skip empty labels\n",
    "                    continue\n",
    "                # store filename relative to crops_dir (we will join later)\n",
    "                self.samples.append((os.path.basename(img_path), label))\n",
    "        elif os.path.isfile(labels_path):\n",
    "            # labels_path is a single file with \"filename label\" per line\n",
    "            if verbose:\n",
    "                print(f\"Loading labels from file: {labels_path}\")\n",
    "            with open(labels_path, \"r\", encoding='utf-8') as f:\n",
    "                lines = [l.strip() for l in f if l.strip()]\n",
    "            for ln in lines:\n",
    "                parts = ln.split()\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                fname, label = parts[0], parts[1]\n",
    "                # if fname doesn't exist as-is, try to find with common extensions\n",
    "                if not os.path.isfile(os.path.join(crops_dir, fname)):\n",
    "                    base = os.path.splitext(fname)[0]\n",
    "                    alt = find_image_path(crops_dir, base)\n",
    "                    if alt is None:\n",
    "                        continue\n",
    "                    fname = os.path.basename(alt)\n",
    "                self.samples.append((fname, label))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"labels_path {labels_path} is neither a file nor a directory.\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Dataset: found {len(self.samples)} labeled samples in {crops_dir}\")\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(\"No labeled samples found. Check your crops_dir and labels_path.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def resize_keep_aspect(self, img: Image.Image) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        new_h = self.imgH\n",
    "        new_w = int(w * (new_h / h))\n",
    "        new_w = max(1, min(new_w, self.max_width))\n",
    "        return img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn, label = self.samples[idx]\n",
    "        img_path = os.path.join(self.crops_dir, fn)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.resize_keep_aspect(img)\n",
    "        img = self.transform(img)\n",
    "        # pad to max_width on the right\n",
    "        c, h, w = img.shape\n",
    "        pad = torch.zeros((c, h, self.max_width), dtype=img.dtype)\n",
    "        pad[:, :, :w] = img\n",
    "        return pad, label, w\n",
    "\n",
    "def encode_label(label: str):\n",
    "    # Only keep digits, map to indexes. If invalid char found, raise ValueError.\n",
    "    digits = \"\".join([ch for ch in label if ch.isdigit()])\n",
    "    if len(digits) == 0:\n",
    "        raise ValueError(f\"Empty or no-digit label: '{label}'\")\n",
    "    return [CHARSET.index(ch) for ch in digits]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels, widths = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    encoded = [torch.tensor(encode_label(l), dtype=torch.long) for l in labels]\n",
    "    lengths = torch.tensor([len(e) for e in encoded], dtype=torch.long)\n",
    "    targets = torch.cat(encoded) if len(encoded) > 0 else torch.tensor([], dtype=torch.long)\n",
    "    return images, targets, lengths, widths\n",
    "\n",
    "def train(crops=\"./train/images/\", labels=\"./train/labels/\", imgH=32, maxw=256, batch=16, epochs=20, lr=1e-3, device=None):\n",
    "    \"\"\"\n",
    "    crops: directory with images\n",
    "    labels: either a single label file or a directory with one .txt per image\n",
    "    \"\"\"\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    dataset = AadhaarCropDataset(crops_dir=crops, labels_path=labels, imgH=imgH, max_width=maxw)\n",
    "    loader = DataLoader(dataset, batch_size=batch, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=False)\n",
    "\n",
    "    model = CRNN(imgH=imgH, nc=1, nclass=len(CHARSET)+1, nh=256).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "        for imgs, targets, lengths, widths in pbar:\n",
    "            imgs = imgs.to(device)\n",
    "            # targets and lengths are for CTC\n",
    "            preds = model(imgs)  # seq_len x batch x nclass\n",
    "            seq_len, batch_size, nclass = preds.size()\n",
    "            preds_logsoft = F.log_softmax(preds, dim=2)\n",
    "            input_lengths = torch.full(size=(batch_size,), fill_value=seq_len, dtype=torch.long)\n",
    "            try:\n",
    "                loss = criterion(preds_logsoft, targets.to(device), input_lengths, lengths.to(device))\n",
    "            except Exception as e:\n",
    "                # print debug info and re-raise\n",
    "                print(\"CTC loss computation error:\", e)\n",
    "                print(\"preds shape:\", preds.shape, \"targets shape:\", targets.shape, \"lengths:\", lengths)\n",
    "                raise\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix_str(f\"loss={total_loss:.4f}\")\n",
    "        avg_loss = total_loss / max(1, len(loader))\n",
    "        print(f\"Epoch {epoch} average loss {avg_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"crnn_epoch{epoch}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example call - adjust paths as needed\n",
    "    train(\n",
    "        crops=\"./train/images/\",\n",
    "        labels=\"./train/labels/\",   # can be \"./train/rec_labels.txt\" OR \"./train/labels/\" directory with per-image .txt\n",
    "        imgH=32,\n",
    "        maxw=256,\n",
    "        batch=16,\n",
    "        epochs=20\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a90e74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN not loaded, will use tesseract as fallback. [Errno 2] No such file or directory: 'crnn_epoch19.pth'\n",
      "\n",
      "image 1/1 C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\images\\1b26b406ad9485c98cd496bc371db57f_jpg.rf.3d95677f5daed847c3d4477325e99ac6.jpg: 640x640 2 12s, 1 14, 1 2, 1 4, 1 5, 1 6, 1 7, 1 9, 126.6ms\n",
      "Speed: 1.9ms preprocess, 126.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "image 1/1 C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\images\\1b26b406ad9485c98cd496bc371db57f_jpg.rf.3d95677f5daed847c3d4477325e99ac6.jpg: 640x640 2 12s, 1 14, 1 2, 1 4, 1 5, 1 6, 1 7, 1 9, 126.6ms\n",
      "Speed: 1.9ms preprocess, 126.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "399490782825\n",
      "399490782825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'digits': '',\n",
       "  'valid': False,\n",
       "  'conf': 0.6937074065208435,\n",
       "  'box': [241, 253, 299, 291]},\n",
       " {'digits': '',\n",
       "  'valid': False,\n",
       "  'conf': 0.6650499701499939,\n",
       "  'box': [164, 0, 569, 107]},\n",
       " {'digits': '',\n",
       "  'valid': False,\n",
       "  'conf': 0.6473112106323242,\n",
       "  'box': [25, 126, 196, 427]},\n",
       " {'digits': '',\n",
       "  'valid': False,\n",
       "  'conf': 0.6319316029548645,\n",
       "  'box': [463, 213, 611, 432]},\n",
       " {'digits': '399490782825',\n",
       "  'valid': True,\n",
       "  'conf': 0.5936887860298157,\n",
       "  'box': [194, 423, 435, 484]},\n",
       " {'digits': '0',\n",
       "  'valid': False,\n",
       "  'conf': 0.5880429148674011,\n",
       "  'box': [345, 201, 451, 245]},\n",
       " {'digits': '4',\n",
       "  'valid': False,\n",
       "  'conf': 0.5842002034187317,\n",
       "  'box': [64, 0, 124, 123]},\n",
       " {'digits': '',\n",
       "  'valid': False,\n",
       "  'conf': 0.5252200961112976,\n",
       "  'box': [190, 163, 335, 205]},\n",
       " {'digits': '10193',\n",
       "  'valid': False,\n",
       "  'conf': 0.3638243079185486,\n",
       "  'box': [346, 201, 451, 245]}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "End-to-end inference:\n",
    "- load YOLO detector (Ultralytics)\n",
    "- for each image: detect aadhaar_number boxes\n",
    "- crop, preprocess, then run either:\n",
    "    - CRNN model (if trained)\n",
    "    - fallback to pytesseract if CRNN not available\n",
    "- normalize digits and apply Verhoeff check\n",
    "\"\"\"\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "\n",
    "# paths (adjust)\n",
    "YOLO_WEIGHTS = \"runs/detect/aadhaar_detect/weights/best.pt\"  # where ultralytics saves weights\n",
    "#CRNN_WEIGHTS = \"crnn_epoch19.pth\"  # example crnn weights\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load detector\n",
    "detector = YOLO(YOLO_WEIGHTS)\n",
    "\n",
    "# load crnn if exists\n",
    "use_crnn = False\n",
    "try:\n",
    "    crnn = CRNN(imgH=32, nc=1, nclass=11, nh=256)\n",
    "    crnn.load_state_dict(torch.load(CRNN_WEIGHTS, map_location=DEVICE))\n",
    "    crnn.to(DEVICE)\n",
    "    crnn.eval()\n",
    "    use_crnn = True\n",
    "    print(\"Loaded CRNN.\")\n",
    "except Exception as e:\n",
    "    print(\"CRNN not loaded, will use tesseract as fallback.\", e)\n",
    "    crnn = None\n",
    "\n",
    "CHARSET = \"0123456789\"\n",
    "BLANK_IDX = len(CHARSET)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def crnn_recognize(pil_img):\n",
    "    # expects grayscale PIL image of reasonable height (~32)\n",
    "    img = pil_img.convert(\"L\")\n",
    "    h = 32\n",
    "    w = int(img.width * (h / img.height))\n",
    "    img = img.resize((w, h), Image.BILINEAR)\n",
    "    arr = np.array(img).astype(np.float32) / 255.0\n",
    "    tensor = torch.from_numpy(arr).unsqueeze(0).unsqueeze(0)  # b x c x h x w\n",
    "    # pad to max width used during training (assume 256)\n",
    "    maxw = 256\n",
    "    if tensor.size(3) < maxw:\n",
    "        pad = torch.zeros((1,1,h,maxw - tensor.size(3)))\n",
    "        tensor = torch.cat([tensor, pad], dim=3)\n",
    "    else:\n",
    "        tensor = tensor[:, :, :, :maxw]\n",
    "    tensor = (tensor - 0.5) / 0.5\n",
    "    tensor = tensor.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        preds = crnn(tensor)  # seq x b x nclass\n",
    "        preds = F.log_softmax(preds, dim=2)\n",
    "        preds = preds.cpu().numpy()\n",
    "    # greedy decode\n",
    "    seq = np.argmax(preds, axis=2)[:,0]  # seq_len\n",
    "    # collapse duplicates and remove blanks\n",
    "    s = []\n",
    "    prev = -1\n",
    "    for ch in seq:\n",
    "        if ch != prev and ch != BLANK_IDX:\n",
    "            if ch < len(CHARSET):\n",
    "                s.append(CHARSET[ch])\n",
    "        prev = ch\n",
    "    return \"\".join(s)\n",
    "\n",
    "def tesseract_recognize(pil_img):\n",
    "    # restrict to digits\n",
    "    config = \"--psm 7 -c tessedit_char_whitelist=0123456789\"\n",
    "    txt = pytesseract.image_to_string(pil_img, config=config)\n",
    "    # cleanup\n",
    "    digits = \"\".join([c for c in txt if c.isdigit()])\n",
    "    return digits\n",
    "\n",
    "def normalize_digits(s):\n",
    "    s2 = \"\".join([c for c in s if c.isdigit()])\n",
    "    if len(s2) == 12:\n",
    "        return s2\n",
    "    # try to split if spaces present\n",
    "    parts = s.split()\n",
    "    joined = \"\".join([p for p in parts if p.isdigit()])\n",
    "    if len(joined) == 12:\n",
    "        return joined\n",
    "    return s2\n",
    "\n",
    "def process_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    results = detector(img_path)  # or detector(img) ; using path is fine\n",
    "    outputs = []\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            conf = float(box.conf[0])\n",
    "            crop = img.crop((x1, y1, x2, y2)).convert(\"RGB\")\n",
    "            # try CRNN first\n",
    "            digits = \"\"\n",
    "            if use_crnn:\n",
    "                try:\n",
    "                    digits = crnn_recognize(crop)\n",
    "                except Exception:\n",
    "                    digits = \"\"\n",
    "            if not digits:\n",
    "                digits = tesseract_recognize(crop)\n",
    "            digits = normalize_digits(digits)\n",
    "            valid = len(digits) == 12 and verhoeff_check(digits)\n",
    "            outputs.append({\n",
    "                \"digits\": digits,\n",
    "                \"valid\": valid,\n",
    "                \"conf\": conf,\n",
    "                \"box\": [x1,y1,x2,y2]\n",
    "            })\n",
    "    numbers = [o[\"digits\"] for o in outputs if o[\"valid\"]]\n",
    "    print(\"\\n\".join(numbers))\n",
    "    return outputs\n",
    "test_image = \"C:/Users/souna/OneDrive/Desktop/LLM/project/test/images/1b26b406ad9485c98cd496bc371db57f_jpg.rf.3d95677f5daed847c3d4477325e99ac6.jpg\"\n",
    "results = process_image(test_image)\n",
    "results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c1e7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.2.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting mlflow-tracing==3.2.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Using cached flask-3.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow) (3.10.5)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow) (2.2.6)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow) (2.3.1)\n",
      "Collecting pyarrow<22,>=4.0.0 (from mlflow)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting scikit-learn<2 (from mlflow)\n",
      "  Downloading scikit_learn-1.7.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow) (1.16.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow) (2.0.41)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (8.2.1)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading databricks_sdk-0.62.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (24.2)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (2.11.7)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (2.32.4)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mlflow-skinny==3.2.0->mlflow) (4.14.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docker<8,>=4.0.0->mlflow) (311)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click<9,>=7.0->mlflow-skinny==3.2.0->mlflow) (0.4.6)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.2.0->mlflow) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.2.0->mlflow) (2025.6.15)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from uvicorn<1->mlflow-skinny==3.2.0->mlflow) (0.16.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.2.0->mlflow)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\souna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.2.0->mlflow) (1.3.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.2.0->mlflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading mlflow-3.2.0-py3-none-any.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.8 MB 6.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.1/25.8 MB 6.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.5/25.8 MB 8.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.8/25.8 MB 9.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/25.8 MB 11.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.0/25.8 MB 14.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.4/25.8 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading mlflow_skinny-3.2.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 36.2 MB/s eta 0:00:00\n",
      "Downloading mlflow_tracing-3.2.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 9.2/26.2 MB 40.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 43.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.2 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 35.3 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.1-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 7.9/8.7 MB 37.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 33.9 MB/s eta 0:00:00\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading databricks_sdk-0.62.0-py3-none-any.whl (681 kB)\n",
      "   ---------------------------------------- 0.0/681.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 681.8/681.8 kB 13.5 MB/s eta 0:00:00\n",
      "Using cached fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Using cached protobuf-6.31.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: zipp, werkzeug, waitress, threadpoolctl, sqlparse, smmap, pyasn1, pyarrow, protobuf, Mako, joblib, itsdangerous, graphql-core, cloudpickle, cachetools, blinker, uvicorn, starlette, scikit-learn, rsa, pyasn1-modules, importlib_metadata, graphql-relay, gitdb, Flask, docker, alembic, opentelemetry-api, graphene, google-auth, gitpython, fastapi, opentelemetry-semantic-conventions, databricks-sdk, opentelemetry-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-3.1.1 Mako-1.3.10 alembic-1.16.4 blinker-1.9.0 cachetools-5.5.2 cloudpickle-3.1.1 databricks-sdk-0.62.0 docker-7.1.0 fastapi-0.116.1 gitdb-4.0.12 gitpython-3.1.45 google-auth-2.40.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 importlib_metadata-8.7.0 itsdangerous-2.2.0 joblib-1.5.1 mlflow-3.2.0 mlflow-skinny-3.2.0 mlflow-tracing-3.2.0 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 protobuf-6.31.1 pyarrow-21.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 scikit-learn-1.7.1 smmap-5.0.2 sqlparse-0.5.3 starlette-0.47.2 threadpoolctl-3.6.0 uvicorn-0.35.0 waitress-3.0.2 werkzeug-3.1.3 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\souna\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f21e325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.10.6 ms, read: 2.00.2 MB/s, size: 36.2 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.10.6 ms, read: 2.00.2 MB/s, size: 36.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\train\\labels.cache... 1182 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1182/1182 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/74 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [03:17<00:00,  2.67s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1182       9397     0.0181     0.0171    0.00982    0.00259\n",
      "Speed: 1.1ms preprocess, 131.5ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Speed: 1.1ms preprocess, 131.5ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Train metrics: {'precision': np.float64(0.01808088321709802), 'recall': np.float64(0.017113279650440873), 'map50': np.float64(0.009817033270377466), 'map50_95': np.float64(0.0025935269940803307)}\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Train metrics: {'precision': np.float64(0.01808088321709802), 'recall': np.float64(0.017113279650440873), 'map50': np.float64(0.009817033270377466), 'map50_95': np.float64(0.0025935269940803307)}\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 1.60.5 ms, read: 2.70.5 MB/s, size: 37.1 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 1.60.5 ms, read: 2.70.5 MB/s, size: 37.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:19<00:00,  2.76s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879     0.0162      0.013    0.00961    0.00269\n",
      "Speed: 1.1ms preprocess, 131.2ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Speed: 1.1ms preprocess, 131.2ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "Val metrics: {'precision': np.float64(0.016199583741016182), 'recall': np.float64(0.013039878113407528), 'map50': np.float64(0.009614834963060868), 'map50_95': np.float64(0.002685688953254833)}\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Val metrics: {'precision': np.float64(0.016199583741016182), 'recall': np.float64(0.013039878113407528), 'map50': np.float64(0.009614834963060868), 'map50_95': np.float64(0.002685688953254833)}\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.30.8 ms, read: 2.00.1 MB/s, size: 36.7 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.30.8 ms, read: 2.00.1 MB/s, size: 36.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\labels... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<00:00, 135.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:08<00:00,  2.23s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55        433     0.0156    0.00865     0.0121    0.00261\n",
      "Speed: 1.8ms preprocess, 140.1ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n",
      "Speed: 1.8ms preprocess, 140.1ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n",
      "Test metrics: {'precision': np.float64(0.015632415632415634), 'recall': np.float64(0.008647561588738058), 'map50': np.float64(0.012059932072282073), 'map50_95': np.float64(0.0026070319902997724)}\n",
      "Test metrics: {'precision': np.float64(0.015632415632415634), 'recall': np.float64(0.008647561588738058), 'map50': np.float64(0.012059932072282073), 'map50_95': np.float64(0.0026070319902997724)}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import mlflow\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "\n",
    "# ====== 1. Load YOLOv8 model ======\n",
    "model_path = \"yolov8n.pt\"  # your trained weights\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# ====== 2. Define evaluation function ======\n",
    "def evaluate_yolo(model, data_path, split=\"val\"):\n",
    "    # Run YOLO validation\n",
    "    results = model.val(data=data_path, split=split, save=False, verbose=False)\n",
    "    \n",
    "    # Extract built-in metrics\n",
    "    metrics = {\n",
    "        \"precision\": results.results_dict[\"metrics/precision(B)\"],\n",
    "        \"recall\": results.results_dict[\"metrics/recall(B)\"],\n",
    "        \"map50\": results.results_dict[\"metrics/mAP50(B)\"],\n",
    "        \"map50_95\": results.results_dict[\"metrics/mAP50-95(B)\"]\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ====== 3. Log to MLflow ======\n",
    "data_yaml = \"data.yaml\"  # path to your dataset config\n",
    "with mlflow.start_run(run_name=\"YOLOv8_Evaluation\"):\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        metrics = evaluate_yolo(model, data_yaml, split=split)\n",
    "        for k, v in metrics.items():\n",
    "            mlflow.log_metric(f\"{split}_{k}\", v)\n",
    "        print(f\"{split.capitalize()} metrics:\", metrics)\n",
    "\n",
    "    # Save model to MLflow\n",
    "    mlflow.log_artifact(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f47d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Model summary (fused): 72 layers, 3,008,573 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Model summary (fused): 72 layers, 3,008,573 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.1 ms, read: 92.312.9 MB/s, size: 36.4 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.1 ms, read: 92.312.9 MB/s, size: 36.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\train\\labels.cache... 1182 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1182/1182 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/74 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 74/74 [01:03<00:00,  1.17it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1182       9397       0.69      0.769      0.732      0.628\n",
      "Speed: 0.5ms preprocess, 47.7ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n",
      "Speed: 0.5ms preprocess, 47.7ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.90.6 ms, read: 71.810.4 MB/s, size: 40.8 KB)\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.90.6 ms, read: 71.810.4 MB/s, size: 40.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.10it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.635      0.768      0.685      0.549\n",
      "Speed: 0.6ms preprocess, 48.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "Speed: 0.6ms preprocess, 48.9ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val9\u001b[0m\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.80.7 ms, read: 58.724.3 MB/s, size: 39.3 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.80.7 ms, read: 58.724.3 MB/s, size: 39.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\labels.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\labels.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55        433      0.467      0.994      0.681      0.547\n",
      "Speed: 0.6ms preprocess, 52.4ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val10\u001b[0m\n",
      "Speed: 0.6ms preprocess, 52.4ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val10\u001b[0m\n",
      "\n",
      "YOLOv8 Evaluation Results:\n",
      "\n",
      "+---------------+-----------+--------+--------+-----------+\n",
      "| Dataset Split | Precision | Recall | mAP@50 | mAP@50-95 |\n",
      "+---------------+-----------+--------+--------+-----------+\n",
      "|     Train     |  0.6897   | 0.7691 | 0.7322 |  0.6276   |\n",
      "|      Val      |  0.6352   | 0.768  | 0.6851 |  0.5493   |\n",
      "|     Test      |  0.4673   | 0.9936 | 0.6809 |  0.5473   |\n",
      "+---------------+-----------+--------+--------+-----------+\n",
      "\n",
      "YOLOv8 Evaluation Results:\n",
      "\n",
      "+---------------+-----------+--------+--------+-----------+\n",
      "| Dataset Split | Precision | Recall | mAP@50 | mAP@50-95 |\n",
      "+---------------+-----------+--------+--------+-----------+\n",
      "|     Train     |  0.6897   | 0.7691 | 0.7322 |  0.6276   |\n",
      "|      Val      |  0.6352   | 0.768  | 0.6851 |  0.5493   |\n",
      "|     Test      |  0.4673   | 0.9936 | 0.6809 |  0.5473   |\n",
      "+---------------+-----------+--------+--------+-----------+\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import mlflow\n",
    "from tabulate import tabulate  # For pretty printing\n",
    "\n",
    "# ====== 1. Load YOLOv8 model ======\n",
    "model_path = \"C:/Users/souna/OneDrive/Desktop/LLM/project/runs/detect/aadhaar_detect/weights/best.pt\"  # your trained weights\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# ====== 2. Define evaluation function ======\n",
    "def evaluate_yolo(model, data_path, split=\"val\"):\n",
    "    results = model.val(data=data_path, split=split, save=False, verbose=False)\n",
    "    metrics = {\n",
    "        \"Precision\": results.results_dict[\"metrics/precision(B)\"],\n",
    "        \"Recall\": results.results_dict[\"metrics/recall(B)\"],\n",
    "        \"mAP_50\": results.results_dict[\"metrics/mAP50(B)\"],       # replaced @ with _\n",
    "        \"mAP_50_95\": results.results_dict[\"metrics/mAP50-95(B)\"]   # replaced @ with _\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ====== 3. Log to MLflow and print nicely ======\n",
    "data_yaml = \"data.yaml\"  # path to your dataset config\n",
    "\n",
    "with mlflow.start_run(run_name=\"YOLOv8_Evaluation\"):\n",
    "    all_results = []\n",
    "    \n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        metrics = evaluate_yolo(model, data_yaml, split=split)\n",
    "        \n",
    "        # Log metrics to MLflow (safe names)\n",
    "        for k, v in metrics.items():\n",
    "            safe_key = f\"{split}_{k}\".replace(\"@\", \"_\").replace(\"-\", \"_\")\n",
    "            mlflow.log_metric(safe_key, v)\n",
    "        \n",
    "        # Store results for table\n",
    "        row = [split.capitalize()] + [round(v, 4) for v in metrics.values()]\n",
    "        all_results.append(row)\n",
    "    \n",
    "    # Print results as a table\n",
    "    headers = [\"Dataset Split\", \"Precision\", \"Recall\", \"mAP@50\", \"mAP@50-95\"]\n",
    "    print(\"\\nYOLOv8 Evaluation Results:\\n\")\n",
    "    print(tabulate(all_results, headers=headers, tablefmt=\"pretty\"))\n",
    "    \n",
    "    # Save model to MLflow\n",
    "    mlflow.log_artifact(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef786640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "model = YOLO(\"runs/detect/aadhaar_detect/weights/best.pt\")\n",
    "\n",
    "img = \"train/images/Real_Aadhar_28_jpg.rf.e75dee4c146ea1b0a4e20266297b4581.jpg\"\n",
    "res = model(img, conf=0.05, verbose=False)   # low conf to see candidates\n",
    "boxes_pred = res[0].boxes.cpu().numpy()      # ultralytics returns handy objects\n",
    "# draw predicted boxes and ground-truth boxes (load from .txt) for this image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4da08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/12 12:51:40 INFO mlflow.tracking.fluent: Experiment with name 'yolo_threshold_sweep' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with conf=0.05 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "Model summary (fused): 72 layers, 3,008,573 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 2.30.4 MB/s, size: 35.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769      0.685      0.562\n",
      "Speed: 0.5ms preprocess, 44.2ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val11\u001b[0m\n",
      "{'precision': 0.640521027680651, 'recall': 0.7689083309011053, 'mAP50': 0.6848245755036737, 'mAP50_95': None, 'conf': np.float64(0.05)}\n",
      "Evaluating with conf=0.10 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 425.373.3 MB/s, size: 38.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769      0.683      0.561\n",
      "Speed: 0.6ms preprocess, 43.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val12\u001b[0m\n",
      "{'precision': 0.640521027680651, 'recall': 0.7689083309011053, 'mAP50': 0.6832201112686611, 'mAP50_95': None, 'conf': np.float64(0.1)}\n",
      "Evaluating with conf=0.15 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 456.197.4 MB/s, size: 42.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769      0.683      0.561\n",
      "Speed: 0.5ms preprocess, 45.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val13\u001b[0m\n",
      "{'precision': 0.640521027680651, 'recall': 0.7689083309011053, 'mAP50': 0.6825361154842823, 'mAP50_95': None, 'conf': np.float64(0.15)}\n",
      "Evaluating with conf=0.20 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 364.2116.3 MB/s, size: 37.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769       0.68      0.559\n",
      "Speed: 0.6ms preprocess, 46.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val14\u001b[0m\n",
      "{'precision': 0.6413386552835495, 'recall': 0.7689083309011053, 'mAP50': 0.679595158384919, 'mAP50_95': None, 'conf': np.float64(0.2)}\n",
      "Evaluating with conf=0.25 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 455.952.9 MB/s, size: 39.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769      0.674      0.555\n",
      "Speed: 0.6ms preprocess, 48.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val15\u001b[0m\n",
      "{'precision': 0.6413386552835495, 'recall': 0.7689083309011053, 'mAP50': 0.6740653358964569, 'mAP50_95': None, 'conf': np.float64(0.25)}\n",
      "Evaluating with conf=0.30 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 474.160.2 MB/s, size: 40.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.641      0.769      0.661      0.545\n",
      "Speed: 0.6ms preprocess, 46.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val16\u001b[0m\n",
      "{'precision': 0.6413386552835495, 'recall': 0.7689083309011053, 'mAP50': 0.6610101668807632, 'mAP50_95': None, 'conf': np.float64(0.3)}\n",
      "Evaluating with conf=0.35 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 490.665.0 MB/s, size: 43.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.618       0.79      0.646      0.538\n",
      "Speed: 0.5ms preprocess, 47.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val17\u001b[0m\n",
      "{'precision': 0.6183256623129662, 'recall': 0.790379237040811, 'mAP50': 0.6462128281832615, 'mAP50_95': None, 'conf': np.float64(0.35)}\n",
      "Evaluating with conf=0.40 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 440.3141.3 MB/s, size: 37.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.662      0.731      0.635      0.531\n",
      "Speed: 0.6ms preprocess, 48.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val18\u001b[0m\n",
      "{'precision': 0.661944330560439, 'recall': 0.7305067745731055, 'mAP50': 0.6346789608802405, 'mAP50_95': None, 'conf': np.float64(0.39999999999999997)}\n",
      "Evaluating with conf=0.45 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 445.9107.0 MB/s, size: 42.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.676      0.681       0.62      0.521\n",
      "Speed: 0.6ms preprocess, 47.8ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val19\u001b[0m\n",
      "{'precision': 0.6759646218667881, 'recall': 0.6811363759611172, 'mAP50': 0.6197024369282885, 'mAP50_95': None, 'conf': np.float64(0.44999999999999996)}\n",
      "Evaluating with conf=0.50 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 551.9194.0 MB/s, size: 49.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.695      0.655      0.613       0.52\n",
      "Speed: 0.6ms preprocess, 52.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val20\u001b[0m\n",
      "{'precision': 0.6954015933793848, 'recall': 0.6553069398920476, 'mAP50': 0.6131304026869628, 'mAP50_95': None, 'conf': np.float64(0.49999999999999994)}\n",
      "Evaluating with conf=0.55 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 534.5107.1 MB/s, size: 40.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:16<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.693      0.605      0.592      0.504\n",
      "Speed: 1.1ms preprocess, 136.9ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val21\u001b[0m\n",
      "{'precision': 0.6932680175495017, 'recall': 0.6046345807799262, 'mAP50': 0.5917643395654513, 'mAP50_95': None, 'conf': np.float64(0.5499999999999999)}\n",
      "Evaluating with conf=0.60 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 185.129.0 MB/s, size: 39.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:16<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.675      0.527      0.555      0.476\n",
      "Speed: 1.1ms preprocess, 136.2ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val22\u001b[0m\n",
      "{'precision': 0.6746300119218867, 'recall': 0.5272540041488571, 'mAP50': 0.5549465065843916, 'mAP50_95': None, 'conf': np.float64(0.6)}\n",
      "Evaluating with conf=0.65 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 108.138.2 MB/s, size: 34.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:12<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879       0.65      0.414      0.503      0.432\n",
      "Speed: 1.0ms preprocess, 106.2ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val23\u001b[0m\n",
      "{'precision': 0.6501249220036728, 'recall': 0.4135585126354592, 'mAP50': 0.5032751029213914, 'mAP50_95': None, 'conf': np.float64(0.65)}\n",
      "Evaluating with conf=0.70 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 458.584.1 MB/s, size: 38.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.651      0.289      0.459      0.397\n",
      "Speed: 0.7ms preprocess, 45.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val24\u001b[0m\n",
      "{'precision': 0.6514524707623679, 'recall': 0.2894378046545387, 'mAP50': 0.4592519566685952, 'mAP50_95': None, 'conf': np.float64(0.7)}\n",
      "Evaluating with conf=0.75 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 418.857.9 MB/s, size: 38.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.565      0.183      0.377      0.328\n",
      "Speed: 0.6ms preprocess, 47.1ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val25\u001b[0m\n",
      "{'precision': 0.5648283295651717, 'recall': 0.18297060636482096, 'mAP50': 0.3770590932397459, 'mAP50_95': None, 'conf': np.float64(0.75)}\n",
      "Evaluating with conf=0.80 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 395.4182.2 MB/s, size: 42.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.389      0.144      0.269      0.233\n",
      "Speed: 0.5ms preprocess, 46.6ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val26\u001b[0m\n",
      "{'precision': 0.3894736842105263, 'recall': 0.14367615774832268, 'mAP50': 0.2693250412449393, 'mAP50_95': None, 'conf': np.float64(0.7999999999999999)}\n",
      "Evaluating with conf=0.85 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 502.182.1 MB/s, size: 39.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.123      0.133      0.132      0.114\n",
      "Speed: 0.6ms preprocess, 48.3ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val27\u001b[0m\n",
      "{'precision': 0.12280701754385964, 'recall': 0.13333333333333333, 'mAP50': 0.13203508771929826, 'mAP50_95': None, 'conf': np.float64(0.85)}\n",
      "Evaluating with conf=0.90 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 491.3118.5 MB/s, size: 42.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.133      0.103      0.118      0.104\n",
      "Speed: 0.5ms preprocess, 51.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val28\u001b[0m\n",
      "{'precision': 0.13333333333333333, 'recall': 0.10250000000000001, 'mAP50': 0.11790666666666667, 'mAP50_95': None, 'conf': np.float64(0.9)}\n",
      "Evaluating with conf=0.95 ... Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.1 ms, read: 144.633.2 MB/s, size: 40.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\valid\\labels.cache... 112 images, 0 backgrounds, 0 corrupt: 100%|██████████| 112/112 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:16<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        112        879      0.133     0.0217     0.0775     0.0715\n",
      "Speed: 1.1ms preprocess, 135.9ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val29\u001b[0m\n",
      "{'precision': 0.13333333333333333, 'recall': 0.021666666666666667, 'mAP50': 0.07749904761904762, 'mAP50_95': None, 'conf': np.float64(0.95)}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.12.10 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i5-12450H)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.40.3 ms, read: 2.50.5 MB/s, size: 36.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\souna\\OneDrive\\Desktop\\LLM\\project\\test\\labels.cache... 55 images, 0 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<?, ?it/s]\n",
      "C:\\Users\\souna\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:09<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55        433       0.61      0.604      0.608      0.505\n",
      "Speed: 1.3ms preprocess, 142.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val30\u001b[0m\n",
      "test metrics: {'precision': 0.6099479215655239, 'recall': 0.6039397108625744, 'mAP50': 0.6078928005509905, 'mAP50_95': None}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - setup\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.tracking\n",
    "import time\n",
    "\n",
    "# Adjust paths/names\n",
    "YOLO_WEIGHTS = \"runs/detect/aadhaar_detect/weights/best.pt\"\n",
    "DATA_YAML = \"data.yaml\"   # must define train/val/test paths and nc,names\n",
    "EXPERIMENT_NAME = \"yolo_threshold_sweep\"\n",
    "# Cell 2 - model + helper\n",
    "model = YOLO(YOLO_WEIGHTS)\n",
    "\n",
    "def extract_metrics_from_val_result(val_result):\n",
    "    # val_result is Ultralytics Results object; it has results_dict in many versions\n",
    "    rd = getattr(val_result, \"results_dict\", None)\n",
    "    if rd is None:\n",
    "        # fallback: use val_result.metrics if present\n",
    "        rd = getattr(val_result, \"metrics\", None) or {}\n",
    "    # keys vary across ultralytics versions; try common ones\n",
    "    def get(k):\n",
    "        return rd.get(k) if isinstance(rd, dict) else None\n",
    "\n",
    "    # different Ultralytics versions store keys differently. handle common cases:\n",
    "    precision = None\n",
    "    recall = None\n",
    "    map50 = None\n",
    "    map50_95 = None\n",
    "\n",
    "    # try several key names:\n",
    "    for k in [\"metrics/precision(B)\",\"metrics/precision\", \"precision\"]:\n",
    "        v = get(k)\n",
    "        if v is not None:\n",
    "            precision = float(v)\n",
    "            break\n",
    "    for k in [\"metrics/recall(B)\",\"metrics/recall\", \"recall\"]:\n",
    "        v = get(k)\n",
    "        if v is not None:\n",
    "            recall = float(v)\n",
    "            break\n",
    "    for k in [\"metrics/mAP50(B)\",\"metrics/mAP50\", \"map50\", \"mAP50\"]:\n",
    "        v = get(k)\n",
    "        if v is not None:\n",
    "            map50 = float(v)\n",
    "            break\n",
    "    for k in [\"metrics/mAP(B)\",\"metrics/mAP\",\"map\",\"mAP\"]:\n",
    "        v = get(k)\n",
    "        if v is not None:\n",
    "            map50_95 = float(v)\n",
    "            break\n",
    "\n",
    "    # If still None, try val_result.box or attributes\n",
    "    # As fallback try attributes common on Results\n",
    "    if precision is None and hasattr(val_result, \"boxes\"):\n",
    "        # best-effort; leave None otherwise\n",
    "        pass\n",
    "\n",
    "    return dict(precision=precision, recall=recall, mAP50=map50, mAP50_95=map50_95)\n",
    "\n",
    "\n",
    "# Cell 3 - sweep and log\n",
    "thresholds = list(np.linspace(0.05, 0.95, 19))  # try 0.05,0.1,...0.95\n",
    "rows = []\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "with mlflow.start_run(run_name=f\"threshold_sweep_{int(time.time())}\"):\n",
    "    mlflow.log_param(\"yolo_weights\", YOLO_WEIGHTS)\n",
    "    mlflow.log_param(\"data_yaml\", DATA_YAML)\n",
    "\n",
    "    for conf in thresholds:\n",
    "        print(f\"Evaluating with conf={conf:.2f} ...\", end=\" \", flush=True)\n",
    "        # model.val supports conf keyword in ultralytics API\n",
    "        # split param defaults to validation - but you can pass split=\"val\"/\"test\"/\"train\"\n",
    "        val_res = model.val(data=DATA_YAML, conf=conf, split=\"val\", verbose=False)\n",
    "        metrics = extract_metrics_from_val_result(val_res)\n",
    "        metrics[\"conf\"] = conf\n",
    "        rows.append(metrics)\n",
    "        print(metrics)\n",
    "        # log metrics per threshold\n",
    "        mlflow.log_metric(\"val_precision\", float(metrics[\"precision\"]) if metrics[\"precision\"] is not None else math.nan, step=int(conf*100))\n",
    "        mlflow.log_metric(\"val_recall\", float(metrics[\"recall\"]) if metrics[\"recall\"] is not None else math.nan, step=int(conf*100))\n",
    "        if metrics[\"mAP50\"] is not None:\n",
    "            mlflow.log_metric(\"val_map50\", float(metrics[\"mAP50\"]), step=int(conf*100))\n",
    "        if metrics[\"mAP50_95\"] is not None:\n",
    "            mlflow.log_metric(\"val_map50_95\", float(metrics[\"mAP50_95\"]), step=int(conf*100))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.set_index(\"conf\", inplace=True)\n",
    "df\n",
    "# Cell 4 - plot\n",
    "plt.figure(figsize=(10,6))\n",
    "if \"precision\" in df.columns:\n",
    "    plt.plot(df.index, df[\"precision\"], label=\"Precision\", marker='o')\n",
    "if \"recall\" in df.columns:\n",
    "    plt.plot(df.index, df[\"recall\"], label=\"Recall\", marker='o')\n",
    "if \"mAP50\" in df.columns:\n",
    "    plt.plot(df.index, df[\"mAP50\"], label=\"mAP@50\", marker='o')\n",
    "if \"mAP50_95\" in df.columns:\n",
    "    plt.plot(df.index, df[\"mAP50_95\"], label=\"mAP@50-95\", marker='o')\n",
    "\n",
    "plt.xlabel(\"Confidence threshold\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.title(\"YOLO detection metrics vs confidence threshold (val split)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# quick single threshold eval on test set\n",
    "test_res = model.val(data=DATA_YAML, conf=0.5, split=\"test\", verbose=False)\n",
    "print(\"test metrics:\", extract_metrics_from_val_result(test_res))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
