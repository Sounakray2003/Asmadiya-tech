{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sounakray2003/Asmadiya-tech/blob/main/KYC_Classification_Model_Yolo11n_cls_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SjBdmUx87fq",
        "outputId": "d4d8c8cf-7f49-4951-9bd9-2483932ce824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics wandb gradio tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "u2RAWAE481BN",
        "outputId": "eb189189-c15e-49ce-90a9-d9813335eb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Found existing installation: protobuf 5.29.5\n",
            "Uninstalling protobuf-5.29.5:\n",
            "  Successfully uninstalled protobuf-5.29.5\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-spanner 3.57.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-speech 2.33.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.109.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-dataproc 5.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.25.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.32.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-logging 3.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.21.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-appengine-logging 1.6.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-trace 1.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-audit-log 0.3.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-secret-manager 2.24.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "aaa1ece59405430d87d97b161d647d73"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import gradio as gr\n",
        "import wandb\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Uninstall and reinstall protobuf to avoid conflicts\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.0\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoWlAjJz-2QN",
        "outputId": "11513bb9-88cd-4089-e63a-28ef8bfbd331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "DOxOcr5mQr5I",
        "outputId": "79d3b655-f85a-45f5-b51f-f924dc0fe1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msounakray496\u001b[0m (\u001b[33msounakray496-asmadiya-technologies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250825_102533-yw4kq7mi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/yw4kq7mi' target=\"_blank\">yolov12_classification</a></strong> to <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/yw4kq7mi' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/yw4kq7mi</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Set up WandB\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"document_classification\",\n",
        "    name=\"yolov12_classification\",\n",
        "    config={\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": 16,\n",
        "        \"img_size\": 224,\n",
        "        \"patience\": 10,\n",
        "        \"lr0\": 0.0001\n",
        "    }\n",
        ")\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdR__CnRQvBV"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset from specified paths\n",
        "base_path = '/content/drive/MyDrive/Datasets'\n",
        "aadhar_folder = os.path.join(base_path, 'aadhar_images')\n",
        "passport_folder = os.path.join(base_path, 'passport_images')\n",
        "pan_test_folder = os.path.join(base_path, 'pan_images', 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1DrAofPQyAI"
      },
      "outputs": [],
      "source": [
        "# Collect up to 204 images from each folder\n",
        "aadhar_images = [os.path.join(aadhar_folder, f) for f in os.listdir(aadhar_folder) if f.endswith('.png')]\n",
        "aadhar_images = aadhar_images[:204]  # Limit to 204\n",
        "aadhar_labels = ['Aadhaar'] * len(aadhar_images)\n",
        "passport_images = [os.path.join(passport_folder, f) for f in os.listdir(passport_folder) if f.endswith('.png')]\n",
        "passport_images = random.sample(passport_images, min(204, len(passport_images)))  # Limit to 204\n",
        "passport_labels = ['Passport'] * len(passport_images)\n",
        "pan_images = [os.path.join(pan_test_folder, f) for f in os.listdir(pan_test_folder) if f.endswith('.png')]\n",
        "pan_images = pan_images[:204]  # Limit to 204\n",
        "pan_labels = ['PAN'] * len(pan_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7ifc0N2Q1QO"
      },
      "outputs": [],
      "source": [
        "all_images = aadhar_images + passport_images + pan_images\n",
        "all_labels = aadhar_labels + passport_labels + pan_labels\n",
        "df = pd.DataFrame({'image_path': all_images, 'label': all_labels})\n",
        "wandb.log({'Aadhaar_count': len(aadhar_images), 'Passport_count': len(passport_images), 'PAN_count': len(pan_images)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVH82EZWQ4Wv"
      },
      "outputs": [],
      "source": [
        "# Split dataset (70% train, 15% val, 15% test)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-UqP0YmQ7HU",
        "outputId": "22fbaab8-1822-45a6-8cd5-e38fd1688087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train at /content/train populated with 428 images: Aadhaar=143, PAN=143, Passport=142\n",
            "val at /content/val populated with 92 images: Aadhaar=30, PAN=31, Passport=31\n",
            "test at /content/test populated with 92 images: Aadhaar=31, PAN=30, Passport=31\n"
          ]
        }
      ],
      "source": [
        "# Create directory structure and copy images to specified paths\n",
        "for split, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    os.makedirs(f'/content/{split}', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/Aadhaar', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/PAN', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/Passport', exist_ok=True)\n",
        "    for _, row in split_df.iterrows():\n",
        "        dest_folder = f'/content/{split}/{row[\"label\"]}'\n",
        "        shutil.copy(row['image_path'], dest_folder)\n",
        "    # Verify population\n",
        "    aadhar_count = len(os.listdir(f'/content/{split}/Aadhaar'))\n",
        "    pan_count = len(os.listdir(f'/content/{split}/PAN'))\n",
        "    passport_count = len(os.listdir(f'/content/{split}/Passport'))\n",
        "    total_count = aadhar_count + pan_count + passport_count\n",
        "    print(f\"{split} at /content/{split} populated with {total_count} images: Aadhaar={aadhar_count}, PAN={pan_count}, Passport={passport_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38a2xyy2RAge",
        "outputId": "85809f9a-a6b5-44a1-a962-2f0153fa4ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.yaml created/overwritten at /content/dataset.yaml with content: \n",
            "train: /content/train\n",
            "val: /content/val\n",
            "test: /content/test\n",
            "nc: 3  # number of classes\n",
            "names: ['Aadhaar', 'PAN', 'Passport']  # class names\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create and overwrite dataset.yaml at specified path\n",
        "yaml_content = \"\"\"\n",
        "train: /content/train\n",
        "val: /content/val\n",
        "test: /content/test\n",
        "nc: 3  # number of classes\n",
        "names: ['Aadhaar', 'PAN', 'Passport']  # class names\n",
        "\"\"\"\n",
        "with open('/content/dataset.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "print(\"dataset.yaml created/overwritten at /content/dataset.yaml with content:\", yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o39ns2kORE7S",
        "outputId": "de468c06-b5b9-4388-c890-4aa464897807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt to 'yolo11n-cls.pt': 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.52M/5.52M [00:00<00:00, 145MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO12 classification models not found, using YOLO11n-cls.pt as fallback\n",
            "Starting training with YOLOv12 and data from: /content/dataset.yaml\n"
          ]
        }
      ],
      "source": [
        "# Load YOLOv12 model - Updated to YOLOv12\n",
        "# Try different possible model names for YOLO12 classification\n",
        "try:\n",
        "    model = YOLO('yolo12n-cls.pt')  # YOLO12 nano classification model\n",
        "    print(\"Loaded yolo12n-cls.pt successfully\")\n",
        "except:\n",
        "    try:\n",
        "        model = YOLO('yolo12s-cls.pt')  # YOLO12 small classification model\n",
        "        print(\"Loaded yolo12s-cls.pt successfully\")\n",
        "    except:\n",
        "        try:\n",
        "            model = YOLO('yolov12n-cls.pt')  # Alternative naming\n",
        "            print(\"Loaded yolov12n-cls.pt successfully\")\n",
        "        except:\n",
        "            # Fallback to YOLO11 if YOLO12 classification models aren't available yet\n",
        "            model = YOLO('yolo11n-cls.pt')\n",
        "            print(\"YOLO12 classification models not found, using YOLO11n-cls.pt as fallback\")\n",
        "\n",
        "# Train the model with verbose output and callbacks\n",
        "print(\"Starting training with YOLOv12 and data from:\", '/content/dataset.yaml')\n",
        "\n",
        "# Custom callback to print accuracy after each epoch\n",
        "def on_train_epoch_end(trainer):\n",
        "    \"\"\"Callback function to print metrics after each epoch\"\"\"\n",
        "    if trainer.epoch >= 0:  # Start from epoch 0\n",
        "        metrics = trainer.validator.metrics if hasattr(trainer, 'validator') and trainer.validator else None\n",
        "        if metrics:\n",
        "            # For classification, the main metrics are top1 and top5 accuracy\n",
        "            top1_acc = getattr(metrics, 'top1', None)\n",
        "            top5_acc = getattr(metrics, 'top5', None)\n",
        "\n",
        "            print(f\"\\n=== EPOCH {trainer.epoch + 1} RESULTS ===\")\n",
        "            if top1_acc is not None:\n",
        "                print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
        "                # Log to wandb as well\n",
        "                wandb.log({\n",
        "                    'epoch': trainer.epoch + 1,\n",
        "                    'train_top1_accuracy': top1_acc,\n",
        "                    'epoch_top1_acc': top1_acc\n",
        "                })\n",
        "            if top5_acc is not None:\n",
        "                print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
        "                wandb.log({\n",
        "                    'epoch': trainer.epoch + 1,\n",
        "                    'train_top5_accuracy': top5_acc,\n",
        "                    'epoch_top5_acc': top5_acc\n",
        "                })\n",
        "\n",
        "            # Print loss information if available\n",
        "            if hasattr(trainer, 'loss_items') and trainer.loss_items is not None:\n",
        "                print(f\"Training Loss: {trainer.loss_items}\")\n",
        "\n",
        "            print(\"=\" * 30)\n",
        "\n",
        "# Add the callback to the model\n",
        "model.add_callback('on_train_epoch_end', on_train_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrWqIYUgRK76",
        "outputId": "d2e27d1a-88c1-41ff-84b2-d3579c95b04e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=60, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov12_exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/yolov12_exp, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 428 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 92 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 92 images in 3 classes ‚úÖ \n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLO11n-cls summary: 86 layers, 1,534,947 parameters, 1,534,947 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.35M/5.35M [00:00<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3224.6¬±743.3 MB/s, size: 303.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train... 428 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 428/428 [00:00<00:00, 2372.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1459.5¬±1238.5 MB/s, size: 262.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/val... 92 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92/92 [00:00<00:00, 2017.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov12_exp\u001b[0m\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/60     0.275G      1.173         16        224:  26%|‚ñà‚ñà‚ñå       | 7/27 [00:04<00:07,  2.82it/s]\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 17.0MB/s]\n",
            "       1/60     0.277G     0.7542         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:11<00:00,  2.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 1 RESULTS ===\n",
            "Top-1 Accuracy: 0.0000\n",
            "Top-5 Accuracy: 0.0000\n",
            "Training Loss: 0.22952015697956085\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.946          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/60     0.287G     0.1581         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:04<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 2 RESULTS ===\n",
            "Top-1 Accuracy: 0.9457\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.06952425092458725\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 36.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.946          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/60     0.295G    0.08616         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 3 RESULTS ===\n",
            "Top-1 Accuracy: 0.9457\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.0473475456237793\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 19.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.935          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/60     0.305G     0.1629         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 4 RESULTS ===\n",
            "Top-1 Accuracy: 0.9348\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.2748934328556061\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 25.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/60     0.312G     0.1226         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 5 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.2679905593395233\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 20.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.946          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/60     0.322G    0.09692         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 6 RESULTS ===\n",
            "Top-1 Accuracy: 0.9457\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.4140435755252838\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 23.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.924          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/60      0.33G      0.123         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 7 RESULTS ===\n",
            "Top-1 Accuracy: 0.9239\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.2493698000907898\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 16.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/60      0.34G     0.2147         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 8 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 1.259915828704834\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 22.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.902          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/60     0.348G     0.1845         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 9 RESULTS ===\n",
            "Top-1 Accuracy: 0.9022\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.4526731073856354\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 23.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.989          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/60     0.357G     0.1424         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:06<00:00,  3.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 10 RESULTS ===\n",
            "Top-1 Accuracy: 0.9891\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.014234731905162334\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 28.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/60     0.365G    0.09019         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 11 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.016711512580513954\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 22.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/60     0.375G    0.07608         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:07<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 12 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.00581235671415925\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 26.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.967          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/60     0.383G    0.09375         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 13 RESULTS ===\n",
            "Top-1 Accuracy: 0.9674\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.012187023647129536\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 18.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.978          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/60     0.393G    0.06881         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:07<00:00,  3.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 14 RESULTS ===\n",
            "Top-1 Accuracy: 0.9783\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.007096529006958008\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 23.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.967          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/60       0.4G    0.04113         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 15 RESULTS ===\n",
            "Top-1 Accuracy: 0.9674\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.01160198450088501\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 29.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.924          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/60      0.41G     0.1161         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:07<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 16 RESULTS ===\n",
            "Top-1 Accuracy: 0.9239\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.01437858771532774\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 24.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/60     0.418G    0.05227         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 17 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.01739353872835636\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 20.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.978          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/60     0.426G    0.07295         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:07<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 18 RESULTS ===\n",
            "Top-1 Accuracy: 0.9783\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.10677826404571533\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 27.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.957          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/60     0.436G    0.06017         12        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:05<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 19 RESULTS ===\n",
            "Top-1 Accuracy: 0.9565\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.32149556279182434\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 21.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.967          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 9, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "19 epochs completed in 0.039 hours.\n",
            "Optimizer stripped from runs/train/yolov12_exp/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/train/yolov12_exp/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/train/yolov12_exp/weights/best.pt...\n",
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 428 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 92 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 92 images in 3 classes ‚úÖ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 14.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.989          1\n",
            "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/train/yolov12_exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Train with verbose output\n",
        "results = model.train(\n",
        "    data='/content/',  # Use the root directory containing train, val, and test folders\n",
        "    imgsz=config.img_size,\n",
        "    epochs=config.epochs,\n",
        "    batch=config.batch_size,\n",
        "    patience=config.patience,\n",
        "    lr0=config.lr0,\n",
        "    val=True,\n",
        "    project='runs/train',\n",
        "    name='yolov12_exp',\n",
        "    exist_ok=True,\n",
        "    verbose=True,  # Enable verbose output\n",
        "    plots=True,    # Generate training plots\n",
        "    save_period=10  # Save model every 10 epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0hamlK6ROK2",
        "outputId": "2e15eda4-de45-4671-c47e-b993d0a9c33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 428 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 92 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 92 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1834.4¬±1077.3 MB/s, size: 262.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/val... 92 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 92/92 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.989          1\n",
            "Speed: 0.3ms preprocess, 2.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/train/yolov12_exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "metrics = model.val()\n",
        "test_accuracy = metrics.top1  # Using top-1 accuracy for classification\n",
        "wandb.log({'test_accuracy': test_accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzZM8lD3RUPO",
        "outputId": "27533f6b-9be8-45f2-e732-d186ef02d5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "1: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "2: 224x224 PAN 0.98, Aadhaar 0.02, Passport 0.01, 0.3ms\n",
            "3: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "4: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "5: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "6: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "7: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "8: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "9: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "10: 224x224 Aadhaar 0.96, PAN 0.04, Passport 0.00, 0.3ms\n",
            "11: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "12: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "13: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "14: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "15: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "16: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "17: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "18: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "19: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "20: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "21: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "22: 224x224 Aadhaar 0.99, Passport 0.01, PAN 0.00, 0.3ms\n",
            "23: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "24: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "25: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "26: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "27: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "28: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "29: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "30: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "31: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "32: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "33: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "34: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "35: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "36: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "37: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "38: 224x224 Aadhaar 0.99, PAN 0.01, Passport 0.00, 0.3ms\n",
            "39: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "40: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "41: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "42: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "43: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "44: 224x224 PAN 0.97, Passport 0.02, Aadhaar 0.01, 0.3ms\n",
            "45: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "46: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "47: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "48: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "49: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "50: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "51: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "52: 224x224 PAN 0.99, Aadhaar 0.01, Passport 0.00, 0.3ms\n",
            "53: 224x224 PAN 0.99, Aadhaar 0.01, Passport 0.00, 0.3ms\n",
            "54: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "55: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "56: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "57: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "58: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "59: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "60: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "61: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "62: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "63: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "64: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "65: 224x224 Passport 0.97, Aadhaar 0.03, PAN 0.00, 0.3ms\n",
            "66: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "67: 224x224 Aadhaar 0.85, PAN 0.08, Passport 0.06, 0.3ms\n",
            "68: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "69: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "70: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "71: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "72: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "73: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "74: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "75: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "76: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "77: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "78: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "79: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "80: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "81: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "82: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "83: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "84: 224x224 PAN 1.00, Passport 0.00, Aadhaar 0.00, 0.3ms\n",
            "85: 224x224 Aadhaar 0.66, PAN 0.34, Passport 0.00, 0.3ms\n",
            "86: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "87: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "88: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "89: 224x224 PAN 0.99, Aadhaar 0.01, Passport 0.00, 0.3ms\n",
            "90: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "91: 224x224 Aadhaar 0.97, PAN 0.03, Passport 0.00, 0.3ms\n",
            "Speed: 4.1ms preprocess, 0.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "True classes: (array([0, 1, 2]), array([31, 30, 31]))\n",
            "Predicted classes: (array([0, 1, 2]), array([32, 29, 31]))\n",
            "True classes array: [1 1 1 0 0 0 0 2 0 0 1 1 2 2 1 0 0 2 2 0 1 1 0 2 2 1 2 2 0 1 2 0 1 2 1 2 0 2 0 1 0 1 1 2 1 1 0 2 1 2 2 0 1 1 0 1 0 2 2 0 2 0 0 1 2 2 0 0 2 2 2 1 1 1 2 2 0 0 1 1 2 2 2 1 1 0 0 0 2 1 0 0]\n",
            "Predicted classes array: [1 1 1 0 0 0 0 2 0 0 0 1 2 2 1 0 0 2 2 0 1 1 0 2 2 1 2 2 0 1 2 0 1 2 1 2 0 2 0 1 0 1 1 2 1 1 0 2 1 2 2 0 1 1 0 1 0 2 2 0 2 0 0 1 2 2 0 0 2 2 2 1 1 1 2 2 0 0 1 1 2 2 2 1 1 0 0 0 2 1 0 0]\n"
          ]
        }
      ],
      "source": [
        "# Predictions for test set\n",
        "predictions = model.predict(test_df['image_path'].tolist(), imgsz=config.img_size)\n",
        "predicted_classes = np.array([x.probs.top1 for x in predictions])  # Convert to numpy array\n",
        "true_classes = test_df['label'].map({'Aadhaar': 0, 'PAN': 1, 'Passport': 2}).values\n",
        "\n",
        "print(f\"True classes: {np.unique(true_classes, return_counts=True)}\")\n",
        "print(f\"Predicted classes: {np.unique(predicted_classes, return_counts=True)}\")\n",
        "print(f\"True classes array: {true_classes}\")\n",
        "print(f\"Predicted classes array: {predicted_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIuHVPTMRjdx",
        "outputId": "248601f2-28b7-4c88-ab73-8fad008325d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Aadhaar accuracy: 1.0000\n",
            "Class PAN accuracy: 0.9667\n",
            "Class Passport accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy for each class\n",
        "for i, label in enumerate(['Aadhaar', 'PAN', 'Passport']):\n",
        "    mask = true_classes == i\n",
        "    if np.sum(mask) > 0:\n",
        "        matches = predicted_classes[mask] == i\n",
        "        accuracy = np.mean(matches)\n",
        "        print(f\"Class {label} accuracy: {accuracy:.4f}\")\n",
        "        wandb.log({f'{label}_accuracy': accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-7hbA5ERq1R",
        "outputId": "a7804aaf-a7db-4aca-d4d0-362e6e422f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Test Accuracy: 0.9891\n"
          ]
        }
      ],
      "source": [
        "# Calculate overall accuracy\n",
        "overall_accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
        "wandb.log({'overall_test_accuracy': overall_accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfSMjBuNSqGY",
        "outputId": "dc4e3ab3-8d03-4d67-9106-5c81e470ca8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Artifact document_classifier_yolo12_model>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Save the model with YOLOv12 naming\n",
        "model.save('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "artifact = wandb.Artifact('document_classifier_yolo12_model', type='model')\n",
        "artifact.add_file('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "wandb.log_artifact(artifact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usXPeT9QTFyj",
        "outputId": "eea4bced-8f0e-4ea9-f0dd-10a6f9774b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best trained model from: runs/train/yolov12_exp/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model (use the best weights from training)\n",
        "try:\n",
        "    # Try to load the best model from training results\n",
        "    model_path = 'runs/train/yolov12_exp/weights/best.pt'\n",
        "    if os.path.exists(model_path):\n",
        "        model = YOLO(model_path)\n",
        "        print(f\"Loaded best trained model from: {model_path}\")\n",
        "    else:\n",
        "        # Fallback to saved model\n",
        "        model = YOLO('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "        print(\"Loaded model from Drive\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Last resort - load the original trained model object\n",
        "    print(\"Using the model object from training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6io6_gfmUR-y"
      },
      "outputs": [],
      "source": [
        "def is_blurry(image_np, threshold=150):\n",
        "    if len(image_np.shape) == 3:\n",
        "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = image_np\n",
        "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    wandb.log({'laplacian_variance': laplacian_var})\n",
        "    return laplacian_var < threshold\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Output'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "for label in ['Aadhaar', 'PAN', 'Passport']:\n",
        "    folder = os.path.join(output_dir, label)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmKOfuhpTJIa"
      },
      "outputs": [],
      "source": [
        "def classify_document(selected_type, image):\n",
        "    blur_result = is_blurry(image)\n",
        "\n",
        "    # Proper preprocessing for YOLO model\n",
        "    # Convert RGB to BGR if needed (YOLO expects BGR)\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    else:\n",
        "        image_bgr = image\n",
        "\n",
        "    # Let YOLO handle the preprocessing instead of manual resizing\n",
        "    predictions = model.predict(image_bgr, imgsz=config.img_size, verbose=False)[0]\n",
        "\n",
        "    # Get prediction results\n",
        "    predicted_class_idx = predictions.probs.top1\n",
        "    predicted_label = ['Aadhaar', 'PAN', 'Passport'][predicted_class_idx]\n",
        "    confidence = predictions.probs.top1conf.item()\n",
        "\n",
        "    # Get all class probabilities for debugging\n",
        "    class_probs = predictions.probs.data.cpu().numpy() if hasattr(predictions.probs.data, 'cpu') else predictions.probs.data\n",
        "\n",
        "    print(f\"Class probabilities - Aadhaar: {class_probs[0]:.4f}, PAN: {class_probs[1]:.4f}, Passport: {class_probs[2]:.4f}\")\n",
        "    print(f\"Selected: {selected_type}, Predicted: {predicted_label}, Confidence: {confidence:.4f}, Blurry: {blur_result}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'selected_type': selected_type,\n",
        "        'predicted_label': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'blurry': blur_result,\n",
        "        'aadhaar_prob': float(class_probs[0]),\n",
        "        'pan_prob': float(class_probs[1]),\n",
        "        'passport_prob': float(class_probs[2])\n",
        "    })\n",
        "\n",
        "    if blur_result:\n",
        "        return \"Please upload a clearer image.\"\n",
        "\n",
        "    # Lower the confidence threshold temporarily for debugging\n",
        "    confidence_threshold = 0.3  # Reduced from 0.8 for testing\n",
        "\n",
        "    if predicted_label == selected_type and confidence >= confidence_threshold:\n",
        "        timestamp = int(time.time())\n",
        "        filename = f\"{selected_type}_{timestamp}.png\"\n",
        "        save_path = os.path.join(output_dir, selected_type, filename)\n",
        "        image_bgr_save = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(save_path, image_bgr_save)\n",
        "        return f\"Document accepted. Confidence: {confidence:.3f}\"\n",
        "    else:\n",
        "        if predicted_label == selected_type:\n",
        "            return f\"Document type correct but confidence too low ({confidence:.3f}). Please try a clearer image.\"\n",
        "        else:\n",
        "            if selected_type == 'Passport':\n",
        "                return f\"Please upload a valid Indian passport. Detected as {predicted_label} with {confidence:.3f} confidence.\"\n",
        "            else:\n",
        "                return f\"Please upload a valid {selected_type} document. Detected as {predicted_label} with {confidence:.3f} confidence.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqSEnH2dTTP9"
      },
      "outputs": [],
      "source": [
        "iface = gr.Interface(\n",
        "    fn=classify_document,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=['Aadhaar', 'PAN', 'Passport'], label=\"Select Document Type\"),\n",
        "        gr.Image(type=\"numpy\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Document Classifier - YOLOv12\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "K0rMJieJTXoQ",
        "outputId": "9a1e43aa-2f8e-4b36-b57d-c49297a110fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://991209b8919a3ec1a3.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://991209b8919a3ec1a3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class probabilities - Aadhaar: 0.0011, PAN: 0.0035, Passport: 0.9954\n",
            "Selected: Passport, Predicted: Passport, Confidence: 0.9954, Blurry: False\n",
            "Class probabilities - Aadhaar: 0.0000, PAN: 0.0000, Passport: 1.0000\n",
            "Selected: Passport, Predicted: Passport, Confidence: 1.0000, Blurry: False\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://991209b8919a3ec1a3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SRciZ30TdDT"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXIgza8UfCKh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43796830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e811868-da6b-4f15-bf0c-0c6cff5e8ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/319.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hFound existing installation: protobuf 5.29.5\n",
            "Uninstalling protobuf-5.29.5:\n",
            "  Successfully uninstalled protobuf-5.29.5\n",
            "Collecting protobuf==3.20.0\n",
            "  Using cached protobuf-3.20.0-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
            "Installing collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-spanner 3.57.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-speech 2.33.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.109.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-dataproc 5.21.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.25.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.32.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-logging 3.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.21.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-appengine-logging 1.6.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-trace 1.16.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-audit-log 0.3.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-secret-manager 2.24.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4f065111a572449991af6c340b72fbf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Aadhaar_accuracy</td><td>‚ñÅ</td></tr><tr><td>Aadhaar_count</td><td>‚ñÅ</td></tr><tr><td>PAN_accuracy</td><td>‚ñÅ</td></tr><tr><td>PAN_count</td><td>‚ñÅ</td></tr><tr><td>Passport_accuracy</td><td>‚ñÅ</td></tr><tr><td>Passport_count</td><td>‚ñÅ</td></tr><tr><td>aadhaar_prob</td><td>‚ñà‚ñÅ</td></tr><tr><td>confidence</td><td>‚ñÅ‚ñà</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>epoch_top1_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>epoch_top5_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>laplacian_variance</td><td>‚ñÅ‚ñà</td></tr><tr><td>overall_test_accuracy</td><td>‚ñÅ</td></tr><tr><td>pan_prob</td><td>‚ñà‚ñÅ</td></tr><tr><td>passport_prob</td><td>‚ñÅ‚ñà</td></tr><tr><td>test_accuracy</td><td>‚ñÅ</td></tr><tr><td>train_top1_accuracy</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_top5_accuracy</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Aadhaar_accuracy</td><td>1</td></tr><tr><td>Aadhaar_count</td><td>204</td></tr><tr><td>PAN_accuracy</td><td>0.96667</td></tr><tr><td>PAN_count</td><td>204</td></tr><tr><td>Passport_accuracy</td><td>1</td></tr><tr><td>Passport_count</td><td>204</td></tr><tr><td>aadhaar_prob</td><td>0.0</td></tr><tr><td>blurry</td><td>False</td></tr><tr><td>confidence</td><td>1.0</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>epoch_top1_acc</td><td>0.95652</td></tr><tr><td>epoch_top5_acc</td><td>1</td></tr><tr><td>laplacian_variance</td><td>1030.88221</td></tr><tr><td>overall_test_accuracy</td><td>0.98913</td></tr><tr><td>pan_prob</td><td>0.0</td></tr><tr><td>passport_prob</td><td>1.0</td></tr><tr><td>predicted_label</td><td>Passport</td></tr><tr><td>selected_type</td><td>Passport</td></tr><tr><td>test_accuracy</td><td>0.98913</td></tr><tr><td>train_top1_accuracy</td><td>0.95652</td></tr><tr><td>train_top5_accuracy</td><td>1</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">yolov12_classification</strong> at: <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/yw4kq7mi' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/yw4kq7mi</a><br> View project at: <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250825_102533-yw4kq7mi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250825_131900-tq7rezwq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/tq7rezwq' target=\"_blank\">yolov12_classification</a></strong> to <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/tq7rezwq' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/tq7rezwq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train at /content/train populated with 468 images: Aadhaar=143, PAN=143, Passport=182\n",
            "val at /content/val populated with 118 images: Aadhaar=30, PAN=31, Passport=57\n",
            "test at /content/test populated with 115 images: Aadhaar=31, PAN=30, Passport=54\n",
            "dataset.yaml created/overwritten at /content/dataset.yaml with content: \n",
            "train: /content/train\n",
            "val: /content/val\n",
            "test: /content/test\n",
            "nc: 3  # number of classes\n",
            "names: ['Aadhaar', 'PAN', 'Passport']  # class names\n",
            "\n",
            "YOLO12 classification models not found, using YOLO11n-cls.pt as fallback\n",
            "Starting training with YOLOv12 and data from: /content/dataset.yaml\n",
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=60, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov12_exp, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/yolov12_exp, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 468 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 118 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 115 images in 3 classes ‚úÖ \n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 10                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLO11n-cls summary: 86 layers, 1,534,947 parameters, 1,534,947 gradients, 3.3 GFLOPs\n",
            "Transferred 234/236 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2238.8¬±1132.9 MB/s, size: 303.6 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train... 468 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 468/468 [00:00<00:00, 2258.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1527.9¬±1056.3 MB/s, size: 262.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/val... 118 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<00:00, 693.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/val.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov12_exp\u001b[0m\n",
            "Starting training for 60 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/60     0.582G     0.6736          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:09<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 1 RESULTS ===\n",
            "Top-1 Accuracy: 0.0000\n",
            "Top-5 Accuracy: 0.0000\n",
            "Training Loss: 0.47422027587890625\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  4.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.949          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/60     0.582G     0.1223          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 2 RESULTS ===\n",
            "Top-1 Accuracy: 0.9492\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.08690285682678223\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 26.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/60     0.582G     0.1637          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 3 RESULTS ===\n",
            "Top-1 Accuracy: 0.9576\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 1.7110354900360107\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 33.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/60     0.582G     0.1158          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 4 RESULTS ===\n",
            "Top-1 Accuracy: 0.9576\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.8812270164489746\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 18.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.958          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/60     0.582G     0.1659          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 5 RESULTS ===\n",
            "Top-1 Accuracy: 0.9576\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.013317108154296875\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 14.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.949          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/60     0.582G     0.1385          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 6 RESULTS ===\n",
            "Top-1 Accuracy: 0.9492\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.2484021782875061\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 32.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.949          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/60     0.582G     0.1211          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 7 RESULTS ===\n",
            "Top-1 Accuracy: 0.9492\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.13226318359375\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 13.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/60     0.582G    0.09752          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 8 RESULTS ===\n",
            "Top-1 Accuracy: 0.9661\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.1700192391872406\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 24.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/60     0.582G     0.1019          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 9 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.0006386935710906982\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 25.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/60     0.582G     0.1239          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 10 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.005129396915435791\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 34.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/60     0.582G    0.05486          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 11 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.10944816470146179\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 24.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.975          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/60     0.582G    0.08048          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 12 RESULTS ===\n",
            "Top-1 Accuracy: 0.9746\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.0015585720539093018\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 23.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/60     0.582G    0.06394          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 13 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.45703279972076416\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 37.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/60     0.582G    0.08144          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 14 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.008941173553466797\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 19.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/60     0.582G    0.08745          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 15 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.003988921642303467\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 36.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/60     0.582G    0.06712          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 16 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.004014045000076294\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 17.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/60     0.582G    0.09991          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 17 RESULTS ===\n",
            "Top-1 Accuracy: 0.9831\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.04020333290100098\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 31.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          1          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/60     0.582G    0.06886          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 18 RESULTS ===\n",
            "Top-1 Accuracy: 1.0000\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.003893822431564331\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 22.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/60     0.582G    0.09593          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 19 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.007761716842651367\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 32.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/60     0.582G    0.08462          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 20 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.2856254577636719\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 24.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/60     0.582G    0.05989          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 21 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.014593362808227539\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 25.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/60     0.582G    0.04781          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 22 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.0003902018070220947\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 34.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/60     0.582G    0.04758          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:06<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 23 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.10716292262077332\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 25.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.992          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/60     0.582G    0.04291          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 24 RESULTS ===\n",
            "Top-1 Accuracy: 0.9915\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.15350341796875\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 23.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.975          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/60     0.582G    0.07807          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 25 RESULTS ===\n",
            "Top-1 Accuracy: 0.9746\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.008696198463439941\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 20.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/60     0.582G    0.03324          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:07<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 26 RESULTS ===\n",
            "Top-1 Accuracy: 0.9661\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.0007295012474060059\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 23.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.966          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/60     0.582G     0.0387          4        224: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:08<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EPOCH 27 RESULTS ===\n",
            "Top-1 Accuracy: 0.9661\n",
            "Top-5 Accuracy: 1.0000\n",
            "Training Loss: 0.003361731767654419\n",
            "==============================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 18.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all      0.983          1\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 17, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "27 epochs completed in 0.064 hours.\n",
            "Optimizer stripped from runs/train/yolov12_exp/weights/last.pt, 3.2MB\n",
            "Optimizer stripped from runs/train/yolov12_exp/weights/best.pt, 3.2MB\n",
            "\n",
            "Validating runs/train/yolov12_exp/weights/best.pt...\n",
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 468 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 118 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 115 images in 3 classes ‚úÖ \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  6.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          1          1\n",
            "Speed: 0.1ms preprocess, 0.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/train/yolov12_exp\u001b[0m\n",
            "Ultralytics 8.3.185 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11n-cls summary (fused): 47 layers, 1,529,867 parameters, 0 gradients, 3.2 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/train... found 468 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/val... found 118 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/test... found 115 images in 3 classes ‚úÖ \n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2190.0¬±612.0 MB/s, size: 262.7 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/val... 118 images, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:00<?, ?it/s]\n",
            "               classes   top1_acc   top5_acc: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:01<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          1          1\n",
            "Speed: 0.5ms preprocess, 1.5ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/train/yolov12_exp\u001b[0m\n",
            "\n",
            "0: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "1: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "2: 224x224 PAN 0.99, Aadhaar 0.01, Passport 0.00, 0.3ms\n",
            "3: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "4: 224x224 Aadhaar 0.99, PAN 0.01, Passport 0.00, 0.3ms\n",
            "5: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "6: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "7: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "8: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "9: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "10: 224x224 PAN 0.94, Aadhaar 0.04, Passport 0.02, 0.3ms\n",
            "11: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "12: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "13: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "14: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "15: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "16: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "17: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "18: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "19: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "20: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "21: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "22: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "23: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "24: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "25: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "26: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "27: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "28: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "29: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "30: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "31: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "32: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "33: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "34: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "35: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "36: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "37: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "38: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "39: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "40: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "41: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "42: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "43: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "44: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "45: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "46: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "47: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "48: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "49: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "50: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "51: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "52: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "53: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "54: 224x224 Aadhaar 0.92, PAN 0.08, Passport 0.00, 0.3ms\n",
            "55: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "56: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "57: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "58: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "59: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "60: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "61: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "62: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "63: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "64: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "65: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "66: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "67: 224x224 PAN 0.74, Aadhaar 0.25, Passport 0.01, 0.3ms\n",
            "68: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "69: 224x224 Passport 1.00, PAN 0.00, Aadhaar 0.00, 0.3ms\n",
            "70: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "71: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "72: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "73: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "74: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "75: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "76: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "77: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "78: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "79: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "80: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "81: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "82: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "83: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "84: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "85: 224x224 PAN 0.55, Aadhaar 0.44, Passport 0.01, 0.3ms\n",
            "86: 224x224 Aadhaar 1.00, PAN 0.00, Passport 0.00, 0.3ms\n",
            "87: 224x224 Aadhaar 0.98, PAN 0.02, Passport 0.00, 0.3ms\n",
            "88: 224x224 Passport 1.00, Aadhaar 0.00, PAN 0.00, 0.3ms\n",
            "89: 224x224 PAN 1.00, Aadhaar 0.00, Passport 0.00, 0.3ms\n",
            "90: 224x224 Aadhaar 0.98, PAN 0.02, Passport 0.00, 0.3ms\n",
            "91: 224x224 Aadhaar 0.67, PAN 0.33, Passport 0.00, 0.3ms\n",
            "Speed: 4.4ms preprocess, 0.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
            "True classes: (array([0, 1, 2]), array([31, 30, 31]))\n",
            "Predicted classes: (array([0, 1, 2]), array([29, 32, 31]))\n",
            "True classes array: [1 1 1 0 0 0 0 2 0 0 1 1 2 2 1 0 0 2 2 0 1 1 0 2 2 1 2 2 0 1 2 0 1 2 1 2 0 2 0 1 0 1 1 2 1 1 0 2 1 2 2 0 1 1 0 1 0 2 2 0 2 0 0 1 2 2 0 0 2 2 2 1 1 1 2 2 0 0 1 1 2 2 2 1 1 0 0 0 2 1 0 0]\n",
            "Predicted classes array: [1 1 1 0 0 0 0 2 0 0 1 1 2 2 1 0 0 2 2 0 1 1 0 2 2 1 2 2 0 1 2 0 1 2 1 2 0 2 0 1 0 1 1 2 1 1 0 2 1 2 2 0 1 1 0 1 0 2 2 0 2 0 0 1 2 2 0 1 2 2 2 1 1 1 2 2 0 0 1 1 2 2 2 1 1 1 0 0 2 1 0 0]\n",
            "Class Aadhaar accuracy: 0.9355\n",
            "Class PAN accuracy: 1.0000\n",
            "Class Passport accuracy: 1.0000\n",
            "Overall Test Accuracy: 0.9783\n",
            "Loaded best trained model from: runs/train/yolov12_exp/weights/best.pt\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://660441dd423c83b1a1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://660441dd423c83b1a1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://660441dd423c83b1a1.gradio.live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Aadhaar_accuracy</td><td>‚ñÅ</td></tr><tr><td>Aadhaar_count</td><td>‚ñÅ</td></tr><tr><td>PAN_accuracy</td><td>‚ñÅ</td></tr><tr><td>PAN_count</td><td>‚ñÅ</td></tr><tr><td>Passport_accuracy</td><td>‚ñÅ</td></tr><tr><td>Passport_count</td><td>‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>epoch_top1_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>epoch_top5_acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>overall_test_accuracy</td><td>‚ñÅ</td></tr><tr><td>test_accuracy</td><td>‚ñÅ</td></tr><tr><td>train_top1_accuracy</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_top5_accuracy</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Aadhaar_accuracy</td><td>0.93548</td></tr><tr><td>Aadhaar_count</td><td>204</td></tr><tr><td>PAN_accuracy</td><td>1</td></tr><tr><td>PAN_count</td><td>204</td></tr><tr><td>Passport_accuracy</td><td>1</td></tr><tr><td>Passport_count</td><td>204</td></tr><tr><td>epoch</td><td>27</td></tr><tr><td>epoch_top1_acc</td><td>0.9661</td></tr><tr><td>epoch_top5_acc</td><td>1</td></tr><tr><td>overall_test_accuracy</td><td>0.97826</td></tr><tr><td>test_accuracy</td><td>1</td></tr><tr><td>train_top1_accuracy</td><td>0.9661</td></tr><tr><td>train_top5_accuracy</td><td>1</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">yolov12_classification</strong> at: <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/tq7rezwq' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification/runs/tq7rezwq</a><br> View project at: <a href='https://wandb.ai/sounakray496-asmadiya-technologies/document_classification' target=\"_blank\">https://wandb.ai/sounakray496-asmadiya-technologies/document_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250825_131900-tq7rezwq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics wandb gradio tensorflow\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import gradio as gr\n",
        "import wandb\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Uninstall and reinstall protobuf to avoid conflicts\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.0\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up WandB\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"document_classification\",\n",
        "    name=\"yolov12_classification\",\n",
        "    config={\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": 16,\n",
        "        \"img_size\": 224,\n",
        "        \"patience\": 10,\n",
        "        \"lr0\": 0.0001\n",
        "    }\n",
        ")\n",
        "config = wandb.config\n",
        "\n",
        "# Prepare dataset from specified paths\n",
        "base_path = '/content/drive/MyDrive/Datasets'\n",
        "aadhar_folder = os.path.join(base_path, 'aadhar_images')\n",
        "passport_folder = os.path.join(base_path, 'passport_images')\n",
        "pan_test_folder = os.path.join(base_path, 'pan_images', 'test')\n",
        "\n",
        "# Collect up to 204 images from each folder\n",
        "aadhar_images = [os.path.join(aadhar_folder, f) for f in os.listdir(aadhar_folder) if f.endswith('.png')]\n",
        "aadhar_images = aadhar_images[:204]  # Limit to 204\n",
        "aadhar_labels = ['Aadhaar'] * len(aadhar_images)\n",
        "passport_images = [os.path.join(passport_folder, f) for f in os.listdir(passport_folder) if f.endswith('.png')]\n",
        "passport_images = random.sample(passport_images, min(204, len(passport_images)))  # Limit to 204\n",
        "passport_labels = ['Passport'] * len(passport_images)\n",
        "pan_images = [os.path.join(pan_test_folder, f) for f in os.listdir(pan_test_folder) if f.endswith('.png')]\n",
        "pan_images = pan_images[:204]  # Limit to 204\n",
        "pan_labels = ['PAN'] * len(pan_images)\n",
        "\n",
        "all_images = aadhar_images + passport_images + pan_images\n",
        "all_labels = aadhar_labels + passport_labels + pan_labels\n",
        "df = pd.DataFrame({'image_path': all_images, 'label': all_labels})\n",
        "wandb.log({'Aadhaar_count': len(aadhar_images), 'Passport_count': len(passport_images), 'PAN_count': len(pan_images)})\n",
        "\n",
        "# Split dataset (70% train, 15% val, 15% test)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Create directory structure and copy images to specified paths\n",
        "for split, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    os.makedirs(f'/content/{split}', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/Aadhaar', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/PAN', exist_ok=True)\n",
        "    os.makedirs(f'/content/{split}/Passport', exist_ok=True)\n",
        "    for _, row in split_df.iterrows():\n",
        "        dest_folder = f'/content/{split}/{row[\"label\"]}'\n",
        "        shutil.copy(row['image_path'], dest_folder)\n",
        "    # Verify population\n",
        "    aadhar_count = len(os.listdir(f'/content/{split}/Aadhaar'))\n",
        "    pan_count = len(os.listdir(f'/content/{split}/PAN'))\n",
        "    passport_count = len(os.listdir(f'/content/{split}/Passport'))\n",
        "    total_count = aadhar_count + pan_count + passport_count\n",
        "    print(f\"{split} at /content/{split} populated with {total_count} images: Aadhaar={aadhar_count}, PAN={pan_count}, Passport={passport_count}\")\n",
        "\n",
        "# Create and overwrite dataset.yaml at specified path\n",
        "yaml_content = \"\"\"\n",
        "train: /content/train\n",
        "val: /content/val\n",
        "test: /content/test\n",
        "nc: 3  # number of classes\n",
        "names: ['Aadhaar', 'PAN', 'Passport']  # class names\n",
        "\"\"\"\n",
        "with open('/content/dataset.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "print(\"dataset.yaml created/overwritten at /content/dataset.yaml with content:\", yaml_content)\n",
        "\n",
        "# Load YOLOv12 model - Updated to YOLOv12\n",
        "# Try different possible model names for YOLO12 classification\n",
        "try:\n",
        "    model = YOLO('yolo12n-cls.pt')  # YOLO12 nano classification model\n",
        "    print(\"Loaded yolo12n-cls.pt successfully\")\n",
        "except:\n",
        "    try:\n",
        "        model = YOLO('yolo12s-cls.pt')  # YOLO12 small classification model\n",
        "        print(\"Loaded yolo12s-cls.pt successfully\")\n",
        "    except:\n",
        "        try:\n",
        "            model = YOLO('yolov12n-cls.pt')  # Alternative naming\n",
        "            print(\"Loaded yolov12n-cls.pt successfully\")\n",
        "        except:\n",
        "            # Fallback to YOLO11 if YOLO12 classification models aren't available yet\n",
        "            model = YOLO('yolo11n-cls.pt')\n",
        "            print(\"YOLO12 classification models not found, using YOLO11n-cls.pt as fallback\")\n",
        "\n",
        "# Train the model with verbose output and callbacks\n",
        "print(\"Starting training with YOLOv12 and data from:\", '/content/dataset.yaml')\n",
        "\n",
        "# Custom callback to print accuracy after each epoch\n",
        "def on_train_epoch_end(trainer):\n",
        "    \"\"\"Callback function to print metrics after each epoch\"\"\"\n",
        "    if trainer.epoch >= 0:  # Start from epoch 0\n",
        "        metrics = trainer.validator.metrics if hasattr(trainer, 'validator') and trainer.validator else None\n",
        "        if metrics:\n",
        "            # For classification, the main metrics are top1 and top5 accuracy\n",
        "            top1_acc = getattr(metrics, 'top1', None)\n",
        "            top5_acc = getattr(metrics, 'top5', None)\n",
        "\n",
        "            print(f\"\\n=== EPOCH {trainer.epoch + 1} RESULTS ===\")\n",
        "            if top1_acc is not None:\n",
        "                print(f\"Top-1 Accuracy: {top1_acc:.4f}\")\n",
        "                # Log to wandb as well\n",
        "                wandb.log({\n",
        "                    'epoch': trainer.epoch + 1,\n",
        "                    'train_top1_accuracy': top1_acc,\n",
        "                    'epoch_top1_acc': top1_acc\n",
        "                })\n",
        "            if top5_acc is not None:\n",
        "                print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
        "                wandb.log({\n",
        "                    'epoch': trainer.epoch + 1,\n",
        "                    'train_top5_accuracy': top5_acc,\n",
        "                    'epoch_top5_acc': top5_acc\n",
        "                })\n",
        "\n",
        "            # Print loss information if available\n",
        "            if hasattr(trainer, 'loss_items') and trainer.loss_items is not None:\n",
        "                print(f\"Training Loss: {trainer.loss_items}\")\n",
        "\n",
        "            print(\"=\" * 30)\n",
        "\n",
        "# Add the callback to the model\n",
        "model.add_callback('on_train_epoch_end', on_train_epoch_end)\n",
        "\n",
        "# Train with verbose output\n",
        "results = model.train(\n",
        "    data='/content/',  # Use the root directory containing train, val, and test folders\n",
        "    imgsz=config.img_size,\n",
        "    epochs=config.epochs,\n",
        "    batch=config.batch_size,\n",
        "    patience=config.patience,\n",
        "    lr0=config.lr0,\n",
        "    val=True,\n",
        "    project='runs/train',\n",
        "    name='yolov12_exp',\n",
        "    exist_ok=True,\n",
        "    verbose=True,  # Enable verbose output\n",
        "    plots=True,    # Generate training plots\n",
        "    save_period=10  # Save model every 10 epochs\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = model.val()\n",
        "test_accuracy = metrics.top1  # Using top-1 accuracy for classification\n",
        "wandb.log({'test_accuracy': test_accuracy})\n",
        "\n",
        "# Predictions for test set\n",
        "predictions = model.predict(test_df['image_path'].tolist(), imgsz=config.img_size)\n",
        "predicted_classes = np.array([x.probs.top1 for x in predictions])  # Convert to numpy array\n",
        "true_classes = test_df['label'].map({'Aadhaar': 0, 'PAN': 1, 'Passport': 2}).values\n",
        "\n",
        "print(f\"True classes: {np.unique(true_classes, return_counts=True)}\")\n",
        "print(f\"Predicted classes: {np.unique(predicted_classes, return_counts=True)}\")\n",
        "print(f\"True classes array: {true_classes}\")\n",
        "print(f\"Predicted classes array: {predicted_classes}\")\n",
        "\n",
        "# Calculate accuracy for each class\n",
        "for i, label in enumerate(['Aadhaar', 'PAN', 'Passport']):\n",
        "    mask = true_classes == i\n",
        "    if np.sum(mask) > 0:\n",
        "        matches = predicted_classes[mask] == i\n",
        "        accuracy = np.mean(matches)\n",
        "        print(f\"Class {label} accuracy: {accuracy:.4f}\")\n",
        "        wandb.log({f'{label}_accuracy': accuracy})\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
        "wandb.log({'overall_test_accuracy': overall_accuracy})\n",
        "\n",
        "# Save the model with YOLOv12 naming\n",
        "model.save('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "artifact = wandb.Artifact('document_classifier_yolo12_model', type='model')\n",
        "artifact.add_file('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "wandb.log_artifact(artifact)\n",
        "\n",
        "# Load the trained model (use the best weights from training)\n",
        "try:\n",
        "    # Try to load the best model from training results\n",
        "    model_path = 'runs/train/yolov12_exp/weights/best.pt'\n",
        "    if os.path.exists(model_path):\n",
        "        model = YOLO(model_path)\n",
        "        print(f\"Loaded best trained model from: {model_path}\")\n",
        "    else:\n",
        "        # Fallback to saved model\n",
        "        model = YOLO('/content/drive/MyDrive/document_classifier_yolo12.pt')\n",
        "        print(\"Loaded model from Drive\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    # Last resort - load the original trained model object\n",
        "    print(\"Using the model object from training\")\n",
        "\n",
        "def is_blurry(image_np, threshold=150):\n",
        "    if len(image_np.shape) == 3:\n",
        "        gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        gray = image_np\n",
        "    laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    wandb.log({'laplacian_variance': laplacian_var})\n",
        "    return laplacian_var < threshold\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/Output'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "for label in ['Aadhaar', 'PAN', 'Passport']:\n",
        "    folder = os.path.join(output_dir, label)\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "def classify_document(selected_type, image):\n",
        "    blur_result = is_blurry(image)\n",
        "\n",
        "    # Proper preprocessing for YOLO model\n",
        "    # Convert RGB to BGR if needed (YOLO expects BGR)\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    else:\n",
        "        image_bgr = image\n",
        "\n",
        "    # Let YOLO handle the preprocessing instead of manual resizing\n",
        "    predictions = model.predict(image_bgr, imgsz=config.img_size, verbose=False)[0]\n",
        "\n",
        "    # Get prediction results\n",
        "    predicted_class_idx = predictions.probs.top1\n",
        "    predicted_label = ['Aadhaar', 'PAN', 'Passport'][predicted_class_idx]\n",
        "    confidence = predictions.probs.top1conf.item()\n",
        "\n",
        "    # Get all class probabilities for debugging\n",
        "    class_probs = predictions.probs.data.cpu().numpy() if hasattr(predictions.probs.data, 'cpu') else predictions.probs.data\n",
        "\n",
        "    print(f\"Class probabilities - Aadhaar: {class_probs[0]:.4f}, PAN: {class_probs[1]:.4f}, Passport: {class_probs[2]:.4f}\")\n",
        "    print(f\"Selected: {selected_type}, Predicted: {predicted_label}, Confidence: {confidence:.4f}, Blurry: {blur_result}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'selected_type': selected_type,\n",
        "        'predicted_label': predicted_label,\n",
        "        'confidence': confidence,\n",
        "        'blurry': blur_result,\n",
        "        'aadhaar_prob': float(class_probs[0]),\n",
        "        'pan_prob': float(class_probs[1]),\n",
        "        'passport_prob': float(class_probs[2])\n",
        "    })\n",
        "\n",
        "    if blur_result:\n",
        "        return \"Please upload a clearer image.\"\n",
        "\n",
        "    # Lower the confidence threshold temporarily for debugging\n",
        "    confidence_threshold = 0.3  # Reduced from 0.8 for testing\n",
        "\n",
        "    if predicted_label == selected_type and confidence >= confidence_threshold:\n",
        "        timestamp = int(time.time())\n",
        "        filename = f\"{selected_type}_{timestamp}.png\"\n",
        "        save_path = os.path.join(output_dir, selected_type, filename)\n",
        "        image_bgr_save = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(save_path, image_bgr_save)\n",
        "        return f\"Document accepted. Confidence: {confidence:.3f}\"\n",
        "    else:\n",
        "        if predicted_label == selected_type:\n",
        "            return f\"Document type correct but confidence too low ({confidence:.3f}). Please try a clearer image.\"\n",
        "        else:\n",
        "            if selected_type == 'Passport':\n",
        "                return f\"Please upload a valid Indian passport. Detected as {predicted_label} with {confidence:.3f} confidence.\"\n",
        "            else:\n",
        "                return f\"Please upload a valid {selected_type} document. Detected as {predicted_label} with {confidence:.3f} confidence.\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=classify_document,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=['Aadhaar', 'PAN', 'Passport'], label=\"Select Document Type\"),\n",
        "        gr.Image(type=\"numpy\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Document Classifier - YOLOv12\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUhiHFNgqNYu",
        "outputId": "a0a2cdfa-b54f-45f8-b333-663deb8a7266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import base64\n",
        "import io\n",
        "import gradio as gr\n",
        "\n",
        "def extract_aadhaar_data(image):\n",
        "    img_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # OCR line-wise (clean non-ASCII)\n",
        "    raw_text = pytesseract.image_to_string(image, lang=\"eng\")\n",
        "    lines = [re.sub(r'[^\\x00-\\x7F]+', '', l).strip() for l in raw_text.split(\"\\n\") if l.strip()]\n",
        "\n",
        "    aadhaar_number = None\n",
        "    name = None\n",
        "    dob = None\n",
        "    gender = None\n",
        "\n",
        "    # Aadhaar number search\n",
        "    for l in lines:\n",
        "        m = re.search(r\"\\b[2-9]{1}[0-9]{3}\\s?[0-9]{4}\\s?[0-9]{4}\\b\", l)\n",
        "        if m:\n",
        "            aadhaar_number = m.group(0)\n",
        "            break\n",
        "\n",
        "    # DOB & Name extraction\n",
        "    for i, l in enumerate(lines):\n",
        "        dob_match = re.search(r\"(\\d{2}[\\/\\-]\\d{2}[\\/\\-]\\d{4})\", l)\n",
        "        if dob_match:\n",
        "            dob = dob_match.group(1)\n",
        "            if i > 0:\n",
        "                prev_line = lines[i-1]\n",
        "                name = \" \".join(re.findall(r\"[A-Za-z]+\", prev_line))\n",
        "            break\n",
        "\n",
        "    # Gender detection\n",
        "    for l in lines:\n",
        "        if re.search(r\"\\bMALE\\b\", l, re.IGNORECASE):\n",
        "            gender = \"Male\"\n",
        "            break\n",
        "        elif re.search(r\"\\bFEMALE\\b\", l, re.IGNORECASE):\n",
        "            gender = \"Female\"\n",
        "            break\n",
        "\n",
        "    # Face detection & crop\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(50, 50))\n",
        "\n",
        "    cropped_face_img = None\n",
        "    user_photo_base64 = None\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)[0]\n",
        "        cropped_face_img = img_bgr[y:y + h, x:x + w]\n",
        "        pil_face = Image.fromarray(cv2.cvtColor(cropped_face_img, cv2.COLOR_BGR2RGB))\n",
        "        buf = io.BytesIO()\n",
        "        pil_face.save(buf, format=\"PNG\")\n",
        "        user_photo_base64 = base64.b64encode(buf.getvalue()).decode()\n",
        "\n",
        "    # JSON output\n",
        "    data = {\n",
        "        \"aadhaar_number\": aadhaar_number,\n",
        "        \"name\": name,\n",
        "        \"dob\": dob,\n",
        "        \"gender\": gender,\n",
        "        \"user_photo_base64\": user_photo_base64\n",
        "    }\n",
        "\n",
        "    face_image_display = None\n",
        "    if cropped_face_img is not None:\n",
        "        face_image_display = cv2.cvtColor(cropped_face_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return json.dumps(data, indent=4, ensure_ascii=False), face_image_display\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Aadhaar Card Text & Photo Extractor\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_img = gr.Image(type=\"pil\", label=\"Upload Aadhaar Image\")\n",
        "            btn = gr.Button(\"Extract Data\")\n",
        "        with gr.Column():\n",
        "            output_json = gr.Textbox(label=\"Extracted Data (JSON)\", lines=15)\n",
        "            output_face = gr.Image(type=\"numpy\", label=\"Cropped User Photo\")\n",
        "\n",
        "    btn.click(fn=extract_aadhaar_data, inputs=input_img, outputs=[output_json, output_face])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "k4wBgVmtteMf",
        "outputId": "1c3f4602-5259-46e6-f4a5-7c7c84e59261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://18e18d40268cc7b728.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://18e18d40268cc7b728.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://18e18d40268cc7b728.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import base64\n",
        "import io\n",
        "import gradio as gr\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted headers and keep only meaningful name-like text.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    text = text.strip()\n",
        "    ignore_patterns = [\n",
        "        \"INCOME TAX\", \"GOVT OF INDIA\", \"GOVERNMENT OF INDIA\",\n",
        "        \"INCOME\", \"TAX\", \"DEPARTMENT\", \"Permanent Account Number\"\n",
        "    ]\n",
        "    for pat in ignore_patterns:\n",
        "        if pat.lower() in text.lower():\n",
        "            return None\n",
        "    return text\n",
        "\n",
        "def extract_pan_data(image):\n",
        "    img_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # OCR line-wise (clean non-ASCII)\n",
        "    raw_text = pytesseract.image_to_string(image, lang=\"eng\")\n",
        "    lines = [re.sub(r'[^\\x00-\\x7F]+', '', l).strip() for l in raw_text.split(\"\\n\") if l.strip()]\n",
        "\n",
        "    pan_number = None\n",
        "    name = None\n",
        "    father_name = None\n",
        "    dob = None\n",
        "\n",
        "    # PAN number search (ABCDE1234F)\n",
        "    for l in lines:\n",
        "        m = re.search(r\"\\b([A-Z]{5}[0-9]{4}[A-Z]{1})\\b\", l)\n",
        "        if m:\n",
        "            pan_number = m.group(1)\n",
        "            break\n",
        "\n",
        "    # DOB extraction & Name\n",
        "    for i, l in enumerate(lines):\n",
        "        dob_match = re.search(r\"(\\d{2}[\\/\\-]\\d{2}[\\/\\-]\\d{4})\", l)\n",
        "        if dob_match:\n",
        "            dob = dob_match.group(1)\n",
        "            if i > 0:\n",
        "                father_name = clean_text(lines[i-1])\n",
        "            if i > 1:\n",
        "                name = clean_text(lines[i-2])\n",
        "            break\n",
        "\n",
        "    # Face detection & crop\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(50, 50))\n",
        "\n",
        "    cropped_face_img = None\n",
        "    user_photo_base64 = None\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)[0]\n",
        "        cropped_face_img = img_bgr[y:y + h, x:x + w]\n",
        "        pil_face = Image.fromarray(cv2.cvtColor(cropped_face_img, cv2.COLOR_BGR2RGB))\n",
        "        buf = io.BytesIO()\n",
        "        pil_face.save(buf, format=\"PNG\")\n",
        "        user_photo_base64 = base64.b64encode(buf.getvalue()).decode()\n",
        "\n",
        "    # JSON output\n",
        "    data = {\n",
        "        \"pan_number\": pan_number,\n",
        "        \"name\": name,\n",
        "        \"father_name\": father_name,\n",
        "        \"dob\": dob,\n",
        "        \"user_photo_base64\": user_photo_base64\n",
        "    }\n",
        "\n",
        "    face_image_display = None\n",
        "    if cropped_face_img is not None:\n",
        "        face_image_display = cv2.cvtColor(cropped_face_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return json.dumps(data, indent=4, ensure_ascii=False), face_image_display\n",
        "\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## PAN Card Text & Photo Extractor\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_img = gr.Image(type=\"pil\", label=\"Upload PAN Image\")\n",
        "            btn = gr.Button(\"Extract Data\")\n",
        "        with gr.Column():\n",
        "            output_json = gr.Textbox(label=\"Extracted Data (JSON)\", lines=15)\n",
        "            output_face = gr.Image(type=\"numpy\", label=\"Cropped User Photo\")\n",
        "\n",
        "    btn.click(fn=extract_pan_data, inputs=input_img, outputs=[output_json, output_face])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "R_7_hi1Cwe6Z",
        "outputId": "9442791d-83ab-4dc5-fc78-b41fc983f23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e7c4ebd7fa0ecf4693.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e7c4ebd7fa0ecf4693.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e7c4ebd7fa0ecf4693.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image, method=\"standard\"):\n",
        "    \"\"\"Enhance image for better OCR with multiple preprocessing methods\"\"\"\n",
        "    img_array = np.array(image)\n",
        "    img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if method == \"standard\":\n",
        "        # Basic enhancement\n",
        "        denoised = cv2.fastNlMeansDenoising(gray)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        return clahe.apply(denoised)\n",
        "\n",
        "    elif method == \"sharp\":\n",
        "        # Sharpening for clearer text\n",
        "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
        "        return cv2.fastNlMeansDenoising(sharpened)\n",
        "\n",
        "    elif method == \"threshold\":\n",
        "        # Binary threshold for high contrast\n",
        "        denoised = cv2.fastNlMeansDenoising(gray)\n",
        "        _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        return thresh\n",
        "\n",
        "    return gray\n",
        "\n",
        "def clean_text(text, field_type=\"generic\"):\n",
        "    \"\"\"Clean and normalize extracted text\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "    if field_type == \"name\":\n",
        "        # Remove non-alphabetic characters but keep spaces and common name characters\n",
        "        cleaned = re.sub(r'[^A-Za-z\\s\\'-]', '', text).strip()\n",
        "        # Remove extra spaces and convert to title case\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).title()\n",
        "        # Handle common OCR corrections\n",
        "        cleaned = fix_common_ocr_errors(cleaned)\n",
        "        return cleaned\n",
        "    elif field_type == \"date\":\n",
        "        # Remove spaces and normalize separators\n",
        "        return re.sub(r'[^\\d/\\-]', '', text)\n",
        "    elif field_type == \"passport_number\":\n",
        "        # Keep only alphanumeric characters\n",
        "        return re.sub(r'[^A-Z0-9]', '', text.upper())\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def fix_common_ocr_errors(name):\n",
        "    \"\"\"Fix common OCR errors in names\"\"\"\n",
        "    if not name:\n",
        "        return name\n",
        "\n",
        "    # Common OCR substitutions\n",
        "    ocr_fixes = {\n",
        "        # Letter confusions\n",
        "        '0': 'O',  # Zero to O\n",
        "        '1': 'I',  # One to I (in names)\n",
        "        '5': 'S',  # Five to S\n",
        "        '8': 'B',  # Eight to B\n",
        "        '@': 'A',  # At symbol to A\n",
        "        '3': 'E',  # Three to E\n",
        "        '6': 'G',  # Six to G\n",
        "        # Common word fixes\n",
        "        'Fe male': 'Female',\n",
        "        'Ma le': 'Male',\n",
        "        'Fern': 'F',  # Common OCR error for F\n",
        "        'San': '',   # Common OCR garbage\n",
        "        'Fem': '',   # Common OCR garbage\n",
        "    }\n",
        "\n",
        "    result = name\n",
        "    for wrong, correct in ocr_fixes.items():\n",
        "        result = result.replace(wrong, correct)\n",
        "\n",
        "    return result\n",
        "\n",
        "def is_valid_name(name_str):\n",
        "    \"\"\"Enhanced name validation with better heuristics\"\"\"\n",
        "    if not name_str or len(name_str) < 2:\n",
        "        return False\n",
        "\n",
        "    # Remove common OCR artifacts and check if it's reasonable\n",
        "    cleaned = re.sub(r'[^A-Za-z\\s\\'-]', '', name_str).strip()\n",
        "\n",
        "    if not cleaned:\n",
        "        return False\n",
        "\n",
        "    # Check for reasonable characteristics of names\n",
        "    words = cleaned.split()\n",
        "    if len(words) == 0:\n",
        "        return False\n",
        "\n",
        "    # Names should have reasonable length words\n",
        "    valid_words = 0\n",
        "    for word in words:\n",
        "        if len(word) >= 2:  # At least 2 characters\n",
        "            valid_words += 1\n",
        "        elif len(word) == 1 and word.upper() in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
        "            valid_words += 1  # Single letters are OK (middle initials)\n",
        "\n",
        "    if valid_words == 0:\n",
        "        return False\n",
        "\n",
        "    # Check against common OCR errors that aren't names\n",
        "    ocr_garbage = [\n",
        "        'fem', 'san', 'the', 'and', 'for', 'with', 'date', 'sex', 'place',\n",
        "        'republic', 'india', 'passport', 'number', 'nationality', 'given',\n",
        "        'surname', 'birth', 'issue', 'expiry', 'authority', 'type', 'code',\n",
        "        'holder', 'male', 'female', 'country', 'document', 'personal'\n",
        "    ]\n",
        "\n",
        "    # Check if the entire cleaned text is just garbage\n",
        "    if cleaned.lower() in ocr_garbage:\n",
        "        return False\n",
        "\n",
        "    # Check if it's mostly garbage words\n",
        "    garbage_count = sum(1 for word in words if word.lower() in ocr_garbage)\n",
        "    if len(words) > 1 and garbage_count >= len(words) // 2:\n",
        "        return False\n",
        "\n",
        "    # Check for patterns that suggest OCR errors\n",
        "    error_patterns = [\n",
        "        r'^[A-Z]{1,2}\\s+[A-Z]{1,2}$',  # Very short words like \"FE MA\"\n",
        "        r'.*\\d.*',  # Names shouldn't contain numbers\n",
        "        r'^[^A-Za-z]*$',  # Should contain at least some letters\n",
        "    ]\n",
        "\n",
        "    for pattern in error_patterns:\n",
        "        if re.match(pattern, cleaned):\n",
        "            return False\n",
        "\n",
        "    # Additional validation: check if it looks like a real name structure\n",
        "    # Names typically have 1-4 words, each 2-20 characters long\n",
        "    if len(words) > 4:\n",
        "        return False\n",
        "\n",
        "    for word in words:\n",
        "        if len(word) > 20:  # Unusually long word\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def extract_name_candidates(text):\n",
        "    \"\"\"Extract potential name candidates from text using multiple strategies\"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    # Strategy 1: Context-based extraction\n",
        "    name_contexts = [\n",
        "        # Surname patterns\n",
        "        (r'(?:Surname|Family\\s*Name|Last\\s*Name)[\\s:]*([A-Z][A-Za-z\\s\\'-]+?)(?=\\s*(?:\\n|Given|First|Christian|Forename|Nationality|Sex|Date|Place|$))', 'surname'),\n",
        "        (r'(?:^|\\n)([A-Z][A-Z\\s]{3,25})(?=\\s*\\n.*(?:Given|First|Christian|Forename))', 'surname'),\n",
        "\n",
        "        # Given names patterns\n",
        "        (r'(?:Given\\s*Names?|First\\s*Names?|Christian\\s*Names?|Forenames?)[\\s:]*([A-Z][A-Za-z\\s\\'-]+?)(?=\\s*(?:\\n|Nationality|Sex|Date|Place|$))', 'given_names'),\n",
        "        (r'(?:Given\\s*Names?|First\\s*Names?)[\\s:]*\\n?\\s*([A-Z][A-Za-z\\s\\'-]+)', 'given_names'),\n",
        "\n",
        "        # General name patterns (when context is unclear)\n",
        "        (r'(?:^|\\n)([A-Z][A-Z\\s]{2,25})(?=\\s*\\n)', 'general'),\n",
        "        (r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', 'general'),\n",
        "    ]\n",
        "\n",
        "    for pattern, name_type in name_contexts:\n",
        "        matches = re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            candidate = clean_text(match.group(1), \"name\")\n",
        "            if candidate and is_valid_name(candidate):\n",
        "                candidates.append({\n",
        "                    'text': candidate,\n",
        "                    'type': name_type,\n",
        "                    'confidence': calculate_name_confidence(candidate, name_type, text),\n",
        "                    'source': 'context'\n",
        "                })\n",
        "\n",
        "    # Strategy 2: MRZ extraction\n",
        "    mrz_candidates = extract_mrz_names(text)\n",
        "    candidates.extend(mrz_candidates)\n",
        "\n",
        "    # Strategy 3: Line-by-line analysis for passport format\n",
        "    lines = text.split('\\n')\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if len(line) > 3 and re.match(r'^[A-Z][A-Z\\s]{2,}$', line):\n",
        "            candidate = clean_text(line, \"name\")\n",
        "            if candidate and is_valid_name(candidate):\n",
        "                # Determine type based on position and context\n",
        "                name_type = 'general'\n",
        "                if i < len(lines) - 1:\n",
        "                    next_line = lines[i + 1].lower()\n",
        "                    if any(word in next_line for word in ['given', 'first', 'christian', 'forename']):\n",
        "                        name_type = 'surname'\n",
        "                    elif any(word in next_line for word in ['nationality', 'sex', 'date']):\n",
        "                        name_type = 'given_names'\n",
        "\n",
        "                candidates.append({\n",
        "                    'text': candidate,\n",
        "                    'type': name_type,\n",
        "                    'confidence': calculate_name_confidence(candidate, name_type, text),\n",
        "                    'source': 'line_analysis'\n",
        "                })\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def extract_mrz_names(text):\n",
        "    \"\"\"Extract names from MRZ format\"\"\"\n",
        "    candidates = []\n",
        "\n",
        "    # MRZ pattern for names: P<COUNTRY<SURNAME<<GIVEN_NAMES<<<\n",
        "    mrz_patterns = [\n",
        "        r'P<[A-Z]{3}<([A-Z<]+)<<([A-Z<]*)',\n",
        "        r'P<[A-Z]{3}<([A-Z<]+)<([A-Z<]*)',\n",
        "    ]\n",
        "\n",
        "    for pattern in mrz_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            surname_mrz = match.group(1).replace('<', ' ').strip()\n",
        "            given_names_mrz = match.group(2).replace('<', ' ').strip()\n",
        "\n",
        "            if surname_mrz:\n",
        "                surname_clean = clean_text(surname_mrz, \"name\")\n",
        "                if surname_clean and is_valid_name(surname_clean):\n",
        "                    candidates.append({\n",
        "                        'text': surname_clean,\n",
        "                        'type': 'surname',\n",
        "                        'confidence': 0.9,  # MRZ is usually reliable\n",
        "                        'source': 'mrz'\n",
        "                    })\n",
        "\n",
        "            if given_names_mrz:\n",
        "                given_clean = clean_text(given_names_mrz, \"name\")\n",
        "                if given_clean and is_valid_name(given_clean):\n",
        "                    candidates.append({\n",
        "                        'text': given_clean,\n",
        "                        'type': 'given_names',\n",
        "                        'confidence': 0.9,\n",
        "                        'source': 'mrz'\n",
        "                    })\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def calculate_name_confidence(name, name_type, full_text):\n",
        "    \"\"\"Calculate confidence score for a name candidate\"\"\"\n",
        "    confidence = 0.5  # Base confidence\n",
        "\n",
        "    # Length-based scoring\n",
        "    if 3 <= len(name) <= 20:\n",
        "        confidence += 0.2\n",
        "    elif 2 <= len(name) <= 30:\n",
        "        confidence += 0.1\n",
        "\n",
        "    # Word count scoring\n",
        "    words = name.split()\n",
        "    if name_type == 'surname' and len(words) <= 2:\n",
        "        confidence += 0.2\n",
        "    elif name_type == 'given_names' and 1 <= len(words) <= 3:\n",
        "        confidence += 0.2\n",
        "\n",
        "    # Character composition\n",
        "    if re.match(r'^[A-Za-z\\s\\'-]+$', name):\n",
        "        confidence += 0.2\n",
        "\n",
        "    # Context clues in surrounding text\n",
        "    name_lower = name.lower()\n",
        "    context_words = ['surname', 'given', 'first', 'name', 'names']\n",
        "    for word in context_words:\n",
        "        if word in full_text.lower():\n",
        "            confidence += 0.1\n",
        "            break\n",
        "\n",
        "    # Penalize if it looks like OCR garbage\n",
        "    if any(garbage in name_lower for garbage in ['fem', 'san', 'xxx']):\n",
        "        confidence -= 0.3\n",
        "\n",
        "    return min(1.0, max(0.0, confidence))\n",
        "\n",
        "def select_best_name(candidates, name_type):\n",
        "    \"\"\"Select the best name from candidates based on confidence and validation\"\"\"\n",
        "    if not candidates:\n",
        "        return None\n",
        "\n",
        "    # Filter by type\n",
        "    type_candidates = [c for c in candidates if c['type'] == name_type or c['type'] == 'general']\n",
        "\n",
        "    if not type_candidates:\n",
        "        return None\n",
        "\n",
        "    # Sort by confidence and pick the best\n",
        "    type_candidates.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "    # Additional validation for the top candidate\n",
        "    best = type_candidates[0]\n",
        "    if best['confidence'] < 0.3:  # Too low confidence\n",
        "        return None\n",
        "\n",
        "    return best['text']\n",
        "\n",
        "def validate_date(date_str):\n",
        "    \"\"\"Validate if date string is reasonable\"\"\"\n",
        "    if not date_str:\n",
        "        return False\n",
        "\n",
        "    # Extract day, month, year\n",
        "    parts = re.split(r'[/-]', date_str)\n",
        "    if len(parts) != 3:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        day, month, year = map(int, parts)\n",
        "        # Basic validation\n",
        "        if not (1 <= month <= 12 and 1 <= day <= 31 and 1900 <= year <= 2030):\n",
        "            return False\n",
        "        # Additional checks for invalid dates like 40th month\n",
        "        if month > 12 or day > 31:\n",
        "            return False\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def extract_from_mrz(text, existing_data):\n",
        "    \"\"\"Extract information from Machine Readable Zone (MRZ)\"\"\"\n",
        "    data = existing_data.copy()\n",
        "\n",
        "    # Pattern for first MRZ line: P<COUNTRY<SURNAME<<GIVEN_NAMES<<<\n",
        "    mrz1_pattern = r'P<([A-Z]{3})<([A-Z<]+)<<([A-Z<]*)'\n",
        "    mrz1_match = re.search(mrz1_pattern, text)\n",
        "\n",
        "    if mrz1_match:\n",
        "        country_code = mrz1_match.group(1)\n",
        "        surname = mrz1_match.group(2).replace('<', '')\n",
        "        given_names = mrz1_match.group(3).replace('<', ' ').strip()\n",
        "\n",
        "        # Map common country codes\n",
        "        country_mapping = {\n",
        "            'IND': 'Indian', 'USA': 'American', 'GBR': 'British',\n",
        "            'CAN': 'Canadian', 'AUS': 'Australian', 'DEU': 'German',\n",
        "            'FRA': 'French', 'ITA': 'Italian', 'ESP': 'Spanish'\n",
        "        }\n",
        "\n",
        "        if not data.get('nationality'):\n",
        "            data['nationality'] = country_mapping.get(country_code, country_code)\n",
        "        if not data.get('surname') and surname:\n",
        "            data['surname'] = clean_text(surname, \"name\")\n",
        "        if not data.get('given_names') and given_names:\n",
        "            data['given_names'] = clean_text(given_names, \"name\")\n",
        "\n",
        "    # Pattern for second MRZ line: PASSPORT<COUNTRY<DOB<SEX<EXPIRY<\n",
        "    mrz2_pattern = r'([A-Z][0-9]{7,8})<([A-Z]{3})<([0-9]{6})<([MF])<([0-9]{6})'\n",
        "    mrz2_match = re.search(mrz2_pattern, text)\n",
        "\n",
        "    if mrz2_match:\n",
        "        passport_num = mrz2_match.group(1)\n",
        "        dob_yymmdd = mrz2_match.group(3)\n",
        "        sex = mrz2_match.group(4)\n",
        "\n",
        "        if not data.get('passport_number'):\n",
        "            data['passport_number'] = passport_num\n",
        "\n",
        "        if not data.get('dob') and len(dob_yymmdd) == 6:\n",
        "            yy, mm, dd = dob_yymmdd[:2], dob_yymmdd[2:4], dob_yymmdd[4:6]\n",
        "            # Convert 2-digit year to 4-digit (assumes 19xx for 50-99, 20xx for 00-49)\n",
        "            year = f\"19{yy}\" if int(yy) >= 50 else f\"20{yy}\"\n",
        "            formatted_date = f\"{dd}/{mm}/{year}\"\n",
        "            if validate_date(formatted_date):\n",
        "                data['dob'] = formatted_date\n",
        "\n",
        "        if not data.get('sex'):\n",
        "            data['sex'] = sex\n",
        "\n",
        "    return data\n",
        "\n",
        "def extract_passport_data(text):\n",
        "    \"\"\"Extract passport data using enhanced name extraction\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # 1. Enhanced Name Extraction\n",
        "        name_candidates = extract_name_candidates(text)\n",
        "\n",
        "        # Select best surname and given names\n",
        "        surname = select_best_name(name_candidates, 'surname')\n",
        "        given_names = select_best_name(name_candidates, 'given_names')\n",
        "\n",
        "        if surname:\n",
        "            data['surname'] = surname\n",
        "        if given_names:\n",
        "            data['given_names'] = given_names\n",
        "\n",
        "\n",
        "        # 2. Passport Number - flexible patterns\n",
        "        passport_patterns = [\n",
        "            r'\\b([A-Z][0-9]{7,8})\\b',  # Standard: Letter + 7-8 digits\n",
        "            r'(?:Passport\\s*(?:No|Number)[\\s.:]*)?([A-Z][0-9]{7,8})',\n",
        "            r'(?:IND|USA|GBR|CAN|AUS)\\s+([A-Z][0-9]{7,8})',  # Country code + passport\n",
        "        ]\n",
        "\n",
        "        for pattern in passport_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                passport_num = clean_text(match.group(1), \"passport_number\")\n",
        "                if len(passport_num) >= 8:  # Ensure minimum length\n",
        "                    data['passport_number'] = passport_num\n",
        "                    break\n",
        "\n",
        "        # 3. Nationality - improved patterns and validation\n",
        "        nationality_patterns = [\n",
        "            r'(?:Nationality|Citizen(?:ship)?|Country)[\\s:]*([A-Z][A-Z\\s]+?)(?:\\s*(?:\\n|Sex|Date|Place))',\n",
        "            r'\\b(Indian|American|British|Canadian|Australian|German|French|Italian|Spanish|Chinese|Japanese|Korean|Brazilian|Mexican|Russian)\\b',\n",
        "        ]\n",
        "\n",
        "        for pattern in nationality_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                nationality = clean_text(match.group(1), \"name\")\n",
        "                # Validate nationality (must be reasonable length and not a common OCR error)\n",
        "                if (nationality and len(nationality) > 3 and\n",
        "                    nationality.lower() not in ['fem', 'feu', 'san', 'the', 'and', 'date', 'sex']):\n",
        "                    data['nationality'] = nationality\n",
        "                    break\n",
        "\n",
        "        # Additional nationality fix: if we got gibberish, try to find \"INDIAN\" specifically\n",
        "        if not data.get('nationality') or len(data.get('nationality', '')) < 4:\n",
        "            indian_match = re.search(r'[IF]NDIAN|IND[IF]AN|[IF]ND[IF]AN', text, re.IGNORECASE)\n",
        "            if indian_match:\n",
        "                data['nationality'] = 'Indian'\n",
        "\n",
        "        # 4. Sex - contextual patterns\n",
        "        sex_patterns = [\n",
        "            r'(?:Sex|Gender)[\\s:]*([MF]|Male|Female)',\n",
        "            r'\\b([MF])\\b(?=.*\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',  # M/F near a date\n",
        "            r'(?:Indian|American|British)\\s+([MF])\\s+\\d',  # Nationality + Sex + date pattern\n",
        "        ]\n",
        "\n",
        "        for pattern in sex_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                sex_value = match.group(1).upper()\n",
        "                data['sex'] = 'M' if sex_value in ['M', 'MALE'] else 'F'\n",
        "                break\n",
        "\n",
        "        # 5. Date of Birth - with validation\n",
        "        dob_patterns = [\n",
        "            r'(?:Date\\s*of\\s*Birth|DOB|Born)[\\s:]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'[MF]\\s+(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',  # After sex field\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-](?:19|20)\\d{2})\\b',  # Date with 4-digit year\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                date_candidate = clean_text(match.group(1), \"date\")\n",
        "                if validate_date(date_candidate):\n",
        "                    data['dob'] = date_candidate\n",
        "                    break\n",
        "            if data.get('dob'):\n",
        "                break\n",
        "\n",
        "        # 6. MRZ fallback for any missing fields\n",
        "        data = extract_from_mrz(text, data)\n",
        "\n",
        "    except Exception as e:\n",
        "        data['error'] = f\"Extraction error: {str(e)}\"\n",
        "\n",
        "    return data\n",
        "\n",
        "def process_passport(image):\n",
        "    \"\"\"Process passport image with multiple OCR strategies\"\"\"\n",
        "    if image is None:\n",
        "        return {}, \"\"\n",
        "\n",
        "    try:\n",
        "        all_texts = []\n",
        "\n",
        "        # Strategy 1: Standard OCR\n",
        "        text1 = pytesseract.image_to_string(image, lang='eng')\n",
        "        all_texts.append((\"STANDARD\", text1))\n",
        "\n",
        "        # Strategy 2: Multiple preprocessing methods\n",
        "        for method in [\"standard\", \"sharp\", \"threshold\"]:\n",
        "            enhanced_img = preprocess_image(image, method)\n",
        "            enhanced_pil = Image.fromarray(enhanced_img)\n",
        "            text = pytesseract.image_to_string(enhanced_pil, lang='eng',\n",
        "                                            config='--oem 3 --psm 6')\n",
        "            all_texts.append((f\"ENHANCED_{method.upper()}\", text))\n",
        "\n",
        "        # Strategy 3: MRZ-focused extraction (bottom region)\n",
        "        img_array = np.array(image)\n",
        "        h, w = img_array.shape[:2]\n",
        "        mrz_region = img_array[int(h*0.75):, :]\n",
        "        mrz_img = Image.fromarray(mrz_region)\n",
        "        text_mrz = pytesseract.image_to_string(mrz_img, lang='eng',\n",
        "                                            config='--psm 7 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ<')\n",
        "        all_texts.append((\"MRZ\", text_mrz))\n",
        "\n",
        "        # Strategy 4: Try different PSM modes for problematic text\n",
        "        for psm in [3, 6, 7, 8]:\n",
        "            text_psm = pytesseract.image_to_string(image, lang='eng',\n",
        "                                                config=f'--oem 3 --psm {psm}')\n",
        "            all_texts.append((f\"PSM_{psm}\", text_psm))\n",
        "\n",
        "        # Combine all text sources\n",
        "        combined_sections = []\n",
        "        for label, text in all_texts:\n",
        "            if text.strip():\n",
        "                combined_sections.append(f\"=== {label} ===\\n{text}\")\n",
        "\n",
        "        combined_text = \"\\n\\n\".join(combined_sections)\n",
        "\n",
        "        # Extract data from combined text\n",
        "        extracted_data = extract_passport_data(combined_text)\n",
        "\n",
        "        return extracted_data, combined_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Processing failed: {str(e)}\"}, \"\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Enhanced Passport OCR\") as demo:\n",
        "    gr.Markdown(\"### üõÇ Enhanced Passport OCR with Improved Name Extraction\")\n",
        "    gr.Markdown(\"Advanced pattern matching with multiple validation strategies and OCR error correction.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Image(type=\"pil\", label=\"Upload Passport Image\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            extracted = gr.JSON(label=\"Extracted Data\")\n",
        "        with gr.Column():\n",
        "            raw_text = gr.Textbox(label=\"OCR Text (All Sources)\", lines=15)\n",
        "\n",
        "    inp.change(fn=process_passport, inputs=inp, outputs=[extracted, raw_text])\n",
        "\n",
        "    gr.Markdown(\"### Enhanced Features:\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    - **Advanced Name Extraction**: Multiple strategies including context-based, MRZ, and line analysis\n",
        "    - **OCR Error Correction**: Fixes common character substitutions (0‚ÜíO, 1‚ÜíI, etc.)\n",
        "    - **Confidence Scoring**: Rates name candidates and selects the most reliable\n",
        "    - **Smart Validation**: Enhanced filtering of OCR artifacts and false positives\n",
        "    - **Robust Pattern Matching**: Handles various passport formats and layouts\n",
        "    - **Multiple Fallback Methods**: Uses different OCR approaches when primary methods fail\n",
        "    \"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "vTbtzfwV5w3K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "98bd38da-50c3-4c61-a19b-077d97381e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6b1a521c5e4a9ecb80.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6b1a521c5e4a9ecb80.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}