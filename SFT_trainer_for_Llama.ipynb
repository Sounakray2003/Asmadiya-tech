{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sounakray2003/Asmadiya-tech/blob/main/SFT_trainer_for_Llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0DfRdkdEQhe"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes xformers datasets huggingface_hub torch --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7etta3FEyG0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()  # Paste your HF token (must accept Llama license)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5aXfcCSFU4L"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# QLoRA Fine-Tuning: Llama-3.2-1B-Instruct\n",
        "# 4-bit + LoRA | Free Colab T4 | ~10-15 mins | 1k samples\n",
        "# =====================================================\n",
        "\n",
        "# --- CELL 1: Install ---\n",
        "!pip install -q bitsandbytes accelerate peft trl transformers datasets huggingface_hub\n",
        "\n",
        "# --- CELL 2: HF Login ---\n",
        "from huggingface_hub import notebook_login\n",
        "print(\"Paste your Hugging Face token (required for Llama):\")\n",
        "notebook_login()\n",
        "\n",
        "# --- CELL 3: Load 4-bit Model + LoRA ---\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer + add [PAD]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load 4-bit model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\",\n",
        "    token=True,\n",
        ")\n",
        "\n",
        "# Resize embeddings for [PAD]\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Prepare for QLoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(f\"VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "# --- CELL 4: Load & Format Dataset ---\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def format(examples):\n",
        "    texts = [alpaca_prompt.format(i, o) + EOS_TOKEN for i, o in zip(examples[\"instruction\"], examples[\"output\"])]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(format, batched=True, remove_columns=dataset.column_names)\n",
        "dataset = dataset.shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# --- CELL 5: Tokenize ---\n",
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# --- CELL 6: Data Collator ---\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collator(features):\n",
        "    input_ids = [f[\"input_ids\"] for f in features]\n",
        "    attention_mask = [f[\"attention_mask\"] for f in features]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_loader = DataLoader(tokenized, batch_size=2, shuffle=True, collate_fn=collator)\n",
        "\n",
        "# --- CELL 7: Training Loop ---\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
        "num_epochs = 1\n",
        "total_steps = len(train_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=10, num_training_steps=total_steps)\n",
        "\n",
        "model.train()\n",
        "accum_steps = 4\n",
        "step = 0\n",
        "\n",
        "print(f\"Starting QLoRA training – {total_steps} steps\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        step += 1\n",
        "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss / accum_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if step % accum_steps == 0:\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Step {step} | Loss: {loss.item()*accum_steps:.4f} | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# --- CELL 8: Save LoRA + Merge ---\n",
        "lora_dir = \"llama32-1b-qlora\"\n",
        "model.save_pretrained(lora_dir)\n",
        "tokenizer.save_pretrained(lora_dir)\n",
        "print(f\"LoRA adapter saved: ~30 MB → {lora_dir}\")\n",
        "\n",
        "# Merge into full 16-bit\n",
        "if input(\"Merge & save full 16-bit model? (y/n): \").lower() == \"y\":\n",
        "    from peft import PeftModel\n",
        "\n",
        "    print(\"Loading base model with [PAD] token...\")\n",
        "    tokenizer_merged = AutoTokenizer.from_pretrained(model_name, token=True)\n",
        "    if tokenizer_merged.pad_token is None:\n",
        "        tokenizer_merged.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        token=True,\n",
        "    )\n",
        "    base_model.resize_token_embeddings(len(tokenizer_merged))\n",
        "\n",
        "    print(\"Merging LoRA...\")\n",
        "    model_peft = PeftModel.from_pretrained(base_model, lora_dir)\n",
        "    merged_model = model_peft.merge_and_unload()\n",
        "\n",
        "    merged_dir = \"llama32-1b-qlora-merged\"\n",
        "    merged_model.save_pretrained(merged_dir)\n",
        "    tokenizer_merged.save_pretrained(merged_dir)\n",
        "    print(f\"Merged 16-bit model saved: ~2 GB → {merged_dir}\")\n",
        "\n",
        "# --- CELL 9: Inference (on merged model) ---\n",
        "from transformers import pipeline\n",
        "\n",
        "merged_dir = \"llama32-1b-qlora-merged\"\n",
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=merged_dir,\n",
        "    tokenizer=merged_dir,\n",
        "    device=0,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "What is the capital of Japan?\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nGenerating...\")\n",
        "out = gen(prompt, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
        "response = out[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
        "print(f\"Model says:\\n{response}\")\n",
        "\n",
        "print(\"\\nAll done! Full QLoRA pipeline complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# QLoRA Fine-Tuning: Llama-3.2-1B-Instruct (Custom JSON Dataset)\n",
        "# 4-bit + LoRA | Free Colab T4 | ~10-15 mins | Your JSON Dataset\n",
        "# =====================================================\n",
        "\n",
        "# --- CELL 1: Install ---\n",
        "!pip install -q bitsandbytes accelerate peft trl transformers datasets huggingface_hub\n",
        "\n",
        "# --- CELL 2: HF Login ---\n",
        "from huggingface_hub import notebook_login\n",
        "print(\"Paste your Hugging Face token (required for Llama):\")\n",
        "notebook_login()\n",
        "\n",
        "# --- CELL 3: Prepare Custom Dataset ---\n",
        "# Step 1: Save your JSON to 'dataset.json' (copy from your message)\n",
        "# In Colab Files panel: Create 'dataset.json', paste the JSON array, save.\n",
        "\n",
        "# Step 2: Load the JSON\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"dataset.json\")[\"train\"]\n",
        "print(f\"Loaded custom dataset with {len(dataset)} examples\")\n",
        "print(f\"Columns: {dataset.column_names}\")\n",
        "\n",
        "# --- CELL 4: Load 4-bit Model + LoRA ---\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# 4-bit config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer + add [PAD]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load 4-bit model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\",\n",
        "    token=True,\n",
        ")\n",
        "\n",
        "# Resize embeddings for [PAD]\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Prepare for QLoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(f\"VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "# --- CELL 5: Format Dataset ---\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def format(examples):\n",
        "    # Concat 'input' to 'instruction' if present and non-empty\n",
        "    instructions = examples[\"instruction\"]\n",
        "    if \"input\" in examples:\n",
        "        instructions = [f\"{instr}\\n{examples['input'][i]}\" if examples['input'][i] else instr\n",
        "                        for i, instr in enumerate(examples[\"instruction\"])]\n",
        "\n",
        "    texts = [alpaca_prompt.format(instr, out) + EOS_TOKEN\n",
        "             for instr, out in zip(instructions, examples[\"output\"])]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(format, batched=True)\n",
        "dataset = dataset.shuffle(seed=42).select(range(min(1000, len(dataset))))  # Use up to 1000 examples\n",
        "\n",
        "print(f\"Formatted custom dataset ready – {len(dataset)} examples\")\n",
        "\n",
        "# --- CELL 6: Tokenize ---\n",
        "def tokenize(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# --- CELL 7: Data Collator ---\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collator(features):\n",
        "    input_ids = [f[\"input_ids\"] for f in features]\n",
        "    attention_mask = [f[\"attention_mask\"] for f in features]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "\n",
        "    labels = input_ids.clone()\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_loader = DataLoader(tokenized, batch_size=2, shuffle=True, collate_fn=collator)\n",
        "\n",
        "# --- CELL 8: Training Loop ---\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
        "num_epochs = 4\n",
        "total_steps = len(train_loader)\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=10, num_training_steps=total_steps)\n",
        "\n",
        "model.train()\n",
        "accum_steps = 4\n",
        "step = 0\n",
        "\n",
        "print(f\"Starting QLoRA training on custom dataset – {total_steps} steps\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        step += 1\n",
        "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss / accum_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if step % accum_steps == 0:\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Step {step} | Loss: {loss.item()*accum_steps:.4f} | VRAM: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# --- CELL 9: Save LoRA + Merge ---\n",
        "lora_dir = \"llama32-1b-qlora-custom\"\n",
        "model.save_pretrained(lora_dir)\n",
        "tokenizer.save_pretrained(lora_dir)\n",
        "print(f\"LoRA adapter saved: ~30 MB → {lora_dir}\")\n",
        "\n",
        "# Merge into full 16-bit\n",
        "if input(\"Merge & save full 16-bit model? (y/n): \").lower() == \"y\":\n",
        "    from peft import PeftModel\n",
        "\n",
        "    print(\"Loading base model with [PAD] token...\")\n",
        "    tokenizer_merged = AutoTokenizer.from_pretrained(model_name, token=True)\n",
        "    if tokenizer_merged.pad_token is None:\n",
        "        tokenizer_merged.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        token=True,\n",
        "    )\n",
        "    base_model.resize_token_embeddings(len(tokenizer_merged))\n",
        "\n",
        "    print(\"Merging LoRA...\")\n",
        "    model_peft = PeftModel.from_pretrained(base_model, lora_dir)\n",
        "    merged_model = model_peft.merge_and_unload()\n",
        "\n",
        "    merged_dir = \"llama32-1b-qlora-merged-custom\"\n",
        "    merged_model.save_pretrained(merged_dir)\n",
        "    tokenizer_merged.save_pretrained(merged_dir)\n",
        "    print(f\"Merged 16-bit model saved: ~2 GB → {merged_dir}\")\n",
        "\n",
        "# --- CELL 10: Inference (on merged model) ---\n",
        "from transformers import pipeline\n",
        "\n",
        "merged_dir = \"llama32-1b-qlora-merged-custom\"\n",
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=merged_dir,\n",
        "    tokenizer=merged_dir,\n",
        "    device=0,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "What is the capital of Japan?\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nGenerating...\")\n",
        "out = gen(prompt, max_new_tokens=64, do_sample=True, temperature=0.7)\n",
        "response = out[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
        "print(f\"Model says:\\n{response}\")\n",
        "\n",
        "print(\"\\nAll done! QLoRA on custom JSON dataset complete.\")"
      ],
      "metadata": {
        "id": "qxkSJybujgDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- QUICK TEST: Ask about Asmadiya ---\n",
        "from transformers import pipeline\n",
        "\n",
        "gen = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"llama32-1b-qlora-merged-custom\",\n",
        "    tokenizer=\"llama32-1b-qlora-merged-custom\",\n",
        "    device=0,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "def ask_question(instruction):\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "    out = gen(prompt, max_new_tokens=128, do_sample=True, temperature=0.7)\n",
        "    return out[0][\"generated_text\"].split(\"### Response:\")[-1].strip()\n",
        "\n",
        "# Test questions\n",
        "print(\"Q1:\", ask_question(\"What services does Asmadiya Technologies offer?\"))\n",
        "print(\"\\nQ2:\", ask_question(\"What is AlphaTrain?\"))\n",
        "print(\"\\nQ3:\", ask_question(\"Who is the CEO of Asmadiya Technologies?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoBbG72lpElU",
        "outputId": "a9d5e997-2f8b-4fb0-fc2d-e2c2b32b0e46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: Services: Software Development, AI/ML, Cybersecurity, DevOps, Quality Assurance, Mobile App Development, IT Consulting.\n",
            "\n",
            "Q2: AlphaTrain is an AI/ML platform for training multiple models simultaneously, providing parallel training and real-time monitoring.\n",
            "\n",
            "Q3: Ashish Mishra is the CEO of Asmadiya Technologies.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNJXaKiiejPi4TN+72GaI0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}