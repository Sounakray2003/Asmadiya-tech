{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sounakray2003/Asmadiya-tech/blob/main/KYC_OCR_AWS_Textract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics wandb gradio boto3"
      ],
      "metadata": {
        "id": "XcfUm2zcsZxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99b435a-7f1f-4d15-d6cb-85ba930121f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import boto3\n",
        "import gradio as gr\n",
        "import wandb\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "u363tvqHAvUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffdf06f-3f13-4c19-bc4d-bf704e257772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "euTQK3qQgLEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6139b8b-e31a-49b2-8e4a-19e4b70891e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Haar cascade for face detection\n",
        "!wget -q https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
      ],
      "metadata": {
        "id": "neIG4BYk_t2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set AWS credentials from Colab Secrets\n",
        "try:\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "    os.environ['AWS_DEFAULT_REGION'] = userdata.get('AWS_DEFAULT_REGION')\n",
        "    print(\"AWS credentials loaded from Colab Secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Colab Secrets: {str(e)}\")\n",
        "    raise Exception(\"Failed to load AWS credentials from Colab Secrets. Check Secrets tab.\")"
      ],
      "metadata": {
        "id": "dphHnAaAgQ10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11e0594-da86-4914-afa0-67f4f7daeabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AWS credentials loaded from Colab Secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize AWS Textract client\n",
        "try:\n",
        "    textract = boto3.client('textract', region_name='ap-south-1')\n",
        "    print(\"Textract client initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Textract client: {str(e)}\")\n",
        "    raise Exception(\"Failed to initialize Textract client.\")"
      ],
      "metadata": {
        "id": "-1-kRiK_glU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbaa5e10-9dcb-4645-9dd7-a8d530cc4298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textract client initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set up WandB\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"document_classification_ocr\",\n",
        "    name=\"yolov11_classification_with_ocr\",\n",
        "    config={\n",
        "        \"img_size\": 224\n",
        "    }\n",
        ")\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "-eAhn7RPrjdS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "0f11af01-5dc7-42e3-c0f3-38f2462d82ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajputayushsingh07042004\u001b[0m (\u001b[33mrajputayushsingh07042004-asmadiya-technologies\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250819_052906-15ev9574</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr/runs/15ev9574' target=\"_blank\">yolov11_classification_with_ocr</a></strong> to <a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr' target=\"_blank\">https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr/runs/15ev9574' target=\"_blank\">https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr/runs/15ev9574</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained YOLOv11 model\n",
        "model_path = '/content/drive/MyDrive/document_classifier_yolo12.pt'\n",
        "try:\n",
        "    model = YOLO(model_path)\n",
        "    print(f\"Loaded model from: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise Exception(\"Failed to load the model. Ensure the file exists and is a valid YOLO model.\")\n"
      ],
      "metadata": {
        "id": "0K2onKy0rkeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74985de-cc4f-40a2-e4ed-79f8c930e1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from: /content/drive/MyDrive/document_classifier_yolo12.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess image for OCR\n",
        "def preprocess_image_for_ocr(image):\n",
        "    try:\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        # Noise reduction\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        # Binarization\n",
        "        _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        print(\"Image preprocessed for OCR\")\n",
        "        return binary\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "4AmpyAgWrs1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Blur detection\n",
        "def is_blurry(image, threshold=100):\n",
        "    try:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
        "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        wandb.log({'laplacian_variance': laplacian_var})\n",
        "        return laplacian_var < threshold\n",
        "    except Exception as e:\n",
        "        print(f\"Error in blur detection: {str(e)}\")\n",
        "        return True  # Assume blurry on error"
      ],
      "metadata": {
        "id": "CC7100JXrufy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced text cleaning function\n",
        "def clean_extracted_text(text, field_type=\"general\"):\n",
        "    \"\"\"Clean extracted text based on field type\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove common OCR artifacts\n",
        "    text = re.sub(r'[^\\w\\s/.-]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    if field_type == \"date\":\n",
        "        # Extract date patterns\n",
        "        date_match = re.search(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}\\b', text)\n",
        "        if date_match:\n",
        "            return date_match.group(0)\n",
        "    elif field_type == \"number\":\n",
        "        # Extract only digits and spaces\n",
        "        return re.sub(r'[^\\d\\s]', '', text).strip()\n",
        "    elif field_type == \"name\":\n",
        "        # Clean name - remove extra spaces and common prefixes\n",
        "        text = re.sub(r'(?i)^(mr\\.?|ms\\.?|mrs\\.?|dr\\.?)\\s*', '', text)\n",
        "        # Remove trailing unwanted text that might be concatenated\n",
        "        text = re.sub(r'\\s+(will|shall|and|the|of|for|in|at|to|from).*$', '', text, flags=re.IGNORECASE)\n",
        "        # Keep only alphabetic characters and spaces\n",
        "        text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
        "        # Remove extra spaces and capitalize properly\n",
        "        text = ' '.join(text.split())\n",
        "        return text.upper() if text else \"\"\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "HOIyq5Gw-7cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract key-value pairs from Textract response with enhanced logic\n",
        "def extract_key_value_pairs(textract_response, doc_type):\n",
        "    \"\"\"Extract key-value pairs using layout-based logic for each document type\"\"\"\n",
        "    key_value_pairs = {}\n",
        "\n",
        "    # Get all text blocks with their bounding boxes for layout analysis\n",
        "    text_blocks = []\n",
        "    line_blocks = []\n",
        "\n",
        "    try:\n",
        "        for block in textract_response['Blocks']:\n",
        "            if block['BlockType'] == 'WORD' and 'Text' in block:\n",
        "                text_blocks.append({\n",
        "                    'text': block['Text'],\n",
        "                    'bbox': block['Geometry']['BoundingBox'],\n",
        "                    'confidence': block.get('Confidence', 0)\n",
        "                })\n",
        "            elif block['BlockType'] == 'LINE' and 'Text' in block:\n",
        "                line_blocks.append({\n",
        "                    'text': block['Text'],\n",
        "                    'bbox': block['Geometry']['BoundingBox'],\n",
        "                    'confidence': block.get('Confidence', 0)\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Textract blocks: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "    # Sort blocks by position (top to bottom, left to right)\n",
        "    line_blocks.sort(key=lambda x: (x['bbox']['Top'], x['bbox']['Left']))\n",
        "\n",
        "    # Full text for fallback regex matching\n",
        "    full_text = '\\n'.join([block['text'] for block in line_blocks])\n",
        "\n",
        "    if doc_type == 'Aadhaar':\n",
        "        key_value_pairs = extract_aadhaar_data(line_blocks, full_text)\n",
        "    elif doc_type == 'PAN':\n",
        "        key_value_pairs = extract_pan_data(line_blocks, full_text)\n",
        "    elif doc_type == 'Passport':\n",
        "        key_value_pairs = extract_passport_data(line_blocks, full_text)\n",
        "\n",
        "    return key_value_pairs"
      ],
      "metadata": {
        "id": "T9mBQruj-8vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_aadhaar_data(line_blocks, full_text):\n",
        "    \"\"\"Extract Aadhaar card data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # Find indices of key landmarks for positional extraction\n",
        "        govt_india_index = -1\n",
        "        dob_index = -1\n",
        "\n",
        "        # First pass: Find Government of India and DOB positions\n",
        "        for i, block in enumerate(line_blocks):\n",
        "            block_text = block['text'].strip()\n",
        "\n",
        "            # Look for \"Government of India\" text\n",
        "            if re.search(r'(?i)government\\s+of\\s+india', block_text):\n",
        "                govt_india_index = i\n",
        "                print(f\"Found 'Government of India' at index {i}: {block_text}\")\n",
        "\n",
        "            # Look for DOB/YOB patterns\n",
        "            if re.search(r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth|जन्म\\s*तिथि)', block_text):\n",
        "                dob_index = i\n",
        "                print(f\"Found DOB pattern at index {i}: {block_text}\")\n",
        "\n",
        "        # Enhanced name extraction using positional logic\n",
        "        name_found = False\n",
        "\n",
        "        if govt_india_index != -1 and dob_index != -1 and govt_india_index < dob_index:\n",
        "            # Look for name between Government of India and DOB\n",
        "            print(f\"Searching for name between indices {govt_india_index} and {dob_index}\")\n",
        "\n",
        "            for i in range(govt_india_index + 1, dob_index):\n",
        "                if i >= len(line_blocks):\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[i]['text'].strip()\n",
        "\n",
        "                # Skip empty blocks or blocks with unwanted patterns\n",
        "                if not block_text or len(block_text) < 2:\n",
        "                    continue\n",
        "\n",
        "                # Skip blocks containing numbers, addresses, or other non-name content\n",
        "                if re.search(r'\\d{4}\\s*\\d{4}\\s*\\d{4}|\\d{4}|\\bPin\\b|\\bpincode\\b|address|DOB|YOB|Gender|Male|Female|VID|UID', block_text, re.IGNORECASE):\n",
        "                    continue\n",
        "\n",
        "                # Skip government/authority related text\n",
        "                if re.search(r'(?i)government|india|unique|identification|authority|aadhaar|uid|uidai', block_text):\n",
        "                    continue\n",
        "\n",
        "                # Look for English alphabetic names (2+ words preferred, but accept single names too)\n",
        "                if re.match(r'^[A-Za-z\\s]{2,}$', block_text):\n",
        "                    # Additional validation: should not be common non-name words\n",
        "                    if not re.search(r'(?i)^(the|and|or|of|in|at|to|for|with|by)$', block_text.strip()):\n",
        "                        candidate_name = clean_extracted_text(block_text, \"name\")\n",
        "                        if candidate_name and len(candidate_name) >= 2:\n",
        "                            data['name'] = candidate_name\n",
        "                            name_found = True\n",
        "                            print(f\"Found name at index {i}: {candidate_name}\")\n",
        "                            break\n",
        "\n",
        "        # Fallback method 1: If positional method fails, look for name just above any DOB line\n",
        "        if not name_found and dob_index > 0:\n",
        "            print(\"Using fallback method 1: Looking for name just above DOB\")\n",
        "\n",
        "            # Check 1-3 lines above DOB\n",
        "            for offset in range(1, min(4, dob_index + 1)):\n",
        "                check_index = dob_index - offset\n",
        "                if check_index < 0:\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[check_index]['text'].strip()\n",
        "\n",
        "                # Same validation as above\n",
        "                if (re.match(r'^[A-Za-z\\s]{2,}$', block_text) and\n",
        "                    not re.search(r'\\d|government|india|unique|identification|authority|aadhaar|uid|address|pin', block_text, re.IGNORECASE) and\n",
        "                    not re.search(r'(?i)^(the|and|or|of|in|at|to|for|with|by)$', block_text.strip())):\n",
        "\n",
        "                    candidate_name = clean_extracted_text(block_text, \"name\")\n",
        "                    if candidate_name and len(candidate_name) >= 2:\n",
        "                        data['name'] = candidate_name\n",
        "                        name_found = True\n",
        "                        print(f\"Found name using fallback method 1: {candidate_name}\")\n",
        "                        break\n",
        "\n",
        "        # Fallback method 2: If still not found, use regex on full text\n",
        "        if not name_found:\n",
        "            print(\"Using fallback method 2: Regex pattern matching\")\n",
        "\n",
        "            # Look for name patterns in context\n",
        "            name_patterns = [\n",
        "                # Pattern: Government of India followed by name followed by DOB/address\n",
        "                r'(?i)government\\s+of\\s+india.*?\\n\\s*([A-Z][A-Za-z\\s]{1,49}?)\\s*\\n.*?(?:DOB|YOB|Date|जन्म|Address)',\n",
        "                # Pattern: Name followed by DOB with reasonable distance\n",
        "                r'(?i)([A-Z][A-Za-z\\s]{1,49}?)\\s*\\n.*?(?:DOB|YOB|Date\\s*of\\s*Birth|जन्म\\s*तिथि)',\n",
        "                # Pattern: Government of India followed immediately by name\n",
        "                r'(?i)government\\s+of\\s+india\\s*?\\n\\s*([A-Z][A-Za-z\\s]{1,49}?)',\n",
        "                # General pattern for names (stricter validation)\n",
        "                r'(?i)\\n\\s*([A-Z][A-Za-z\\s]{3,49}?)\\s*\\n(?!.*(?:government|india|authority|unique|identification))'\n",
        "            ]\n",
        "\n",
        "            for pattern in name_patterns:\n",
        "                matches = re.finditer(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    candidate_name = clean_extracted_text(match.group(1), \"name\")\n",
        "\n",
        "                    # Additional validation\n",
        "                    if (candidate_name and\n",
        "                        len(candidate_name.split()) >= 1 and  # At least 1 word\n",
        "                        len(candidate_name) >= 3 and  # At least 3 characters\n",
        "                        not re.search(r'(?i)government|india|authority|unique|identification|address|pincode|male|female', candidate_name)):\n",
        "\n",
        "                        data['name'] = candidate_name\n",
        "                        name_found = True\n",
        "                        print(f\"Found name using regex fallback: {candidate_name}\")\n",
        "                        break\n",
        "\n",
        "                if name_found:\n",
        "                    break\n",
        "\n",
        "        if not name_found:\n",
        "            print(\"Warning: Could not extract name using any method\")\n",
        "\n",
        "\n",
        "        # DOB/YOB extraction (keeping existing logic)\n",
        "        dob_patterns = [\n",
        "            r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth)\\s*?:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth)\\s*?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'(?i)जन्म\\s*तिथि[:/\\s]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "        # Gender extraction with enhanced logic\n",
        "        gender_found = False\n",
        "\n",
        "        # Method 1: Look for Gender label\n",
        "        gender_match = re.search(r'(?i)(?:Gender|लिंग)\\s*[:/]?\\s*(Male|Female|Transgender|M|F)', full_text)\n",
        "        if not gender_match:\n",
        "            # Check for format like /Gender(Male/Female)\n",
        "            gender_match = re.search(r'/Gender\\s*\\(\\s*(Male|Female)\\s*\\)', full_text)\n",
        "\n",
        "        if gender_match:\n",
        "            gender = gender_match.group(1).upper()\n",
        "            if gender in ['M', 'MALE']:\n",
        "                data['gender'] = 'Male'\n",
        "            elif gender in ['F', 'FEMALE']:\n",
        "                data['gender'] = 'Female'\n",
        "            else:\n",
        "                data['gender'] = gender_match.group(1)\n",
        "            gender_found = True\n",
        "            print(f\"Found gender using label method: {data['gender']}\")\n",
        "\n",
        "        # Method 2: If no Gender label found, look for gender beneath DOB\n",
        "        if not gender_found and dob_index != -1:\n",
        "            print(\"Using fallback method: Looking for gender beneath DOB\")\n",
        "\n",
        "            # Check 1-3 lines below DOB\n",
        "            for offset in range(1, min(4, len(line_blocks) - dob_index)):\n",
        "                check_index = dob_index + offset\n",
        "                if check_index >= len(line_blocks):\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[check_index]['text'].strip()\n",
        "\n",
        "                # Look for gender patterns (English part only)\n",
        "                gender_patterns = [\n",
        "                    r'(?i)\\b(Male|Female|Transgender|M|F)\\b',\n",
        "                    r'(?i)/(Male|Female)',  # Format like \"पुरुष/Male\"\n",
        "                    r'(?i)(Male|Female)/',  # Format like \"Male/पुरुष\"\n",
        "                ]\n",
        "\n",
        "                for pattern in gender_patterns:\n",
        "                    gender_match = re.search(pattern, block_text)\n",
        "                    if gender_match:\n",
        "                        gender = gender_match.group(1).upper()\n",
        "                        if gender in ['M', 'MALE']:\n",
        "                            data['gender'] = 'Male'\n",
        "                        elif gender in ['F', 'FEMALE']:\n",
        "                            data['gender'] = 'Female'\n",
        "                        elif gender == 'TRANSGENDER':\n",
        "                            data['gender'] = 'Transgender'\n",
        "                        else:\n",
        "                            data['gender'] = gender_match.group(1)\n",
        "\n",
        "                        gender_found = True\n",
        "                        print(f\"Found gender beneath DOB at index {check_index}: {data['gender']}\")\n",
        "                        break\n",
        "\n",
        "                if gender_found:\n",
        "                    break\n",
        "\n",
        "        # Method 3: Final fallback - regex search in full text for standalone gender\n",
        "        if not gender_found:\n",
        "            print(\"Using final fallback: Regex search for standalone gender\")\n",
        "\n",
        "            # Look for gender in context after DOB\n",
        "            fallback_patterns = [\n",
        "                r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|जन्म\\s*तिथि).*?\\n.*?(Male|Female|Transgender)',\n",
        "                r'(?i)\\b(Male|Female|Transgender)\\b(?!\\s*(?:Name|DOB|Address))',  # Standalone gender not followed by field names\n",
        "                r'(?i)/(Male|Female)(?!/)',  # Gender in bilingual format\n",
        "                r'(?i)(Male|Female)/(?!Name)',  # Gender in bilingual format\n",
        "            ]\n",
        "\n",
        "            for pattern in fallback_patterns:\n",
        "                gender_match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if gender_match:\n",
        "                    gender = gender_match.group(1).upper()\n",
        "                    if gender in ['M', 'MALE']:\n",
        "                        data['gender'] = 'Male'\n",
        "                    elif gender in ['F', 'FEMALE']:\n",
        "                        data['gender'] = 'Female'\n",
        "                    elif gender == 'TRANSGENDER':\n",
        "                        data['gender'] = 'Transgender'\n",
        "                    else:\n",
        "                        data['gender'] = gender_match.group(1)\n",
        "\n",
        "                    gender_found = True\n",
        "                    print(f\"Found gender using final fallback: {data['gender']}\")\n",
        "                    break\n",
        "\n",
        "        if not gender_found:\n",
        "            print(\"Warning: Could not extract gender using any method\")\n",
        "\n",
        "        # Aadhaar number and VID extraction (keeping existing logic)\n",
        "        # Look for 12-digit Aadhaar number first\n",
        "        aadhaar_match = re.search(r'\\b(\\d{4}\\s*\\d{4}\\s*\\d{4})\\b', full_text)\n",
        "        if aadhaar_match:\n",
        "            aadhaar_num = aadhaar_match.group(1).replace(' ', '')\n",
        "            # Check if it's masked (contains X or is partially hidden)\n",
        "            if 'X' not in aadhaar_match.group(0) and len(aadhaar_num) == 12:\n",
        "                data['aadhaar_number'] = aadhaar_num\n",
        "            else:\n",
        "                # Look for VID (16-digit number)\n",
        "                vid_match = re.search(r'(?i)VID\\s*:?\\s*(\\d{16})', full_text)\n",
        "                if vid_match:\n",
        "                    data['vid'] = vid_match.group(1)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting Aadhaar data: {str(e)}\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "TU0FGA4a_ENd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pan_data(line_blocks, full_text):\n",
        "    \"\"\"Extract PAN card data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # PAN number extraction\n",
        "        pan_match = re.search(r'\\b([A-Z]{5}\\d{4}[A-Z])\\b', full_text)\n",
        "        if pan_match:\n",
        "            data['pan_number'] = pan_match.group(1)\n",
        "\n",
        "        # Enhanced name extraction using line-by-line analysis\n",
        "        name_found = False\n",
        "        father_name_found = False\n",
        "\n",
        "        for i, block in enumerate(line_blocks):\n",
        "            block_text = block['text'].strip()\n",
        "\n",
        "            # Look for Name label\n",
        "            if re.search(r'(?i)नाम\\s*/\\s*Name', block_text) and not name_found:\n",
        "                # Get the next line as the name\n",
        "                if i + 1 < len(line_blocks):\n",
        "                    name_text = line_blocks[i + 1]['text'].strip()\n",
        "                    # Clean and validate the name\n",
        "                    if re.match(r'^[A-Z\\s]{2,50}$', name_text) and not re.search(r'\\d|Father|पिता', name_text):\n",
        "                        data['name'] = clean_extracted_text(name_text, \"name\")\n",
        "                        name_found = True\n",
        "\n",
        "            # Look for Father's Name label\n",
        "            elif re.search(r'(?i)पिता\\s*का\\s*नाम\\s*/\\s*Father\\'?s?\\s*Name', block_text) and not father_name_found:\n",
        "                # Get the next line as father's name\n",
        "                if i + 1 < len(line_blocks):\n",
        "                    father_text = line_blocks[i + 1]['text'].strip()\n",
        "                    # Clean and validate the father's name\n",
        "                    if re.match(r'^[A-Z\\s]{2,50}$', father_text) and not re.search(r'\\d|Date|DOB', father_text):\n",
        "                        data['father_name'] = clean_extracted_text(father_text, \"name\")\n",
        "                        father_name_found = True\n",
        "\n",
        "        # Fallback regex patterns if line-by-line analysis fails\n",
        "        if not name_found:\n",
        "            name_patterns = [\n",
        "                r'(?i)नाम\\s*/\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|$)',\n",
        "                r'(?i)/Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|पिता|Father)',\n",
        "                r'(?i)Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|पिता|Father)'\n",
        "            ]\n",
        "\n",
        "            for pattern in name_patterns:\n",
        "                match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if match:\n",
        "                    candidate_name = clean_extracted_text(match.group(1), \"name\")\n",
        "                    # Validate it's not father's name or other text\n",
        "                    if not re.search(r'Father|FATHER|पिता|DOB|Date', candidate_name):\n",
        "                        data['name'] = candidate_name\n",
        "                        break\n",
        "\n",
        "        if not father_name_found:\n",
        "            father_patterns = [\n",
        "                r'(?i)पिता\\s*का\\s*नाम\\s*/\\s*Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|$)',\n",
        "                r'(?i)Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|Date|DOB|जन्म)',\n",
        "                r'(?i)/Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|Date|DOB)'\n",
        "            ]\n",
        "\n",
        "            for pattern in father_patterns:\n",
        "                match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if match:\n",
        "                    candidate_father = clean_extracted_text(match.group(1), \"name\")\n",
        "                    # Validate it's not other text\n",
        "                    if not re.search(r'DOB|Date|Birth|जन्म|तिथि', candidate_father):\n",
        "                        data['father_name'] = candidate_father\n",
        "                        break\n",
        "\n",
        "        # DOB extraction\n",
        "        dob_patterns = [\n",
        "            r'(?i)(?:जन्म\\s*तिथि\\s*/\\s*Date\\s*of\\s*Birth|DOB)\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'(?i)/Date\\s*of\\s*Birth\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting PAN data: {str(e)}\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "CerdJllt_H5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_passport_data(line_blocks, full_text):\n",
        "    \"\"\"Extract Passport data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # Passport number extraction\n",
        "        passport_patterns = [\n",
        "            r'(?i)/Passport\\s*No\\.?\\s*\\n\\s*([A-Z]\\d{7})',\n",
        "            r'(?i)Passport\\s*No\\.?\\s*:?\\s*([A-Z]\\d{7})',\n",
        "            r'\\b([A-Z]\\d{7})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in passport_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['passport_number'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Surname extraction\n",
        "        surname_patterns = [\n",
        "            r'(?i)/Surname\\s*\\n\\s*([A-Z\\s]+)',\n",
        "            r'(?i)Surname\\s*:?\\s*([A-Z][A-Za-z\\s]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in surname_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['surname'] = clean_extracted_text(match.group(1), \"name\")\n",
        "                break\n",
        "\n",
        "        # Given name extraction\n",
        "        given_name_patterns = [\n",
        "            r'(?i)/?Given\\s*Names?\\s*\\(\\s*s\\s*\\)?\\s*\\n\\s*([A-Z\\s]+)',  # Handles \"/Given Name(s)\" or \"/Given Names\"\n",
        "            r'(?i)/?Given\\s*Names?\\s*\\n\\s*([A-Z\\s]+)',  # Handles \"/Given Name\" or \"/Given Names\"\n",
        "            r'(?i)Given\\s*Names?\\s*\\(\\s*s\\s*\\)?\\s*:?\\s*([A-Z][A-Za-z\\s]+)',  # Handles \"Given Name(s):\" or \"Given Names:\"\n",
        "            r'(?i)Given\\s*Names?\\s*:?\\s*([A-Z][A-Za-z\\s]+)'  # Handles \"Given Name:\" or \"Given Names:\"\n",
        "        ]\n",
        "\n",
        "        for pattern in given_name_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['given_names'] = clean_extracted_text(match.group(1), \"name\")\n",
        "                break\n",
        "\n",
        "        # Nationality extraction - extract only English part\n",
        "        nationality_patterns = [\n",
        "            r'(?i)/Nationality\\s*\\n\\s*(?:.*/)?(INDIAN|AMERICAN|BRITISH|[A-Z]+)',\n",
        "            r'(?i)Nationality\\s*:?\\s*(?:.*/)?(INDIAN|AMERICAN|BRITISH|[A-Z]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in nationality_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['nationality'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Sex extraction\n",
        "        sex_patterns = [\n",
        "            r'(?i)/Sex\\s*\\n\\s*([MF])',\n",
        "            r'(?i)Sex\\s*:?\\s*([MF]|Male|Female)'\n",
        "        ]\n",
        "\n",
        "        for pattern in sex_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                sex = match.group(1).upper()\n",
        "                data['sex'] = 'M' if sex in ['M', 'MALE'] else 'F' if sex in ['F', 'FEMALE'] else sex\n",
        "                break\n",
        "\n",
        "        # DOB extraction\n",
        "        dob_patterns = [\n",
        "            r'(?i)/Date\\s*of\\s*Birth\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'(?i)Date\\s*of\\s*Birth\\s*:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting Passport data: {str(e)}\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "89jYJP8l_LPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract user photo from document\n",
        "def extract_user_photo(image, doc_type):\n",
        "    \"\"\"Extract user photo based on document type and layout\"\"\"\n",
        "    try:\n",
        "        if doc_type == 'Aadhaar':\n",
        "            # Left-center area for Aadhaar\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.2):int(h*0.8), int(w*0.05):int(w*0.35)]\n",
        "        elif doc_type == 'PAN':\n",
        "            # Top-left area for PAN\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.1):int(h*0.6), int(w*0.05):int(w*0.3)]\n",
        "        elif doc_type == 'Passport':\n",
        "            # Left side for Passport\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.2):int(h*0.7), int(w*0.05):int(w*0.25)]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # Verify it contains a human face using basic face detection\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        gray = cv2.cvtColor(photo_region, cv2.COLOR_RGB2GRAY) if len(photo_region.shape) == 3 else photo_region\n",
        "\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            return photo_region\n",
        "        else:\n",
        "            print(f\"No clear human face detected in {doc_type} photo region\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting photo: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "9u4DZtNy_T-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify and extract document data\n",
        "def classify_and_extract_document(selected_type, image):\n",
        "    # Check for blurriness\n",
        "    if is_blurry(image):\n",
        "        return \"Please upload a clearer image.\", {}\n",
        "\n",
        "    # Classify with YOLOv11\n",
        "    try:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if len(image.shape) == 3 else image\n",
        "        predictions = model.predict(image_bgr, imgsz=config.img_size, verbose=False)[0]\n",
        "        predicted_class_idx = predictions.probs.top1\n",
        "        predicted_label = ['Aadhaar', 'PAN', 'Passport'][predicted_class_idx]\n",
        "        confidence = predictions.probs.top1conf.item()\n",
        "\n",
        "        wandb.log({\n",
        "            'selected_type': selected_type,\n",
        "            'predicted_label': predicted_label,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error during classification: {str(e)}\")\n",
        "        return \"Error during classification.\", {}\n",
        "\n",
        "    if predicted_label != selected_type or confidence < 0.3:\n",
        "        return f\"Invalid document. Detected as {predicted_label} with {confidence:.3f} confidence.\", {}\n",
        "\n",
        "    # Extract user photo\n",
        "    user_photo = extract_user_photo(image, predicted_label)\n",
        "\n",
        "    # Preprocess for OCR\n",
        "    processed_image = preprocess_image_for_ocr(image)\n",
        "    if processed_image is None:\n",
        "        return \"Error preprocessing image for OCR.\", {}\n",
        "\n",
        "    # Encode image for Textract\n",
        "    try:\n",
        "        _, buffer = cv2.imencode('.png', processed_image)\n",
        "        image_bytes = buffer.tobytes()\n",
        "        print(f\"Image encoded successfully, size: {len(image_bytes)} bytes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error encoding image: {str(e)}\")\n",
        "        return \"Error encoding image for Textract.\", {}\n",
        "\n",
        "    # Call AWS Textract\n",
        "    try:\n",
        "        response = textract.analyze_document(\n",
        "            Document={'Bytes': image_bytes},\n",
        "            FeatureTypes=['FORMS']\n",
        "        )\n",
        "        print(\"Textract response received\")\n",
        "    except Exception as e:\n",
        "        print(f\"Textract error: {str(e)}\")\n",
        "        return f\"Textract error: {str(e)}\", {}\n",
        "\n",
        "    # Extract key-value pairs using enhanced logic\n",
        "    key_value_pairs = extract_key_value_pairs(response, predicted_label)\n",
        "\n",
        "    # Save image and extracted data\n",
        "    output_dir = f'/content/drive/MyDrive/Output/{predicted_label}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = int(time.time())\n",
        "    image_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}.png\")\n",
        "    try:\n",
        "        cv2.imwrite(image_path, image_bgr)\n",
        "        print(f\"Image saved to: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving image: {str(e)}\")\n",
        "\n",
        "    # Save user photo if extracted\n",
        "    if user_photo is not None:\n",
        "        photo_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}_photo.png\")\n",
        "        try:\n",
        "            cv2.imwrite(photo_path, user_photo)\n",
        "            key_value_pairs['user_photo_path'] = photo_path\n",
        "            print(f\"User photo saved to: {photo_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving user photo: {str(e)}\")\n",
        "\n",
        "    json_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}.json\")\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(key_value_pairs, f, indent=4)\n",
        "        print(f\"JSON saved to: {json_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON: {str(e)}\")\n",
        "\n",
        "    wandb.log({'extracted_fields': key_value_pairs})\n",
        "\n",
        "    # Format output message\n",
        "    result_message = f\"Document accepted: {predicted_label} (Confidence: {confidence:.3f})\\n\\nExtracted Data:\\n\"\n",
        "    for key, value in key_value_pairs.items():\n",
        "        if value and key != 'user_photo_path':  # Don't show empty fields or photo path in main output\n",
        "            result_message += f\"{key.replace('_', ' ').title()}: {value}\\n\"\n",
        "\n",
        "    return result_message, key_value_pairs"
      ],
      "metadata": {
        "id": "3ccFvOTu_aSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=classify_and_extract_document,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=['Aadhaar', 'PAN', 'Passport'], label=\"Select Document Type\"),\n",
        "        gr.Image(type=\"numpy\")\n",
        "    ],\n",
        "    outputs=[\"text\", \"json\"],\n",
        "    title=\"Enhanced Document Classifier and OCR - YOLOv11 with AWS Textract\",\n",
        "    description=\"Upload a clear image of your Aadhaar, PAN, or Passport for accurate data extraction.\"\n",
        ")"
      ],
      "metadata": {
        "id": "CmWrXfcM_jBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "0SAejpDs_i80",
        "outputId": "640361ff-ea10-4066-bb1f-0bec66ee7cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://b9000a4cc4680bcb92.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b9000a4cc4680bcb92.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No clear human face detected in PAN photo region\n",
            "Image preprocessed for OCR\n",
            "Image encoded successfully, size: 29968 bytes\n",
            "Textract response received\n",
            "Image saved to: /content/drive/MyDrive/Output/PAN/PAN_1755581430.png\n",
            "JSON saved to: /content/drive/MyDrive/Output/PAN/PAN_1755581430.json\n",
            "No clear human face detected in PAN photo region\n",
            "Image preprocessed for OCR\n",
            "Image encoded successfully, size: 29968 bytes\n",
            "Textract response received\n",
            "Image saved to: /content/drive/MyDrive/Output/PAN/PAN_1755581571.png\n",
            "JSON saved to: /content/drive/MyDrive/Output/PAN/PAN_1755581571.json\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b9000a4cc4680bcb92.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Finish WandB run\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "sURIKlGRsDtI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "9278fae9-28c8-484b-9aae-df9db12730b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>confidence</td><td>███▁█</td></tr><tr><td>laplacian_variance</td><td>▂▂▁█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>confidence</td><td>1.0</td></tr><tr><td>laplacian_variance</td><td>1030.88221</td></tr><tr><td>predicted_label</td><td>Passport</td></tr><tr><td>selected_type</td><td>Passport</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">yolov11_classification_with_ocr</strong> at: <a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr/runs/tge5z4db' target=\"_blank\">https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr/runs/tge5z4db</a><br> View project at: <a href='https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr' target=\"_blank\">https://wandb.ai/rajputayushsingh07042004-asmadiya-technologies/document_classification_ocr</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250814_054320-tge5z4db/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTgMw53IhYFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cff7c3ae"
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q ultralytics wandb gradio boto3\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import boto3\n",
        "import gradio as gr\n",
        "import wandb\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download Haar cascade for face detection\n",
        "!wget -q https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
        "\n",
        "# Set AWS credentials from Colab Secrets\n",
        "try:\n",
        "    os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "    os.environ['AWS_DEFAULT_REGION'] = userdata.get('AWS_DEFAULT_REGION')\n",
        "    print(\"AWS credentials loaded from Colab Secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Colab Secrets: {str(e)}\")\n",
        "    raise Exception(\"Failed to load AWS credentials from Colab Secrets. Check Secrets tab.\")\n",
        "\n",
        "# Initialize AWS Textract client\n",
        "try:\n",
        "    textract = boto3.client('textract', region_name='ap-south-1')\n",
        "    print(\"Textract client initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Textract client: {str(e)}\")\n",
        "    raise Exception(\"Failed to initialize Textract client.\")\n",
        "\n",
        "# Set up WandB\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project=\"document_classification_ocr\",\n",
        "    name=\"yolov11_classification_with_ocr\",\n",
        "    config={\n",
        "        \"img_size\": 224\n",
        "    }\n",
        ")\n",
        "config = wandb.config\n",
        "\n",
        "# Load pre-trained YOLOv11 model\n",
        "model_path = '/content/drive/MyDrive/document_classifier_yolo12.pt'\n",
        "try:\n",
        "    model = YOLO(model_path)\n",
        "    print(f\"Loaded model from: {model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    raise Exception(\"Failed to load the model. Ensure the file exists and is a valid YOLO model.\")\n",
        "\n",
        "# Preprocess image for OCR\n",
        "def preprocess_image_for_ocr(image):\n",
        "    try:\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        # Noise reduction\n",
        "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "        # Binarization\n",
        "        _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        print(\"Image preprocessed for OCR\")\n",
        "        return binary\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Blur detection\n",
        "def is_blurry(image, threshold=100):\n",
        "    try:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
        "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        wandb.log({'laplacian_variance': laplacian_var})\n",
        "        return laplacian_var < threshold\n",
        "    except Exception as e:\n",
        "        print(f\"Error in blur detection: {str(e)}\")\n",
        "        return True  # Assume blurry on error\n",
        "\n",
        "# Enhanced text cleaning function\n",
        "def clean_extracted_text(text, field_type=\"general\"):\n",
        "    \"\"\"Clean extracted text based on field type\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove common OCR artifacts\n",
        "    text = re.sub(r'[^\\w\\s/.-]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    if field_type == \"date\":\n",
        "        # Extract date patterns\n",
        "        date_match = re.search(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}\\b', text)\n",
        "        if date_match:\n",
        "            return date_match.group(0)\n",
        "    elif field_type == \"number\":\n",
        "        # Extract only digits and spaces\n",
        "        return re.sub(r'[^\\d\\s]', '', text).strip()\n",
        "    elif field_type == \"name\":\n",
        "        # Clean name - remove extra spaces and common prefixes\n",
        "        text = re.sub(r'(?i)^(mr\\.?|ms\\.?|mrs\\.?|dr\\.?)\\s*', '', text)\n",
        "        # Remove trailing unwanted text that might be concatenated\n",
        "        text = re.sub(r'\\s+(will|shall|and|the|of|for|in|at|to|from).*$', '', text, flags=re.IGNORECASE)\n",
        "        # Keep only alphabetic characters and spaces\n",
        "        text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
        "        # Remove extra spaces and capitalize properly\n",
        "        text = ' '.join(text.split())\n",
        "        return text.upper() if text else \"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Extract key-value pairs from Textract response with enhanced logic\n",
        "def extract_key_value_pairs(textract_response, doc_type):\n",
        "    \"\"\"Extract key-value pairs using layout-based logic for each document type\"\"\"\n",
        "    key_value_pairs = {}\n",
        "\n",
        "    # Get all text blocks with their bounding boxes for layout analysis\n",
        "    text_blocks = []\n",
        "    line_blocks = []\n",
        "\n",
        "    try:\n",
        "        for block in textract_response['Blocks']:\n",
        "            if block['BlockType'] == 'WORD' and 'Text' in block:\n",
        "                text_blocks.append({\n",
        "                    'text': block['Text'],\n",
        "                    'bbox': block['Geometry']['BoundingBox'],\n",
        "                    'confidence': block.get('Confidence', 0)\n",
        "                })\n",
        "            elif block['BlockType'] == 'LINE' and 'Text' in block:\n",
        "                line_blocks.append({\n",
        "                    'text': block['Text'],\n",
        "                    'bbox': block['Geometry']['BoundingBox'],\n",
        "                    'confidence': block.get('Confidence', 0)\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing Textract blocks: {str(e)}\")\n",
        "        return {}\n",
        "\n",
        "    # Sort blocks by position (top to bottom, left to right)\n",
        "    line_blocks.sort(key=lambda x: (x['bbox']['Top'], x['bbox']['Left']))\n",
        "\n",
        "    # Full text for fallback regex matching\n",
        "    full_text = '\\n'.join([block['text'] for block in line_blocks])\n",
        "\n",
        "    if doc_type == 'Aadhaar':\n",
        "        key_value_pairs = extract_aadhaar_data(line_blocks, full_text)\n",
        "    elif doc_type == 'PAN':\n",
        "        key_value_pairs = extract_pan_data(line_blocks, full_text)\n",
        "    elif doc_type == 'Passport':\n",
        "        key_value_pairs = extract_passport_data(line_blocks, full_text)\n",
        "\n",
        "    return key_value_pairs\n",
        "\n",
        "def extract_aadhaar_data(line_blocks, full_text):\n",
        "    \"\"\"Extract Aadhaar card data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # Find indices of key landmarks for positional extraction\n",
        "        govt_india_index = -1\n",
        "        dob_index = -1\n",
        "\n",
        "        # First pass: Find Government of India and DOB positions\n",
        "        for i, block in enumerate(line_blocks):\n",
        "            block_text = block['text'].strip()\n",
        "\n",
        "            # Look for \"Government of India\" text\n",
        "            if re.search(r'(?i)government\\s+of\\s+india', block_text):\n",
        "                govt_india_index = i\n",
        "                print(f\"Found 'Government of India' at index {i}: {block_text}\")\n",
        "\n",
        "            # Look for DOB/YOB patterns\n",
        "            if re.search(r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth|जन्म\\s*तिथि)', block_text):\n",
        "                dob_index = i\n",
        "                print(f\"Found DOB pattern at index {i}: {block_text}\")\n",
        "\n",
        "        # Enhanced name extraction using positional logic\n",
        "        name_found = False\n",
        "\n",
        "        if govt_india_index != -1 and dob_index != -1 and govt_india_index < dob_index:\n",
        "            # Look for name between Government of India and DOB\n",
        "            print(f\"Searching for name between indices {govt_india_index} and {dob_index}\")\n",
        "\n",
        "            for i in range(govt_india_index + 1, dob_index):\n",
        "                if i >= len(line_blocks):\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[i]['text'].strip()\n",
        "\n",
        "                # Skip empty blocks or blocks with unwanted patterns\n",
        "                if not block_text or len(block_text) < 2:\n",
        "                    continue\n",
        "\n",
        "                # Skip blocks containing numbers, addresses, or other non-name content\n",
        "                if re.search(r'\\d{4}\\s*\\d{4}\\s*\\d{4}|\\d{4}|\\bPin\\b|\\bpincode\\b|address|DOB|YOB|Gender|Male|Female|VID|UID', block_text, re.IGNORECASE):\n",
        "                    continue\n",
        "\n",
        "                # Skip government/authority related text\n",
        "                if re.search(r'(?i)government|india|unique|identification|authority|aadhaar|uid|uidai', block_text):\n",
        "                    continue\n",
        "\n",
        "                # Look for English alphabetic names (2+ words preferred, but accept single names too)\n",
        "                if re.match(r'^[A-Za-z\\s]{2,}$', block_text):\n",
        "                    # Additional validation: should not be common non-name words\n",
        "                    if not re.search(r'(?i)^(the|and|or|of|in|at|to|for|with|by)$', block_text.strip()):\n",
        "                        candidate_name = clean_extracted_text(block_text, \"name\")\n",
        "                        if candidate_name and len(candidate_name) >= 2:\n",
        "                            data['name'] = candidate_name\n",
        "                            name_found = True\n",
        "                            print(f\"Found name at index {i}: {candidate_name}\")\n",
        "                            break\n",
        "\n",
        "        # Fallback method 1: If positional method fails, look for name just above any DOB line\n",
        "        if not name_found and dob_index > 0:\n",
        "            print(\"Using fallback method 1: Looking for name just above DOB\")\n",
        "\n",
        "            # Check 1-3 lines above DOB\n",
        "            for offset in range(1, min(4, dob_index + 1)):\n",
        "                check_index = dob_index - offset\n",
        "                if check_index < 0:\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[check_index]['text'].strip()\n",
        "\n",
        "                # Same validation as above\n",
        "                if (re.match(r'^[A-Za-z\\s]{2,}$', block_text) and\n",
        "                    not re.search(r'\\d|government|india|unique|identification|authority|aadhaar|uid|address|pin', block_text, re.IGNORECASE) and\n",
        "                    not re.search(r'(?i)^(the|and|or|of|in|at|to|for|with|by)$', block_text.strip())):\n",
        "\n",
        "                    candidate_name = clean_extracted_text(block_text, \"name\")\n",
        "                    if candidate_name and len(candidate_name) >= 2:\n",
        "                        data['name'] = candidate_name\n",
        "                        name_found = True\n",
        "                        print(f\"Found name using fallback method 1: {candidate_name}\")\n",
        "                        break\n",
        "\n",
        "        # Fallback method 2: If still not found, use regex on full text\n",
        "        if not name_found:\n",
        "            print(\"Using fallback method 2: Regex pattern matching\")\n",
        "\n",
        "            # Look for name patterns in context\n",
        "            name_patterns = [\n",
        "                # Pattern: Government of India followed by name followed by DOB/address\n",
        "                r'(?i)government\\s+of\\s+india.*?\\n\\s*([A-Z][A-Za-z\\s]{1,49}?)\\s*\\n.*?(?:DOB|YOB|Date|जन्म|Address)',\n",
        "                # Pattern: Name followed by DOB with reasonable distance\n",
        "                r'(?i)([A-Z][A-Za-z\\s]{1,49}?)\\s*\\n.*?(?:DOB|YOB|Date\\s*of\\s*Birth|जन्म\\s*तिथि)',\n",
        "                # Pattern: Government of India followed immediately by name\n",
        "                r'(?i)government\\s+of\\s+india\\s*?\\n\\s*([A-Z][A-Za-z\\s]{1,49}?)',\n",
        "                # General pattern for names (stricter validation)\n",
        "                r'(?i)\\n\\s*([A-Z][A-Za-z\\s]{3,49}?)\\s*\\n(?!.*(?:government|india|authority|unique|identification))'\n",
        "            ]\n",
        "\n",
        "            for pattern in name_patterns:\n",
        "                matches = re.finditer(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                for match in matches:\n",
        "                    candidate_name = clean_extracted_text(match.group(1), \"name\")\n",
        "\n",
        "                    # Additional validation\n",
        "                    if (candidate_name and\n",
        "                        len(candidate_name.split()) >= 1 and  # At least 1 word\n",
        "                        len(candidate_name) >= 3 and  # At least 3 characters\n",
        "                        not re.search(r'(?i)government|india|authority|unique|identification|address|pincode|male|female', candidate_name)):\n",
        "\n",
        "                        data['name'] = candidate_name\n",
        "                        name_found = True\n",
        "                        print(f\"Found name using regex fallback: {candidate_name}\")\n",
        "                        break\n",
        "\n",
        "                if name_found:\n",
        "                    break\n",
        "\n",
        "        if not name_found:\n",
        "            print(\"Warning: Could not extract name using any method\")\n",
        "\n",
        "\n",
        "        # DOB/YOB extraction (keeping existing logic)\n",
        "        dob_patterns = [\n",
        "            r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth)\\s*?:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|Year\\s*of\\s*Birth)\\s*?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'(?i)जन्म\\s*तिथि[:/\\s]*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "        # Gender extraction with enhanced logic\n",
        "        gender_found = False\n",
        "\n",
        "        # Method 1: Look for Gender label\n",
        "        gender_match = re.search(r'(?i)(?:Gender|लिंग)\\s*[:/]?\\s*(Male|Female|Transgender|M|F)', full_text)\n",
        "        if not gender_match:\n",
        "            # Check for format like /Gender(Male/Female)\n",
        "            gender_match = re.search(r'/Gender\\s*\\(\\s*(Male|Female)\\s*\\)', full_text)\n",
        "\n",
        "        if gender_match:\n",
        "            gender = gender_match.group(1).upper()\n",
        "            if gender in ['M', 'MALE']:\n",
        "                data['gender'] = 'Male'\n",
        "            elif gender in ['F', 'FEMALE']:\n",
        "                data['gender'] = 'Female'\n",
        "            else:\n",
        "                data['gender'] = gender_match.group(1)\n",
        "            gender_found = True\n",
        "            print(f\"Found gender using label method: {data['gender']}\")\n",
        "\n",
        "        # Method 2: If no Gender label found, look for gender beneath DOB\n",
        "        if not gender_found and dob_index != -1:\n",
        "            print(\"Using fallback method: Looking for gender beneath DOB\")\n",
        "\n",
        "            # Check 1-3 lines below DOB\n",
        "            for offset in range(1, min(4, len(line_blocks) - dob_index)):\n",
        "                check_index = dob_index + offset\n",
        "                if check_index >= len(line_blocks):\n",
        "                    break\n",
        "\n",
        "                block_text = line_blocks[check_index]['text'].strip()\n",
        "\n",
        "                # Look for gender patterns (English part only)\n",
        "                gender_patterns = [\n",
        "                    r'(?i)\\b(Male|Female|Transgender|M|F)\\b',\n",
        "                    r'(?i)/(Male|Female)',  # Format like \"पुरुष/Male\"\n",
        "                    r'(?i)(Male|Female)/',  # Format like \"Male/पुरुष\"\n",
        "                ]\n",
        "\n",
        "                for pattern in gender_patterns:\n",
        "                    gender_match = re.search(pattern, block_text)\n",
        "                    if gender_match:\n",
        "                        gender = gender_match.group(1).upper()\n",
        "                        if gender in ['M', 'MALE']:\n",
        "                            data['gender'] = 'Male'\n",
        "                        elif gender in ['F', 'FEMALE']:\n",
        "                            data['gender'] = 'Female'\n",
        "                        elif gender == 'TRANSGENDER':\n",
        "                            data['gender'] = 'Transgender'\n",
        "                        else:\n",
        "                            data['gender'] = gender_match.group(1)\n",
        "\n",
        "                        gender_found = True\n",
        "                        print(f\"Found gender beneath DOB at index {check_index}: {data['gender']}\")\n",
        "                        break\n",
        "\n",
        "                if gender_found:\n",
        "                    break\n",
        "\n",
        "        # Method 3: Final fallback - regex search in full text for standalone gender\n",
        "        if not gender_found:\n",
        "            print(\"Using final fallback: Regex search for standalone gender\")\n",
        "\n",
        "            # Look for gender in context after DOB\n",
        "            fallback_patterns = [\n",
        "                r'(?i)(?:DOB|YOB|Date\\s*of\\s*Birth|जन्म\\s*तिथि).*?\\n.*?(Male|Female|Transgender)',\n",
        "                r'(?i)\\b(Male|Female|Transgender)\\b(?!\\s*(?:Name|DOB|Address))',  # Standalone gender not followed by field names\n",
        "                r'(?i)/(Male|Female)(?!/)',  # Gender in bilingual format\n",
        "                r'(?i)(Male|Female)/(?!Name)',  # Gender in bilingual format\n",
        "            ]\n",
        "\n",
        "            for pattern in fallback_patterns:\n",
        "                gender_match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if gender_match:\n",
        "                    gender = gender_match.group(1).upper()\n",
        "                    if gender in ['M', 'MALE']:\n",
        "                        data['gender'] = 'Male'\n",
        "                    elif gender in ['F', 'FEMALE']:\n",
        "                        data['gender'] = 'Female'\n",
        "                    elif gender == 'TRANSGENDER':\n",
        "                        data['gender'] = 'Transgender'\n",
        "                    else:\n",
        "                        data['gender'] = gender_match.group(1)\n",
        "\n",
        "                    gender_found = True\n",
        "                    print(f\"Found gender using final fallback: {data['gender']}\")\n",
        "                    break\n",
        "\n",
        "        if not gender_found:\n",
        "            print(\"Warning: Could not extract gender using any method\")\n",
        "\n",
        "        # Aadhaar number and VID extraction (keeping existing logic)\n",
        "        # Look for 12-digit Aadhaar number first\n",
        "        aadhaar_match = re.search(r'\\b(\\d{4}\\s*\\d{4}\\s*\\d{4})\\b', full_text)\n",
        "        if aadhaar_match:\n",
        "            aadhaar_num = aadhaar_match.group(1).replace(' ', '')\n",
        "            # Check if it's masked (contains X or is partially hidden)\n",
        "            if 'X' not in aadhaar_match.group(0) and len(aadhaar_num) == 12:\n",
        "                data['aadhaar_number'] = aadhaar_num\n",
        "            else:\n",
        "                # Look for VID (16-digit number)\n",
        "                vid_match = re.search(r'(?i)VID\\s*:?\\s*(\\d{16})', full_text)\n",
        "                if vid_match:\n",
        "                    data['vid'] = vid_match.group(1)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting Aadhaar data: {str(e)}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def extract_pan_data(line_blocks, full_text):\n",
        "    \"\"\"Extract PAN card data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # PAN number extraction\n",
        "        pan_match = re.search(r'\\b([A-Z]{5}\\d{4}[A-Z])\\b', full_text)\n",
        "        if pan_match:\n",
        "            data['pan_number'] = pan_match.group(1)\n",
        "\n",
        "        # Enhanced name extraction using line-by-line analysis\n",
        "        name_found = False\n",
        "        father_name_found = False\n",
        "\n",
        "        for i, block in enumerate(line_blocks):\n",
        "            block_text = block['text'].strip()\n",
        "\n",
        "            # Look for Name label\n",
        "            if re.search(r'(?i)नाम\\s*/\\s*Name', block_text) and not name_found:\n",
        "                # Get the next line as the name\n",
        "                if i + 1 < len(line_blocks):\n",
        "                    name_text = line_blocks[i + 1]['text'].strip()\n",
        "                    # Clean and validate the name\n",
        "                    if re.match(r'^[A-Z\\s]{2,50}$', name_text) and not re.search(r'\\d|Father|पिता', name_text):\n",
        "                        data['name'] = clean_extracted_text(name_text, \"name\")\n",
        "                        name_found = True\n",
        "\n",
        "            # Look for Father's Name label\n",
        "            elif re.search(r'(?i)पिता\\s*का\\s*नाम\\s*/\\s*Father\\'?s?\\s*Name', block_text) and not father_name_found:\n",
        "                # Get the next line as father's name\n",
        "                if i + 1 < len(line_blocks):\n",
        "                    father_text = line_blocks[i + 1]['text'].strip()\n",
        "                    # Clean and validate the father's name\n",
        "                    if re.match(r'^[A-Z\\s]{2,50}$', father_text) and not re.search(r'\\d|Date|DOB', father_text):\n",
        "                        data['father_name'] = clean_extracted_text(father_text, \"name\")\n",
        "                        father_name_found = True\n",
        "\n",
        "        # Fallback regex patterns if line-by-line analysis fails\n",
        "        if not name_found:\n",
        "            name_patterns = [\n",
        "                r'(?i)नाम\\s*/\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|$)',\n",
        "                r'(?i)/Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|पिता|Father)',\n",
        "                r'(?i)Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|पिता|Father)'\n",
        "            ]\n",
        "\n",
        "            for pattern in name_patterns:\n",
        "                match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if match:\n",
        "                    candidate_name = clean_extracted_text(match.group(1), \"name\")\n",
        "                    # Validate it's not father's name or other text\n",
        "                    if not re.search(r'Father|FATHER|पिता|DOB|Date', candidate_name):\n",
        "                        data['name'] = candidate_name\n",
        "                        break\n",
        "\n",
        "        if not father_name_found:\n",
        "            father_patterns = [\n",
        "                r'(?i)पिता\\s*का\\s*नाम\\s*/\\s*Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|$)',\n",
        "                r'(?i)Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|Date|DOB|जन्म)',\n",
        "                r'(?i)/Father\\'?s?\\s*Name\\s*\\n\\s*([A-Z][A-Z\\s]{1,49}?)(?=\\n|Date|DOB)'\n",
        "            ]\n",
        "\n",
        "            for pattern in father_patterns:\n",
        "                match = re.search(pattern, full_text, re.MULTILINE | re.DOTALL)\n",
        "                if match:\n",
        "                    candidate_father = clean_extracted_text(match.group(1), \"name\")\n",
        "                    # Validate it's not other text\n",
        "                    if not re.search(r'DOB|Date|Birth|जन्म|तिथि', candidate_father):\n",
        "                        data['father_name'] = candidate_father\n",
        "                        break\n",
        "\n",
        "        # DOB extraction\n",
        "        dob_patterns = [\n",
        "            r'(?i)(?:जन्म\\s*तिथि\\s*/\\s*Date\\s*of\\s*Birth|DOB)\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'(?i)/Date\\s*of\\s*Birth\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting PAN data: {str(e)}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def extract_passport_data(line_blocks, full_text):\n",
        "    \"\"\"Extract Passport data using specific layout rules\"\"\"\n",
        "    data = {}\n",
        "\n",
        "    try:\n",
        "        # Passport number extraction\n",
        "        passport_patterns = [\n",
        "            r'(?i)/Passport\\s*No\\.?\\s*\\n\\s*([A-Z]\\d{7})',\n",
        "            r'(?i)Passport\\s*No\\.?\\s*:?\\s*([A-Z]\\d{7})',\n",
        "            r'\\b([A-Z]\\d{7})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in passport_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['passport_number'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Surname extraction\n",
        "        surname_patterns = [\n",
        "            r'(?i)/Surname\\s*\\n\\s*([A-Z\\s]+)',\n",
        "            r'(?i)Surname\\s*:?\\s*([A-Z][A-Za-z\\s]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in surname_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['surname'] = clean_extracted_text(match.group(1), \"name\")\n",
        "                break\n",
        "\n",
        "        # Given name extraction\n",
        "        given_name_patterns = [\n",
        "            r'(?i)/?Given\\s*Names?\\s*\\(\\s*s\\s*\\)?\\s*\\n\\s*([A-Z\\s]+)',  # Handles \"/Given Name(s)\" or \"/Given Names\"\n",
        "            r'(?i)/?Given\\s*Names?\\s*\\n\\s*([A-Z\\s]+)',  # Handles \"/Given Name\" or \"/Given Names\"\n",
        "            r'(?i)Given\\s*Names?\\s*\\(\\s*s\\s*\\)?\\s*:?\\s*([A-Z][A-Za-z\\s]+)',  # Handles \"Given Name(s):\" or \"Given Names:\"\n",
        "            r'(?i)Given\\s*Names?\\s*:?\\s*([A-Z][A-Za-z\\s]+)'  # Handles \"Given Name:\" or \"Given Names:\"\n",
        "        ]\n",
        "\n",
        "        for pattern in given_name_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['given_names'] = clean_extracted_text(match.group(1), \"name\")\n",
        "                break\n",
        "\n",
        "        # Nationality extraction - extract only English part\n",
        "        nationality_patterns = [\n",
        "            r'(?i)/Nationality\\s*\\n\\s*(?:.*/)?(INDIAN|AMERICAN|BRITISH|[A-Z]+)',\n",
        "            r'(?i)Nationality\\s*:?\\s*(?:.*/)?(INDIAN|AMERICAN|BRITISH|[A-Z]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in nationality_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['nationality'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Sex extraction\n",
        "        sex_patterns = [\n",
        "            r'(?i)/Sex\\s*\\n\\s*([MF])',\n",
        "            r'(?i)Sex\\s*:?\\s*([MF]|Male|Female)'\n",
        "        ]\n",
        "\n",
        "        for pattern in sex_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                sex = match.group(1).upper()\n",
        "                data['sex'] = 'M' if sex in ['M', 'MALE'] else 'F' if sex in ['F', 'FEMALE'] else sex\n",
        "                break\n",
        "\n",
        "        # DOB extraction\n",
        "        dob_patterns = [\n",
        "            r'(?i)/Date\\s*of\\s*Birth\\s*\\n\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'(?i)Date\\s*of\\s*Birth\\s*:?\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
        "            r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in dob_patterns:\n",
        "            match = re.search(pattern, full_text, re.MULTILINE)\n",
        "            if match:\n",
        "                data['dob'] = clean_extracted_text(match.group(1), \"date\")\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting Passport data: {str(e)}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# Extract user photo from document\n",
        "def extract_user_photo(image, doc_type):\n",
        "    \"\"\"Extract user photo based on document type and layout\"\"\"\n",
        "    try:\n",
        "        if doc_type == 'Aadhaar':\n",
        "            # Left-center area for Aadhaar\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.2):int(h*0.8), int(w*0.05):int(w*0.35)]\n",
        "        elif doc_type == 'PAN':\n",
        "            # Top-left area for PAN\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.1):int(h*0.6), int(w*0.05):int(w*0.3)]\n",
        "        elif doc_type == 'Passport':\n",
        "            # Left side for Passport\n",
        "            h, w = image.shape[:2]\n",
        "            photo_region = image[int(h*0.2):int(h*0.7), int(w*0.05):int(w*0.25)]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # Verify it contains a human face using basic face detection\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "        gray = cv2.cvtColor(photo_region, cv2.COLOR_RGB2GRAY) if len(photo_region.shape) == 3 else photo_region\n",
        "\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            return photo_region\n",
        "        else:\n",
        "            print(f\"No clear human face detected in {doc_type} photo region\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting photo: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Classify and extract document data\n",
        "def classify_and_extract_document(selected_type, image):\n",
        "    # Check for blurriness\n",
        "    if is_blurry(image):\n",
        "        return \"Please upload a clearer image.\", {}\n",
        "\n",
        "    # Classify with YOLOv11\n",
        "    try:\n",
        "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) if len(image.shape) == 3 else image\n",
        "        predictions = model.predict(image_bgr, imgsz=config.img_size, verbose=False)[0]\n",
        "        predicted_class_idx = predictions.probs.top1\n",
        "        predicted_label = ['Aadhaar', 'PAN', 'Passport'][predicted_class_idx]\n",
        "        confidence = predictions.probs.top1conf.item()\n",
        "\n",
        "        wandb.log({\n",
        "            'selected_type': selected_type,\n",
        "            'predicted_label': predicted_label,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error during classification: {str(e)}\")\n",
        "        return \"Error during classification.\", {}\n",
        "\n",
        "    if predicted_label != selected_type or confidence < 0.3:\n",
        "        return f\"Invalid document. Detected as {predicted_label} with {confidence:.3f} confidence.\", {}\n",
        "\n",
        "    # Extract user photo\n",
        "    user_photo = extract_user_photo(image, predicted_label)\n",
        "\n",
        "    # Preprocess for OCR\n",
        "    processed_image = preprocess_image_for_ocr(image)\n",
        "    if processed_image is None:\n",
        "        return \"Error preprocessing image for OCR.\", {}\n",
        "\n",
        "    # Encode image for Textract\n",
        "    try:\n",
        "        _, buffer = cv2.imencode('.png', processed_image)\n",
        "        image_bytes = buffer.tobytes()\n",
        "        print(f\"Image encoded successfully, size: {len(image_bytes)} bytes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error encoding image: {str(e)}\")\n",
        "        return \"Error encoding image for Textract.\", {}\n",
        "\n",
        "    # Call AWS Textract\n",
        "    try:\n",
        "        response = textract.analyze_document(\n",
        "            Document={'Bytes': image_bytes},\n",
        "            FeatureTypes=['FORMS']\n",
        "        )\n",
        "        print(\"Textract response received\")\n",
        "    except Exception as e:\n",
        "        print(f\"Textract error: {str(e)}\")\n",
        "        return f\"Textract error: {str(e)}\", {}\n",
        "\n",
        "    # Extract key-value pairs using enhanced logic\n",
        "    key_value_pairs = extract_key_value_pairs(response, predicted_label)\n",
        "\n",
        "    # Save image and extracted data\n",
        "    output_dir = f'/content/drive/MyDrive/Output/{predicted_label}'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = int(time.time())\n",
        "    image_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}.png\")\n",
        "    try:\n",
        "        cv2.imwrite(image_path, image_bgr)\n",
        "        print(f\"Image saved to: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving image: {str(e)}\")\n",
        "\n",
        "    # Save user photo if extracted\n",
        "    if user_photo is not None:\n",
        "        photo_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}_photo.png\")\n",
        "        try:\n",
        "            cv2.imwrite(photo_path, user_photo)\n",
        "            key_value_pairs['user_photo_path'] = photo_path\n",
        "            print(f\"User photo saved to: {photo_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving user photo: {str(e)}\")\n",
        "\n",
        "    json_path = os.path.join(output_dir, f\"{predicted_label}_{timestamp}.json\")\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(key_value_pairs, f, indent=4)\n",
        "        print(f\"JSON saved to: {json_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON: {str(e)}\")\n",
        "\n",
        "    wandb.log({'extracted_fields': key_value_pairs})\n",
        "\n",
        "    # Format output message\n",
        "    result_message = f\"Document accepted: {predicted_label} (Confidence: {confidence:.3f})\\n\\nExtracted Data:\\n\"\n",
        "    for key, value in key_value_pairs.items():\n",
        "        if value and key != 'user_photo_path':  # Don't show empty fields or photo path in main output\n",
        "            result_message += f\"{key.replace('_', ' ').title()}: {value}\\n\"\n",
        "\n",
        "    return result_message, key_value_pairs\n",
        "\n",
        "# Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=classify_and_extract_document,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=['Aadhaar', 'PAN', 'Passport'], label=\"Select Document Type\"),\n",
        "        gr.Image(type=\"numpy\")\n",
        "    ],\n",
        "    outputs=[\"text\", \"json\"],\n",
        "    title=\"Enhanced Document Classifier and OCR - YOLOv11 with AWS Textract\",\n",
        "    description=\"Upload a clear image of your Aadhaar, PAN, or Passport for accurate data extraction.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)\n",
        "\n",
        " # Finish WandB run\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}