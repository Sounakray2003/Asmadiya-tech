{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlFig2tPR0Lo"
      },
      "outputs": [],
      "source": [
        "!pip install -U diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaqtXwI2R0Lt"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLOBarSPR0Lu"
      },
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxminv5MlMI9"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIrv10cKvQvK"
      },
      "outputs": [],
      "source": [
        "!pip install invisible_watermark transformers accelerate safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Q-YdI4vU7l"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/sdxl-turbo\")\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "# if using torch < 2.0\n",
        "# pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "prompt = \"An astronaut riding a green horse\"\n",
        "\n",
        "images = pipe(prompt=prompt).images[0]\n",
        "images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZlqWfIEd0kIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb94d2c9-32d0-4de2-c1cb-dc2540af6fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CzkDtIo51Iab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89364b19-622e-4a27-866f-9c7a496cbdb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement trl.trainer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for trl.trainer\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install trl.trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6480b1f5"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d63a026"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torchvision.transforms as T\n",
        "import bitsandbytes.optim as bnb\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Initialize Accelerator ---\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=4)\n",
        "device = accelerator.device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Verify GPU availability\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"GPU not available. Ensure Colab is set to GPU runtime.\")\n",
        "\n",
        "# --- Load SDXL ---\n",
        "model_id = \"stabilityai/sdxl-turbo\"\n",
        "# ---- 4-bit quantisation config (new API) ----\n",
        "quant_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.to(device)\n",
        "\n",
        "# Load tokenizers and text encoders\n",
        "tokenizer_one = AutoTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
        "tokenizer_two = AutoTokenizer.from_pretrained(model_id, subfolder=\"tokenizer_2\")\n",
        "text_encoder_one = pipe.text_encoder\n",
        "text_encoder_two = pipe.text_encoder_2\n",
        "text_encoder_one.to(device)\n",
        "text_encoder_two.to(device)\n",
        "\n",
        "# --- LoRA on UNet ---\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "pipe.unet = get_peft_model(pipe.unet, lora_config)\n",
        "pipe.unet.enable_gradient_checkpointing()\n",
        "\n",
        "# --- Load dataset (limit to 500 images) ---\n",
        "dataset_path = \"./drive/MyDrive/cc_data/cc_data\"\n",
        "image_dir = os.path.join(dataset_path, \"images\")\n",
        "jsonl_path = os.path.join(dataset_path, \"dataset.jsonl\")\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset directory not found at {dataset_path}\")\n",
        "if not os.path.exists(jsonl_path):\n",
        "    raise FileNotFoundError(f\"JSONL file not found at {jsonl_path}\")\n",
        "\n",
        "print(f\"Dataset directory: {dataset_path}\")\n",
        "print(f\"JSONL file: {jsonl_path}\")\n",
        "\n",
        "data = []\n",
        "with open(jsonl_path, \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 3:\n",
        "            print(f\"Line {i+1}: {line.strip()}\")\n",
        "        try:\n",
        "            entry = json.loads(line.strip())\n",
        "            prompt_key = \"text\"  # Confirmed from output\n",
        "            image_key = \"image_path\"  # Confirmed from output\n",
        "            if prompt_key in entry and image_key in entry:\n",
        "                image_path = os.path.join(image_dir, os.path.basename(entry[image_key]))\n",
        "                if os.path.exists(image_path):\n",
        "                    data.append({\"prompt\": entry[prompt_key], \"image_path\": image_path})\n",
        "                else:\n",
        "                    print(f\"Warning: Image not found: {image_path}\")\n",
        "                if len(data) >= 100:\n",
        "                    break\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON on line {i+1}: {e}\")\n",
        "if not data:\n",
        "    raise ValueError(\"No valid data loaded from dataset.jsonl\")\n",
        "print(f\"Loaded {len(data)} samples (capped at 500).\")\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# --- Preprocessing ---\n",
        "transform = T.Compose([\n",
        "    T.Resize((256, 256)),  # Reduced for memory efficiency\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "def preprocess(example):\n",
        "    try:\n",
        "        image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
        "        example[\"image\"] = transform(image)\n",
        "        prompt = example[\"prompt\"]\n",
        "        if not prompt or not isinstance(prompt, str):\n",
        "            raise ValueError(\"Invalid or empty prompt\")\n",
        "        input_ids_one = tokenizer_one(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,  # Standard for CLIP\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.squeeze(0)\n",
        "        input_ids_two = tokenizer_two(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77,  # Align with tokenizer_one\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.squeeze(0)\n",
        "        # Validate input IDs\n",
        "        if input_ids_one.shape != torch.Size([77]) or input_ids_two.shape != torch.Size([77]):\n",
        "            raise ValueError(f\"Invalid input_ids shapes: one={input_ids_one.shape}, two={input_ids_two.shape}\")\n",
        "        example[\"input_ids_one\"] = input_ids_one\n",
        "        example[\"input_ids_two\"] = input_ids_two\n",
        "        return example\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {example['image_path']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Preprocess offline to avoid memory spikes\n",
        "processed_data = []\n",
        "for example in dataset:\n",
        "    result = preprocess(example)\n",
        "    if result is not None:\n",
        "        processed_data.append(result)\n",
        "dataset = Dataset.from_list(processed_data)\n",
        "print(f\"Dataset size after preprocessing: {len(dataset)}\")\n",
        "if len(dataset) == 0:\n",
        "    raise ValueError(\"No valid samples after preprocessing. Check images and prompts.\")\n",
        "\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# --- DataLoader ---\n",
        "def collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch:\n",
        "        return None\n",
        "    images = torch.stack([item[\"image\"] for item in batch])\n",
        "    input_ids_one = torch.stack([item[\"input_ids_one\"] for item in batch])\n",
        "    input_ids_two = torch.stack([item[\"input_ids_two\"] for item in batch])\n",
        "    return {\"image\": images, \"input_ids_one\": input_ids_one, \"input_ids_two\": input_ids_two}\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "# --- Training Setup ---\n",
        "pipe.unet.train()\n",
        "pipe.vae.eval()\n",
        "text_encoder_one.eval()\n",
        "text_encoder_two.eval()\n",
        "optimizer = bnb.AdamW8bit(pipe.unet.parameters(), lr=1e-5)\n",
        "pipe.unet, optimizer, train_dataloader = accelerator.prepare(pipe.unet, optimizer, train_dataloader)\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if batch is None:\n",
        "            print(f\"Warning: Empty batch at step {step}\")\n",
        "            continue\n",
        "        with accelerator.accumulate(pipe.unet):\n",
        "            images = batch[\"image\"].to(device, dtype=torch.float16)\n",
        "            input_ids_one = batch[\"input_ids_one\"].to(device)\n",
        "            input_ids_two = batch[\"input_ids_two\"].to(device)\n",
        "\n",
        "            # Debug input shapes\n",
        "            print(f\"Step {step}: input_ids_one.shape={input_ids_one.shape}, input_ids_two.shape={input_ids_two.shape}\")\n",
        "\n",
        "            # Text embeddings\n",
        "            with torch.no_grad():\n",
        "                text_embeds_one = text_encoder_one(input_ids_one)[0]  # [batch_size, 77, 768]\n",
        "                text_encoder_two_output = text_encoder_two(input_ids_two)\n",
        "                text_embeds_two = text_encoder_two_output.last_hidden_state  # [batch_size, 77, 1280]\n",
        "\n",
        "                # Manual mean pooling for text_encoder_two\n",
        "                attention_mask = (input_ids_two != tokenizer_two.pad_token_id).float()\n",
        "                text_embeds_two_pooled = (text_embeds_two * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(dim=1, keepdim=True)  # [batch_size, 1280]\n",
        "\n",
        "                # Debug shapes and dtypes\n",
        "                print(f\"Step {step}: text_embeds_one.shape={text_embeds_one.shape}, dtype={text_embeds_one.dtype}\")\n",
        "                print(f\"Step {step}: text_embeds_two.shape={text_embeds_two.shape}, dtype={text_embeds_two.dtype}\")\n",
        "                print(f\"Step {step}: text_embeds_two_pooled.shape={text_embeds_two_pooled.shape}, dtype={text_embeds_two_pooled.dtype}\")\n",
        "\n",
        "                # Ensure sequence embeddings are 3D\n",
        "                if text_embeds_one.dim() != 3:\n",
        "                    raise ValueError(f\"Expected 3D text_embeds_one, got {text_embeds_one.shape}\")\n",
        "                if text_embeds_two.dim() != 3:\n",
        "                    raise ValueError(f\"Expected 3D text_embeds_two, got {text_embeds_two.shape}\")\n",
        "\n",
        "                # Pad sequence embeddings\n",
        "                max_seq_len = max(text_embeds_one.shape[1], text_embeds_two.shape[1])\n",
        "                if text_embeds_one.shape[1] < max_seq_len:\n",
        "                    text_embeds_one = torch.nn.functional.pad(text_embeds_one, (0, 0, 0, max_seq_len - text_embeds_one.shape[1]))\n",
        "                if text_embeds_two.shape[1] < max_seq_len:\n",
        "                    text_embeds_two = torch.nn.functional.pad(text_embeds_two, (0, 0, 0, max_seq_len - text_embeds_two.shape[1]))\n",
        "\n",
        "                text_embeddings = torch.cat([text_embeds_one, text_embeds_two], dim=-1)  # [batch_size, seq_len, 768+1280]\n",
        "\n",
        "                # Ensure pooled embeddings are 2D\n",
        "                if text_embeds_two_pooled.dim() != 2:\n",
        "                    raise ValueError(f\"Expected 2D text_embeds_two_pooled, got {text_embeds_two_pooled.shape}\")\n",
        "\n",
        "            # Encode images\n",
        "            with torch.no_grad():\n",
        "                latents = pipe.vae.encode(images).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
        "\n",
        "            # Add noise\n",
        "            noise = torch.randn_like(latents, dtype=torch.float16, device=device)  # Match latents dtype\n",
        "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (latents.shape[0],), device=device)\n",
        "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Time IDs\n",
        "            add_time_ids = torch.tensor([[256, 256, 0, 0, 256, 256]], dtype=torch.float16, device=device)\n",
        "            add_time_ids = add_time_ids.repeat(latents.shape[0], 1)\n",
        "\n",
        "            # UNet forward\n",
        "            added_cond_kwargs = {\"text_embeds\": text_embeds_two_pooled, \"time_ids\": add_time_ids}\n",
        "            noise_pred = pipe.unet(\n",
        "                noisy_latents,\n",
        "                timesteps,\n",
        "                encoder_hidden_states=text_embeddings,\n",
        "                added_cond_kwargs=added_cond_kwargs\n",
        "            ).sample\n",
        "\n",
        "            # Loss (cast to float32 for stability)\n",
        "            noise_pred = noise_pred.float()  # Cast to float32\n",
        "            noise = noise.float()  # Cast to float32\n",
        "            loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "            print(f\"Step {step}: loss.dtype={loss.dtype}\")\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "# --- Save LoRA ---\n",
        "save_dir = \"./drive/MyDrive/sdxl-sft-finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "pipe.unet.save_pretrained(save_dir)\n",
        "print(f\"✅ LoRA saved to {save_dir}\")\n",
        "\n",
        "# --- Inference Test ---\n",
        "pipe.load_lora_weights(save_dir)\n",
        "prompt = \"An astronaut riding a green horse\"\n",
        "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, height=256, width=256).images[0]\n",
        "image.save(\"sft_aligned_astronaut.png\")\n",
        "print(\"✅ Saved sft_aligned_astronaut.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "263c65fd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from transformers import CLIPModel, CLIPProcessor, BitsAndBytesConfig\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "import bitsandbytes as bnb\n",
        "from bitsandbytes.optim import AdamW8bit\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import gc\n",
        "\n",
        "# ================================\n",
        "# 1. SETUP\n",
        "# ================================\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=8)\n",
        "device = accelerator.device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths\n",
        "dataset_path = \"./drive/MyDrive/cc_data/cc_data\"\n",
        "image_dir = os.path.join(dataset_path, \"images\")\n",
        "jsonl_path = os.path.join(dataset_path, \"dataset.jsonl\")\n",
        "negative_dir = os.path.join(dataset_path, \"negative_images\")\n",
        "os.makedirs(negative_dir, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(jsonl_path):\n",
        "    raise FileNotFoundError(f\"JSONL not found: {jsonl_path}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 2. GENERATE NEGATIVE IMAGES\n",
        "# ================================\n",
        "def generate_negative_images():\n",
        "    print(\"Generating negative images (192x192 + CPU offload)...\")\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/sdxl-turbo\",\n",
        "        torch_dtype=torch.float16,\n",
        "        use_safetensors=True,\n",
        "        variant=\"fp16\",\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    pipe.enable_model_cpu_offload()\n",
        "\n",
        "    # Load LoRA correctly\n",
        "    try:\n",
        "        pipe.load_lora_weights(\"./sdxl-sft-finetuned\", prefix=None)\n",
        "        print(\"LoRA loaded.\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: LoRA not loaded:\", e)\n",
        "\n",
        "    data = []\n",
        "    with open(jsonl_path, \"r\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= 100: break  # limit to 1\n",
        "            try:\n",
        "                entry = json.loads(line.strip())\n",
        "                prompt = entry.get(\"text\", \"\").strip()\n",
        "                img_path = os.path.join(image_dir, os.path.basename(entry[\"image_path\"]))\n",
        "                if not prompt or not os.path.exists(img_path):\n",
        "                    continue\n",
        "\n",
        "                neg_prompt = f\"blurry, low quality, {prompt}\"\n",
        "                neg_img = pipe(\n",
        "                    neg_prompt,\n",
        "                    num_inference_steps=15,\n",
        "                    guidance_scale=5.0,\n",
        "                    height=192,\n",
        "                    width=192,\n",
        "                    output_type=\"pil\"\n",
        "                ).images[0]\n",
        "\n",
        "                neg_path = os.path.join(negative_dir, f\"neg_{i}.png\")\n",
        "                neg_img.save(neg_path)\n",
        "\n",
        "                data.append({\n",
        "                    \"prompt\": prompt,\n",
        "                    \"chosen_path\": img_path,\n",
        "                    \"rejected_path\": neg_path\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(\"Gen error:\", e)\n",
        "\n",
        "    del pipe\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return data\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 3. LOAD DATA\n",
        "# ================================\n",
        "data = generate_negative_images()\n",
        "if not data:\n",
        "    raise ValueError(\"No data loaded.\")\n",
        "print(f\"Loaded {len(data)} pairs.\")\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. PREPROCESS\n",
        "# ================================\n",
        "def preprocess(ex):\n",
        "    try:\n",
        "        return {\n",
        "            \"prompt\": ex[\"prompt\"],\n",
        "            \"chosen_image\": Image.open(ex[\"chosen_path\"]).convert(\"RGB\"),\n",
        "            \"rejected_image\": Image.open(ex[\"rejected_path\"]).convert(\"RGB\")\n",
        "        }\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "dataset = dataset.map(preprocess, remove_columns=dataset.column_names)\n",
        "dataset = [x for x in dataset if x is not None]\n",
        "dataset = Dataset.from_list(dataset)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5. LOAD 4-BIT QLoRA CLIP\n",
        "# ================================\n",
        "clip_model_id = \"openai/clip-vit-large-patch14\"\n",
        "print(\"Loading 4-bit CLIP with QLoRA...\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\n",
        "    clip_model_id,\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None,\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "clip_model = get_peft_model(clip_model, lora_config)\n",
        "clip_model.print_trainable_parameters()\n",
        "\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_id)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6. DATALOADER\n",
        "# ================================\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"prompt\": [x[\"prompt\"] for x in batch],\n",
        "        \"chosen_images\": [x[\"chosen_image\"] for x in batch],\n",
        "        \"rejected_images\": [x[\"rejected_image\"] for x in batch],\n",
        "    }\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 7. TRAINING SETUP\n",
        "# ================================\n",
        "from bitsandbytes.optim import AdamW8bit   # ← ADD THIS IMPORT\n",
        "\n",
        "clip_model.train()\n",
        "optimizer = AdamW8bit(clip_model.parameters(), lr=1e-5)  # ← Fixed\n",
        "clip_model, optimizer, dataloader = accelerator.prepare(clip_model, optimizer, dataloader)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 8. TRAINING LOOP\n",
        "# ================================\n",
        "for epoch in range(1):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        with accelerator.accumulate(clip_model):\n",
        "            inputs_c = clip_processor(text=batch[\"prompt\"], images=batch[\"chosen_images\"],\n",
        "                                      return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
        "            inputs_r = clip_processor(text=batch[\"prompt\"], images=batch[\"rejected_images\"],\n",
        "                                      return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device)\n",
        "\n",
        "            out_c = clip_model(**inputs_c)\n",
        "            out_r = clip_model(**inputs_r)\n",
        "\n",
        "            loss = -torch.mean(torch.log(torch.sigmoid(out_c.logits_per_image.diag() - out_r.logits_per_image.diag())))\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 9. SAVE\n",
        "# ================================\n",
        "save_dir = \"./drive/MyDrive/clip-reward-lora\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "clip_model.save_pretrained(save_dir)\n",
        "clip_processor.save_pretrained(save_dir)\n",
        "print(f\"Saved to {save_dir}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 10. TEST\n",
        "# ================================\n",
        "clip_model.eval()\n",
        "test_img = Image.open(data[0][\"chosen_path\"]).convert(\"RGB\")\n",
        "inputs = clip_processor(text=[data[0][\"prompt\"]], images=[test_img], return_tensors=\"pt\").to(device)\n",
        "score = clip_model(**inputs).logits_per_image.item()\n",
        "print(f\"Test reward: {score:.4f}\")\n",
        "\n",
        "# Cleanup\n",
        "del clip_model, optimizer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "#  IMPORTS\n",
        "# -------------------------------------------------\n",
        "import torch, json, os, gc\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from bitsandbytes.optim import AdamW8bit\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  MOUNT & ACCELERATOR\n",
        "# -------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('./drive', force_remount=True)\n",
        "\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=8)\n",
        "device = accelerator.device\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  1. LOAD SDXL-Turbo in 4-bit (FIXED)\n",
        "# -------------------------------------------------\n",
        "model_id = \"stabilityai/sdxl-turbo\"\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        "    device_map=\"balanced\",      # ← REQUIRED for 4-bit\n",
        "    load_in_4bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "print(\"SDXL-Turbo loaded in 4-bit!\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  2. LoRA on UNet\n",
        "# -------------------------------------------------\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "pipe.unet = get_peft_model(pipe.unet, lora_cfg)\n",
        "pipe.unet.print_trainable_parameters()\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  3. Load SFT LoRA\n",
        "# -------------------------------------------------\n",
        "try:\n",
        "    pipe.load_lora_weights(\"./drive/MyDrive/sdxl-sft-finetuned\")\n",
        "    print(\"SFT LoRA loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"SFT LoRA not found: {e}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  4. Load CLIP Reward Model (Graceful)\n",
        "# -------------------------------------------------\n",
        "try:\n",
        "    reward_model = CLIPModel.from_pretrained(\"./drive/MyDrive/clip-reward-lora\").to(device)\n",
        "    reward_processor = CLIPProcessor.from_pretrained(\"./drive/MyDrive/clip-reward-lora\")\n",
        "    reward_model.eval()\n",
        "    print(\"Reward model loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Reward model not found: {e}. Using dummy reward.\")\n",
        "    def reward_fn(prompts, images):\n",
        "        return torch.zeros(len(prompts), device=device, dtype=torch.float32)\n",
        "else:\n",
        "    def reward_fn(prompts, images):\n",
        "        rewards = []\n",
        "        for p, img in zip(prompts, images):\n",
        "            try:\n",
        "                inputs = reward_processor(text=p, images=img, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "                with torch.no_grad():\n",
        "                    score = reward_model(**inputs).logits_per_image[0, 0].float()\n",
        "                rewards.append(score)\n",
        "            except:\n",
        "                rewards.append(torch.tensor(0.0, device=device))\n",
        "        return torch.stack(rewards)\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  5. Load Prompts\n",
        "# -------------------------------------------------\n",
        "jsonl_path = \"./drive/MyDrive/cc_data/cc_data/dataset.jsonl\"\n",
        "data = []\n",
        "\n",
        "with open(jsonl_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 100: break  # Change to 100\n",
        "        try:\n",
        "            entry = json.loads(line.strip())\n",
        "            if \"text\" in entry:\n",
        "                data.append({\"prompt\": entry[\"text\"]})\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if not data:\n",
        "    raise ValueError(\"No prompts loaded.\")\n",
        "print(f\"Loaded {len(data)} prompts.\")\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\"prompt\": [x[\"prompt\"] for x in batch]}\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  6. TRAINING SETUP (FIXED: NO prepare on PEFT model)\n",
        "# -------------------------------------------------\n",
        "pipe.unet.train()\n",
        "optimizer = AdamW8bit(pipe.unet.parameters(), lr=1e-6)\n",
        "\n",
        "# ONLY prepare optimizer and dataloader\n",
        "optimizer, dataloader = accelerator.prepare(optimizer, dataloader)\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  7. RLHF TRAINING LOOP (ALL FIXED)\n",
        "# -------------------------------------------------\n",
        "num_epochs = 1\n",
        "beta = 0.1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        with accelerator.accumulate(pipe.unet):\n",
        "            prompts = batch[\"prompt\"]\n",
        "            batch_size = len(prompts)\n",
        "\n",
        "            # ---- Generate images ----\n",
        "            with torch.no_grad():\n",
        "                images = pipe(\n",
        "                    prompts,\n",
        "                    num_inference_steps=4,\n",
        "                    guidance_scale=0.0,\n",
        "                    height=256, width=256,\n",
        "                    output_type=\"pil\"\n",
        "                ).images\n",
        "\n",
        "            # ---- Reward ----\n",
        "            rewards = reward_fn(prompts, images)\n",
        "            print(f\"Step {step} | Reward: {rewards.mean().item():.4f}\")\n",
        "\n",
        "            # ---- Preprocess images ----\n",
        "            preprocess = T.Compose([\n",
        "                T.Resize((256, 256)),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.5], [0.5])\n",
        "            ])\n",
        "            pixel_values = torch.stack([preprocess(img) for img in images]).to(device, dtype=torch.float16)\n",
        "\n",
        "            # ---- Encode latents ----\n",
        "            latents = pipe.vae.encode(pixel_values).latent_dist.sample()\n",
        "            latents = latents * pipe.vae.config.scaling_factor\n",
        "            latents.requires_grad_(True)\n",
        "\n",
        "            # ---- Add noise ----\n",
        "            noise = torch.randn_like(latents, dtype=torch.float16)\n",
        "            timesteps = torch.randint(0, pipe.scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
        "            noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # ---- Text embeddings ----\n",
        "            text_inputs = pipe.tokenizer(prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").to(device)\n",
        "            text_inputs_2 = pipe.tokenizer_2(prompts, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                emb1 = pipe.text_encoder(text_inputs.input_ids)[0]\n",
        "                emb2 = pipe.text_encoder_2(text_inputs_2.input_ids).last_hidden_state\n",
        "                mask = text_inputs_2.attention_mask.unsqueeze(-1).float()\n",
        "                pooled = (emb2 * mask).sum(1) / mask.sum(1).clamp(min=1e-8)\n",
        "                encoder_hidden_states = torch.cat([emb1, emb2], dim=-1)\n",
        "\n",
        "            # ---- Time IDs (batched) ----\n",
        "            base_time_ids = torch.tensor([256, 256, 0, 0, 256, 256], dtype=torch.float16, device=device)\n",
        "            add_time_ids = base_time_ids.unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "            # ---- UNet forward ----\n",
        "            noise_pred = pipe.unet(\n",
        "                noisy_latents,\n",
        "                timesteps,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                added_cond_kwargs={\"text_embeds\": pooled, \"time_ids\": add_time_ids}\n",
        "            ).sample\n",
        "\n",
        "            # ---- Loss (FIXED: cast to float32) ----\n",
        "            mse = torch.nn.functional.mse_loss(noise_pred.float(), noise.float(), reduction=\"none\").mean([1,2,3])\n",
        "            loss = -torch.mean(rewards * mse) + beta * torch.norm(noise_pred, p=2)\n",
        "            loss = loss.float()  # ← CRITICAL FIX\n",
        "\n",
        "            # ---- Backward ----\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Cleanup\n",
        "        del pixel_values, latents, noisy_latents, noise_pred, loss, images\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  8. SAVE LoRA ONLY\n",
        "# -------------------------------------------------\n",
        "save_dir = \"./drive/MyDrive/sdxl-rlhf-lora\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "pipe.unet.save_pretrained(save_dir)\n",
        "print(f\"LoRA saved -> {save_dir}\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  9. INFERENCE TEST\n",
        "# -------------------------------------------------\n",
        "pipe.unet.eval()\n",
        "with torch.no_grad():\n",
        "    img = pipe(\"An astronaut riding a green horse\", num_inference_steps=1, guidance_scale=0.0).images[0]\n",
        "    img.save(\"rlhf_astronaut.png\")\n",
        "    print(\"Saved rlhf_astronaut.png\")\n",
        "\n",
        "# -------------------------------------------------\n",
        "#  CLEANUP\n",
        "# -------------------------------------------------\n",
        "del pipe, reward_model, optimizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "bZ3N3iIv8zqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}